\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}

% Basic packages
\usepackage{graphicx}  % For images
\usepackage{hyperref}  % For hyperlinks
\usepackage[superscript]{cite}
\usepackage{tcolorbox}
\usepackage{parskip}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{calc} % <-- IMPORTANT for coordinate arithmetic
\usetikzlibrary{decorations.pathmorphing,arrows.meta}
\usepackage{feynmp-auto}
% Extra formatting packages
\usepackage{geometry}  % For setting page margins
\usepackage{fancyhdr}  % For customizing headers and footers
\usepackage{titlesec}  % For customizing section titles
\usepackage{setspace}  % For adjusting line spacing
\usepackage{lmodern}  % For modern font
\usepackage{xcolor}   % For color customization
\usepackage{booktabs,tabularx,hyperref}

\numberwithin{equation}{section}
\usepackage{chngcntr}   % optional in newer LaTeX versions 
\counterwithin{figure}{section}
\counterwithin{table}{section}

\setlength{\headheight}{13.59999pt}
% Set up page margins
\geometry{
  a4paper,
  left=1in,
  right=1in,
  top=1in,
  bottom=1in
}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark}
% Set up line spacing
\onehalfspacing

% Customize section titles
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Set up headers and footers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark} % Left-aligned section title
\fancyhead[R]{\thepage}  % Right-aligned page number
\fancyfoot[C]{\textit{}} % Centered footer with your name

% Customize hyperlink colors
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=magenta,
  pdftitle={NE TN0XXX},  % PDF document title
  pdfauthor={G. Ruffini},       % PDF document author
}

\title{Neural Encoding through Hierarchical Amplitude Modulation\\
NE TN0434}
\author{Giulio Ruffini\thanks{giulio.ruffini@neuroelectrics.com, francesca.castaldo@bcom.one} \ and Francesca Castaldo 
\\ Neuroelectrics Barcelona, Barcelona Computational Foundation}

%\newcommand{\Comparator}{$\lceil \Delta \rceil$}
\usepackage{xspace}
\newcommand{\Comparator}{$\nu$\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
 

 \begin{abstract}
 
Hierarchical encoding is a structural element of the Free Energy Principle and related information-centric accounts of brain function, but a concrete circuit-level mechanism for it remains elusive. 
Here we examine Hierarchical Amplitude Modulation (HAM) – a computationally grounded scheme where information is encoded in the envelope of a carrier and in which slower brain rhythms multiplicatively modulate the amplitude of faster rhythms, creating a cascade of nested oscillatory envelopes. This multiplicative architecture naturally produces intermodulation frequencies (of the form \(f_c + \sum a_i f_i\)) and predicts that oscillation frequency bands should be log-spaced ($r \gtrsim $ 2--3, depending on the number of modulation layers) to avoid spectral overlap, as in constant-Q filter banks, consistent with the observed logarithmic spacing of canonical brain rhythms (with ratios $\sim$2–3). HAM's log-uniform distribution yields a $1/f$ global spectral profile and $1/f^\alpha$ sideband spectra in the broadband “aperiodic” component, with the slope determined by modulation depth $m$ and band ratio $r$. 
We then demonstrate modulation and demodulation in the laminar neural mass model (LaNMM), where a fast excitatory–inhibitory oscillator circuit couples with a slower cortical oscillator (Janse-Rit). Through the network's intrinsic nonlinearities and cross-frequency coupling, amplitude modulation and demodulation are implemented. 
These results provide a novel circuit-level mechanism for hierarchical predictive coding, linking theoretical principles to observed spectral features of brain activity.
\end{abstract}
% with a ratio $r\approx 3$. 

%%%%%%%%%%%%%%%
\clearpage
\tableofcontents

\clearpage
\textbf{\large Highlights}
 
 

\begin{itemize}
\item We introduce HAM: a principle for the organization of hierarchical information processing in the brain. Information is encoded through amplitude modulation, where faster rhythms are multiplicatively modulated by slower processes.  Information can thus live in signals or their envelopes (and envelopes of envelopes).
\item The required intermodulation structure for band protection, $f_c \pm \sum a_i f_i$, finds a natural implementation in near geometric (log) spacing with a frequency ratio $r\gtrsim$ 2--3 (strong super-increasing hierarchy), yielding constant fractional bandwidths and enabling staged demodulation.
\item In addition to geometric frequency spacing, HAM provides two routes to $1/f^\alpha$: (i) a cascade link $\alpha = 2\ln(2/m)/\ln r$; (ii) a log‑uniform mixture that yields $1/f$ in expectation with constant‑Q kernels.
\item We provide a proof of concept modulation and demodulation implementation in NMM using the LaNMM with PING‑like fast generators as carriers and JR‑like slow generators for envelope extraction. 
\end{itemize}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage



\section{Introduction}

Predictive coding has emerged as a central framework for understanding neural processing, positing that the brain continuously generates predictions about incoming sensory data and updates these predictions by minimizing prediction errors \cite{raoPredictiveCodingVisual1999,fristonPredictiveCodingFreeenergy2009}. In the Free Energy Principle, this hierarchical exchange of top-down predictions and bottom-up errors minimizes free energy \cite{fristonFreeEnergyPrinciple2006,parr2022active}. 
Kolmogorov Theory (KT) embeds this process within Algorithmic Information Theory, suggesting that both biological brains and digital agents optimize predictions by constructing compressive models of the world \cite{ruffiniInformationComplexityBrains2007, ruffiniRealitySimplicity2009, ruffiniModelsNetworksAlgorithmic2016,ruffiniAlgorithmicInformationTheory2017,ruffiniAlgorithmicAgentPerspective2024,ruffiniStructuredDynamicsAlgorithmic2025}. All such information-centric proposals of brain function fundamentally rely on encoding information in brain signals to compute prediction errors and update internal representations.

Neural coding schemes have long been debated, with proposals ranging from rate coding (where information is carried by average firing rates) to various temporal strategies—including spike timing, synchrony, phase coding, and burst coding—to capture the richness of neural information \cite{zeldenrustNeuralCodingBurstsCurrent2018, carianiTimeEssenceNeural2022, lismanThetaGammaNeuralCode2013}. In this context, it is increasingly clear that information is encoded not only in the raw oscillatory signals but also in their amplitude envelopes \cite{Ruffini2025Comparator}. This dual coding strategy is reminiscent of amplitude modulation (AM) in radio communications, where a high-frequency carrier encodes information via its modulated envelope \cite{nahinScienceRadio1996}.

In parallel, two empirical regularities stand out in neural field recordings: an aperiodic \(1/f^{\alpha}\) background and a near‑logarithmic spacing of canonical oscillation bands. Both patterns recur across species and recording modalities, and the aperiodic component can be cleanly separated from periodic peaks in modern parameterizations \cite{millerPowerLawScalingBrain2009,heScalefreeBrainActivity2014,donoghueParameterizingNeuralPower2020,penttonenNaturalLogarithmicRelationship2003,buzsaki_brain_2023,donoghueEvaluatingComparingMeasures2024}.

 
Oscillatory activity in the brain naturally segregates into functional bands—such as delta, theta, alpha, beta, and gamma. These canonical ‘bands’ should be understood as families of preferred modes. Although not universally sharp peaks in raw Power Spectra Densities (PSDs), their separability strengthens after accounting for the $1/f$ background or analyzing burst‑wise dynamics, and their centers align with an ~geometric progression across circuits and species.
They exhibit
a geometric progression with a common ratio \(r \approx 2\)--3 (equivalently, equal spacing on a
log-frequency axis; constant‑Q filterbank \cite{donoghueParameterizingNeuralPower2020}, see Figure~\ref{fig:logarithmic_hierarchy}). That is, the center frequencies increase by a constant multiplier, or equivalently, they are evenly spaced when plotted on a logarithmic scale \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiNeuronalOscillationsCortical2004}. The reason for this spacing is unclear. 
A leading account is that the approximately logarithmic (geometric) spacing emerges because a constant ratio between bands lets excitability windows and integration times scale with the oscillation period, enabling circuits with heterogeneous axonal/synaptic delays and sizes to coordinate; in this view, the “family” of bands helps overcome delay‑imposed processing limits \cite{penttonenNaturalLogarithmicRelationship2003}.
Complementarily, it has been proposed that placing bands at equal distances on a log‑frequency axis minimizes mutual entrainment/cross‑talk and reflects an underlying hierarchical network architecture \cite{steinkeBrainRhythmsReveal2011}.

It is believed that oscillatory activity plays a key role in organizing and transmitting information. Gamma oscillations (30–100 Hz) have been implicated in local information processing, feature binding, and communication through coherence \cite{buzsakiMechanismsGammaOscillations2012, jensenHumanGammafrequencyOscillations2007, friesNeuronalGammabandSynchronization2009, singerVisualFeatureIntegration1995}, while slower rhythms (theta 4–8 Hz and alpha 8–12 Hz) are associated with attentional gating and top-down signaling \cite{buzsakiRhythmsBrain2006, klimeschEEGAlphaOscillations2007, hanslmayrRoleOscillationsTemporal2011}.
 
Slower oscillations, with longer periods, provide a broad temporal window and allow integration over larger spatial extents with more variable synaptic delays, whereas faster oscillations offer more precise, spatially limited representations of information. Such evidence reinforces the notion that brain oscillations are organized into a conserved, hierarchical set of timescales, with adjacent rhythm classes separated by roughly constant frequency ratios on the order of 2 to 3 \cite{penttonenNaturalLogarithmicRelationship2003, buzsakiNeuronalOscillationsCortical2004, penttonenNaturalLogarithmicRelationship2003,klimeschFrequencyArchitectureBrain2018a,
buzsakiScalingBrainSize2013,buzsaki_brain_2023}.

In line with hierarchical processing concepts, such findings suggest that neural information processing is inherently multiscale, with nested oscillatory hierarchies that multiplex information much like modern communication systems.
 Gamma oscillations, in particular, serve as carriers of sensory information, while slower rhythms modulate these carriers via cross-frequency coupling. 
Motivated by this, we propose a \textit{hierarchical amplitude modulation} (HAM) framework to describe neural information encoding where information is represented in a layered fashion: the raw oscillatory signal (the carrier), its amplitude envelope, and, recursively, the envelope of that envelope. This nested modulation approach yields a coarse-grained, multiscale, natural representation of information consistent with hierarchical predictive coding. As each successive envelope captures fluctuations at a slower timescale, the system exhibits a logarithmic frequency spacing and power law of oscillatory dynamics—phenomena that align with the observed geometric progression of brain rhythms. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [t]
  \centering
  \includegraphics[width=1\linewidth]{figures/logarithmic_hierarchy.png}
  \label{fig:logarithmic_hierarchy}
  \caption{Logarithmic oscillatory hierarchy.
Oscillation bands are approximately log‑spaced with an approximately common ratio \(r\)
so that \(f_{k+1}=f_k/r\) (empirically \(r\approx 2\)--3 between neighboring
bands) \cite {penttonenNaturalLogarithmicRelationship2003}. This approaches the theoretical limit of $r\gtrsim 3$ for non-overlapping sideband clusters and is equivalent to equal spacing on a log‑frequency axis (corresponding to a constant‑Q arrangement).}
\end{figure}
% https://colab.research.google.com/drive/1XSmrR51GdENpHDsh4ujWh7NFp4po9NwC#scrollTo=-d9JBVYRYILM
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 

In the following sections, we present the mathematical formulation of the model and demonstrate how it can account for the observed logarithmic spacing and provide a mechanism for the power-law distribution of oscillatory bands. To demonstrate the plausibility of this framework, we then provide a proof-of-concept implementation of amplitude modulation in the Laminar Neural Mass Model (LaNMM) \cite{ruffiniP118BiophysicallyRealistic2020, sanchez-todoPhysicalNeuralMass2023,sanchez-todoFastInterneuronDysfunction2025a, Ruffini2025Comparator}, a dual frequency model capturing the interaction between superficial fast and deeper slow oscillations through mechanisms such as phase-amplitude coupling and amplitude-amplitude anticorrelation. 
 

\section{Hierarchical amplitude modulation (HAM)}
\subsection{Construction} 
In standard amplitude modulation (AM) (as used in radio), information is encoded in the \emph{amplitude} of a high-frequency carrier wave. For example, an audio signal $m(t)$ (low-frequency information) can modulate a higher-frequency carrier $C(t) = A_c \cos(2\pi f_c t)$ by varying the carrier’s amplitude in proportion to $m(t)$. The result is a modulated signal 
\begin{equation}
S(t) \;=\; A_c\,[1 + a\,m(t)]\,\cos(2\pi f_c t)\,,
\label{eq:AM_single}
\end{equation}
where $a$ is the modulation index (chosen so that $1+a\,m(t)>0$ to avoid distortion). Demodulation of $S(t)$ recovers the original message $m(t)$ by detecting the envelope. In neural terms, one can think of fast oscillations (e.g., gamma rhythms) acting as carriers whose amplitudes are controlled by slower processes (e.g., theta or delta rhythms).


\begin{figure}[t!]
  \centering
  \includegraphics[width=0.85\linewidth]{figures/AM_signals.png}
 
  \caption{\textbf{Amplitude modulation (AM)}, where information in a slower signal is encoded in the envelope of a faster carrier, as in AM radio. }
  \label{fig:AM}
\end{figure} 


While classical AM involves a single modulating signal (Fig.~\ref{fig:AM}), the brain exhibits \emph{nested} oscillations: faster rhythms modulated by slower rhythms, which in turn are modulated by even slower fluctuations. \textit{Hierarchical Amplitude Modulation} (HAM) generalizes the AM concept to multiple layers of amplitude modulation across different frequency bands. In a HAM cascade, a high-frequency oscillation’s amplitude is modulated by a slower oscillation; this slower oscillation’s amplitude can itself be modulated by an even slower process; and so on, creating a multilevel envelope hierarchy. In effect, information can be carried not only by the fast “carrier” signal, but also by its envelope, the envelope-of-envelope, etc., forming a multiscale encoding of information. Not all frequencies need to be involved in a cascade, but all such combinations should be possible.

\noindent \textbf{Nested modulation across oscillatory bands.} Consider a chain of $N$ nested modulators with descending center frequencies $f_0 > f_1 > \cdots > f_N$. For concreteness, let $f_0$ be a high-frequency carrier and $f_1, f_2, \dots$ be progressively slower oscillatory components. A depth-$N$ HAM signal can be written as a product of an $N$-layer envelope and the carrier,
\begin{equation}
s_N(t) \;=\; A_0 \prod_{i=0}^{N}\Big[\,1 + m_i\cos(2\pi f_i t + \phi_i)\Big]~,
\label{eq:HAM_product}
\end{equation}
with $0<m_i<1$, the modulation depths at each layer. Here, $i=0$ corresponds to the fastest oscillation (the putative “carrier”), and each $i>0$ term represents an oscillatory envelope imposed on the previous layers. All the terms in the product are positive to reflect the nature of firing rates, but this is not a crucial element. Expanding the product in Eq.~(\ref{eq:HAM_product}) and using the trigonometric identity $\cos A\,\cos B = \tfrac{1}{2}[\cos(A+B)+\cos(A-B)]$ iteratively yields a superposition of cosines at various frequencies, revealing a rich \emph{intermodulation spectrum}: new frequency components (sidebands) appear at linear combinations of the base frequencies. In fact, the spectrum of $s_N(t)$ contains sinusoidal components at frequencies 
\begin{equation}
f \;\in\; \Big\{\;\sum_{i=0}^{N} a_i\,f_i:\; a_i\in\{-1,0,1\}\;\Big\}\!,
\label{eq:sideband_set}
\end{equation}
i.e. at the carrier frequency $\pm f_0$, the modulation frequencies $\pm f_1,\pm f_2,\dots$, and all combinations such as $f_0\pm f_1$, $f_0\pm f_2$, $f_0\pm f_1 \pm f_2$, etc. Each modulation layer thus spreads the signal’s spectral energy into new sideband peaks spaced around the original frequencies. Importantly, the strength of higher-order sidebands (involving multiple $m_i$ factors) is suppressed for small $m_i$; thus, the cascade remains dominated by its fundamental bands and nearest sidebands if $m_i \ll 1$.

This multiplicative, multiband structure is illustrated in Figure~\ref{fig:ham_example}. A high-frequency carrier (Fig.~\ref{fig:ham_example}, top) is successively modulated by a slower oscillation, then by an even slower one, and so on, producing a final complex signal with amplitude fluctuations at multiple timescales (middle). The corresponding frequency spectrum (Fig.~\ref{fig:ham_example}, bottom) displays the original carrier line and a comb of sidebands generated by each modulation layer. Notably, the spectral power decays at lower frequencies, reflecting how each envelope layer contributes a factor $m_i<1$ to the amplitude (as discussed below). The nested HAM signal can be demodulated in stages to recover the slower envelope signals from the faster carrier, analogous to how a radio receiver detects a single AM envelope – but here performed iteratively across scales.


 

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.85\linewidth]{figures/ham_signals.png}
  \includegraphics[width=0.8\linewidth]{figures/ham_spectrum.png}
  \caption{\textbf{HAM example.} A high-frequency carrier is successively modulated by slower oscillations in a nested multiplicative structure. Each subplot presents a key stage in the formation of the hierarchical signal:
	(A)	Envelope Modulations (First three subplots from top): Each line represents an envelope function of the form $1 + m_i \cos(2\pi f_i t + \phi_i)$, where each modulation frequency $f_i$ is progressively lower. These envelopes modulate all previously applied layers, creating an intricate structure of amplitude variations at multiple timescales.
	(B)	Demeaned, final Hierarchical AM Signal (fourth plot): The resulting signal after applying all modulation layers. This waveform is the product of all envelope layers modulating a carrier, generating a complex multi-scale amplitude variation. The signal is filtered around the highest carrier frequency (125 Hz). This highlights how the original high-frequency carrier retains structured amplitude variations due to the hierarchical modulation. (C) Spectrum of the full signal. 
The hierarchical nature of modulation creates a combinatorial spectral expansion, where all integer combinations of modulating frequencies contribute to the final spectrum. This structure requires logarithmic spacing of modulation frequencies to prevent spectral overlap and ensure clear separation of modulation layers. The figure also shows a logarithmic decay in sideband power, which stems from the successive product of $m_i$ factors in each band.}
  \label{fig:ham_example}
\end{figure}


\subsection{Avoiding spectral overlap}
A critical challenge in any multi-layer modulation is avoiding spectral \textit{overlap} between components. If the newly generated sideband frequencies crowd into neighboring frequency bands, information at different layers would interfere and demodulation will not be possible. In engineered communication systems, this is solved by allocating \emph{guard bands} and using a \emph{constant-Q} spacing of carriers – meaning each carrier’s bandwidth is a fixed fraction of its center frequency (a logarithmic progression of center frequencies). Strikingly, brain oscillations also appear roughly log-spaced in frequency (each band about a constant ratio higher than the next) \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiNeuronalOscillationsCortical2004}. HAM provides a mechanistic reason for this: \emph{geometric spacing} of frequencies naturally prevents intermodulation overlap.

Mathematically, the necessary and sufficient condition to keep all sideband clusters disjoint (and hence ensure sequential demodulation) is to require that each higher-frequency $f_k$ exceeds twice the sum of all lower frequencies (see Appendix~\ref{sec:hierarchical_am}). In other words, if 
\begin{equation}
f_k > 2\sum_{j>k} f_j \quad \mbox{ for every } k, 
\end{equation}
then no combination of slower oscillators can produce a difference-frequency that reaches the $f_k$ cluster. 
A practical design meeting the necessary condition is to choose a common spacing ratio $r$ such that 
\begin{equation}
f_{k+1} \approx \frac{f_k}{r}\, , \quad \text{ with } r\gtrsim 2\text{--}3,
\label{eq:geometric_spacing}
\end{equation}
yielding an approximately constant factor separation between successive bands. The minimal ratio needed depends on the number of modulation layers, starting at $r=2$ for a single AM layer and quickly saturating at $r=3$ for deeper modultion layers (see Figure~\ref{fig:minimal_r} in Appendix~\ref{app:math}). Geometric spacing (log-uniform in frequency) ensures that each modulation’s sidebands fit into the “gaps” between adjacent bands without overlap if $r$ is large enough. %: $r>2$ suffices for non-overlap, $r>3$ for strict uniqueness. 
Then, the bands behave like well-separated narrow channels. In practice, neural oscillations exhibit $r\approx 2$–3 (e.g., delta~$\sim$2–4 Hz, theta~$\sim$6–8 Hz, alpha~$\sim$10 Hz, etc.), which provides adequate separation for finite-depth or weak modulation. This log-spaced, constant-Q arrangement mirrors classical wavelet filter banks and the cochlear frequency, and it matches the empirical observation of near-logarithmic spacing in canonical brain rhythm bands.

Crucially, the log-spaced hierarchy means each oscillatory “band” retains its identity (its own frequency range) even as it participates in a larger modulation cascade. By keeping modulation depths $m_i$ moderate ($m_i<1$), each layer’s sidebands remain as small perturbations around the existing lines. This avoids excessive distortion and allows the original carrier and envelope signals to be separated by bandpass filtering or sequential demodulation. In summary, HAM’s multiplicative encoding across scales naturally favors a logarithmic frequency arrangement: only with such spacing can a deep hierarchy of oscillations coexist without mutual spectral interference.



\paragraph{Power-laws.}
Many electrophysiological signals exhibit an aperiodic $1/f$-like background in their power spectra. The HAM mechanism offers two complementary explanations for this ubiquitous observation. First, a whole population of log-spaced oscillators will \emph{collectively} produce a $1/f$-type spectrum in the aggregate. Contributions of equal power per octave (a flat distribution on a log-frequency axis) naturally yield a $P(f)\propto 1/f$ spectrum. In other words, if oscillatory power is spread roughly evenly across logarithmic frequency bins (as constant-Q band partitioning does), the summed spectrum follows a pink-noise $1/f$ profile (this is a well-known property in audio and wavelet analysis). Second, a single deep cascade of nested oscillations can generate a $1/f^{\alpha}$ spectrum with $\alpha$ tuned by neuronal parameters (e.g., modulation strengths and band spacing). We discuss next these two routes in more detail.
%The HAM framework encompasses both routes: the “cascade route” linking $\alpha$ to specific circuit parameters (Eq.~(\ref{eq:powerlaw})), and the “ensemble route” where many oscillators with log-uniform frequency spacing produce $\alpha\approx1$ in expectation. Both mechanisms may operate in neural systems, contributing to the observed aperiodic power law scaling in brain activity. 

\subsubsection*{Global Spectrum} 
 An immediate result from the geometric scaling architecture is power law spectral scaling. As shown in Appendix~\ref{app:powerlaw},  a bank of log‑spaced oscillators with equal power per band produces \(S(f)\approx 1/f\).
Morevoer, random AM pairings add a constant pedestal; stronger modulation (\(m\)) or more pairs
increase \(\kappa_0\) and flatten the fitted \(\alpha\) over finite windows. Finally, using constant‑\(Q\)
kernels (or filters) should push the fitted slope in Fig.~\ref{fig:logarithmic_hierarchy} toward \(\alpha\approx 1\) and
increase the fitting range.
The assumption \(\rho(f)\propto 1/f\) parallels empirical \emph{logarithmic spacing} of neural bands \cite{penttonenNaturalLogarithmicRelationship2003} and is mathematically equivalent to a constant‑\(Q\)
tiling (equal spacing on a log‑frequency axis) \cite{brownCalculationConstantSpectral1991}. 





\begin{figure} [t!]
  \centering
  \includegraphics[width=0.99\linewidth]{figures/simulated_spectrum_PBuz.png}
  \caption{\textbf{Log--log spectra for log-spaced oscillators with AM sidebands.}
We display the HAM spectrum using the Penttonen--Buzsáki band centers and equal amplitude carriers, which leads to $1/f$ behavior. \label{fig:simulationfullspectrum}
% https://colab.research.google.com/drive/1Pi7JAZTRPXa7-UuQ-hx75XIW4hYB8_qr#scrollTo=xm2iPf5RuBaB
The solid curve shows the per‑Hz power spectral density (PSD) obtained by summing
constant–$Q$ Gaussian lines for carriers and their first‑order AM sidebands
($f_c\pm f_m$, $f_m<f_c$).  Each base center is replicated with multiplicative jitter
$s\!\sim\!\mathcal{U}[1/2,2]$ to populate each cluster, carrier amplitudes are $A_c\equiv 1$,
and the sideband power is $\big(\tfrac{mA_c}{2}\big)^2$ with $m=0.9$.
Dotted vertical lines mark the Penttonen–Buzsáki center frequencies; faint spans indicate
their reported ranges. The dashed line is a $C/f$ reference. Fitted slopes
over 5–250\,Hz are indicated in the legend. Because generator density per Hz declines roughly as $1/f$
under log spacing, the aggregate PSD decays with frequency; AM sidebands add a low‑$f$
pedestal but preserve the $1/f$ backbone in the interior.
}

\end{figure}
% 
 
 

We tested the prediction that a log-spaced hierarchy of oscillatory generators produces a
per-Hz power density that decays with frequency on a linear axis (reflecting the decreasing
generator density per Hz), and that the aggregate spectrum approaches a power law on a
log--log axis (see Figure~\ref{fig:simulationfullspectrum}).
We considered a frequency organizations using the set of
Penttonen--Buzsáki band centers. To fill gaps on a linear frequency axis, each center
$f_i$ was replicated $K$ times with multiplicative jitter $u\sim\mathrm{Uniform}[1,2]$,
producing micro-generators at $u f_i$ (truncated to $[f_{\min},f_{\max}]$).
Each micro-generator contributes a narrowband line shaped by a constant-$Q$ Gaussian
kernel (fractional width $\sigma/f=q$) so that bandwidth scales with center frequency.
We set all carrier amplitudes to $a_i=1$ (equal power). To emulate amplitude modulation, we drew $M$ random pairs $(f_m,f_c)$ from the
micro-generators with $f_m<f_c$ and added first-order sidebands at $f_c\pm f_m$, each
weighted by $(m/2)^2$ in power, where $m\in(0,1)$ is the modulation depth.
Per-Hz power spectra were computed on a dense linear frequency grid and shown on a log--log
axis. A $1/f$ reference ($k/f$) was overlaid by matching the median of $S(f)\,f$ in an
interior band. The aperiodic slope $\alpha$ was estimated by ordinary least squares on
$\log_{10}S(f)$ vs.\ $\log_{10}f$ over a predefined range (5--250\,Hz). The design
tests the claims that (a) constant-$Q$ log spacing implies decreasing spectral power density with
frequency, and (b) the aggregate spectrum approximates $1/f^{\alpha}$, with sidebands
enhancing low-frequency accumulation while preserving the backbone.




\paragraph{Sideband Spectrum.} 
A further consequence of the HAM cascade is the emergence of \textit{power-law} scaling in the sideband spectrum. Because each modulation layer redistributes a fraction of the signal’s power to lower frequencies, the overall spectral density tends to decline with frequency in a scale-free (fractal-like) manner. In fact, under log-spacing, a simple analysis shows that the power spectral density of a hierarchical cascade approximately follows $P(f)\sim 1/f^{\alpha}$ – a $1/f$-type law – with the exponent $\alpha$ depending on the modulation depth ($m$) and spacing ratio ($r$).


\begin{figure} [t!]
  \centering
  \includegraphics[width=0.6\linewidth]{figures/power_law.png}
\caption{\textbf{Power‑law exponent \(\alpha\) for log‑spaced HAM sideband.}
For \(r>1\), the toy‑model exponent is
\(\alpha=2\ln(2/m)/\ln r\). Larger \(m\) (stronger modulation) and
larger \(r\) (wider spacing) both \emph{reduce} \(\alpha\) (shallower slope);
smaller \(m\) steepens the slope.}

  \label {fig:power_law}
\end{figure}


For intuition, consider an idealized case where every layer uses the same depth $m$ and ratio $r$ for clarity. The first modulation produces sidebands carrying a fraction $\sim (m/2)^2$ of the carrier power (since each first-order sideband has amplitude $\approx m/2$ of the carrier). The next modulation layer acts on those sidebands, generating second-order components of order $(m/2)^2$ times the first layer’s amplitude, i.e. $(m/2)^4$ relative to the original carrier’s amplitude, and so on. After $k$ layers, the characteristic frequency has decreased to $f_k \approx f_0/r^k$, and the power at that scale is suppressed by roughly $(m/2)^{2k}$ relative to the top. Quantitatively, one can show (see Appendix~\ref{app:powerlaw})
\begin{equation}
P(f_k) \;\propto\; \Big(\frac{f_k}{f_0}\Big)^{2\ln(m/2)/\ln(1/r)} \;=\; \frac{1}{f_k^{\,\alpha}}~,
\label{eq:powerlaw}
\end{equation}
where 
\[
\alpha \;=\; \frac{2\,\ln(2/m)}{\ln r}\,. 
\] 
This indicates a $1/f^{\alpha}$ scaling of the cascade’s spectrum. Notably, $\alpha$ increases as $m$ decreases (weaker modulation yields a steeper spectral drop-off) and $\alpha$ decreases as $r$ increases (wider band gaps flatten the spectrum). Figure~\ref {fig:power_law} illustrates this relationship: stronger modulation (larger $m$) or broader spacing (larger $r$) results in a shallower slope (smaller $\alpha$), whereas small $m$ leads to a pronounced $1/f^{\alpha}$ fall-off. 

Thus, the emergence of $1/f^\alpha$ spectral structure in hierarchical AM systems is explained by two minimal structural constraints: (1) a log-spaced distribution of oscillators and (2) a modulation rule enforcing \( f_m < f_c \). Sidebands are not necessary for scale-free behavior but do enhance low-frequency accumulation and steepen the slope.




\section{Modulation and demodulation in a LaNMM node}
 
 We previously introduced a \textit{laminar} neural mass model (LaNMM) \cite{ruffiniP118BiophysicallyRealistic2020,sanchez-todoPhysicalNeuralMass2023,sanchez-todoFastInterneuronDysfunction2025, castaldoRosetta2025} optimized from laminar recordings from the macaque prefrontal cortex \cite{bastos_laminar_2018} and designed to capture both superficial-layer fast and deeper-layer slow oscillations along with their interactions, which give rise to cross-frequency coupling (CFC) \cite{mercadalBridgingLocalGlobal2025}. 
  Here, we show how, thanks to its dual frequency character and nonlinear transfer function, it also provides a candidate circuital implementation of modulation,  demodulation, and envelope extraction.
 
 
To implement the simplest modulation scheme, two frequency generators are needed.
The LaNMM combines conduction physics with two NMMs---Jansen-Rit (P1) and Pyramidal Interneuronal Network Gamma (P2) subpopulations at slow and fast frequencies, respectively---to simulate depth-resolved electrophysiology (see Figure~\ref{fig:LaNMM_Mod_Demod}). Bifurcation properties and cross-frequency coupling have been recently studied \cite{Ruffini2025Comparator,depalmaaristidesEmergenceMultifrequencyActivity2025}. 

Modulation and demodulation require a nonlinearity in the model --- the transfer function $\sigma(\cdot)$ \cite{castaldoRosetta2025}. 
This key element in this and related models transduces the sum of membrane potential perturbations $v(t) = \sum_{s} u_{s}(t) $ into a firing rate $r(t)$, 
\begin{eqnarray} \label{eq:sigmoid}
  r(t) = \sigma(v(t)) = \frac{2r_0}{1+e^{k(v_{0}-v(t))}}
\end{eqnarray}
where $u_s$ are the membrane perturbations, $r_0$ is half the maximum firing rate, $v_0$ is the potential at $r=r_0$, and $k$ determines the sigmoid's slope. Nonlinear coupling gives rise to 
\textit{Signal-Envelope Coupling (SEC)} \cite{spaakLayerspecificEntrainmentGammaband2012, szczepanskiDynamicChangesPhaseamplitude2014, dvorakProperEstimationPhaseamplitude2014, mejiasFeedforwardFeedbackFrequencydependent2016, chackoDistinctPhaseamplitudeCouplings2018, bastos_laminar_2018}, the coupling of a slow signal to the envelope of a faster signal. In predictive coding, for example \cite{Ruffini2025Comparator}, a slower rhythm (encoding predictions) modulates the envelope (i.e., overall magnitude) of faster oscillations (encoding sensory data), aligning the processing of fast inputs with the slower predictive signal.  

 
Consider such a LaNMM sigmoid node in P1 or P2 acting on the sum of a slow input $s_1(t)$ and a narrowband fast input $s_2(t)\approx A_c\cos(2\pi f_c t)$. The inputs may come from other populations or recurrently from the same population hosting the sigmoid. 
A local Volterra/Taylor expansion around the operating point gives
\[
y(t)=\sigma(s_1+s_2)\approx \kappa_0+\kappa_1[s_1(t)+s_2(t)]+\tfrac{\kappa_2}{2}[s_1(t)+s_2(t)]^2+\cdots .
\]
The quadratic cross‑term $\kappa_2\,s_1(t)s_2(t)$ implements amplitude modulation (AM): by the modulation (frequency‑shift) property of the Fourier transform, multiplication by $\cos(2\pi f_c t)$ translates the spectrum of the slow modulator to symmetric sidebands around $\pm f_c$ \href{https://www.amazon.com/Signals-Systems-2nd-Alan-Oppenheim/dp/0138147574}{\cite{oppenheimSignalsSystems1997}}. Under spectral separation (Bedrosian conditions), the analytic‑signal envelope of the band‑passed carrier recovers $s_1(t)$ up to scale, providing a constructive demodulator \cite{bedrosianProductTheoremHilbert1963,boashashEstimatingInterpretingInstantaneous1992}. Such multiplicative mixing yields intermodulation (IM) components at $f_a\!\pm\! f_b$, $2f_a\!\pm\! f_b$, …—the spectral fingerprints of AM/demodulation predicted by HAM—robustly observed across visual paradigms \cite{gordonIntermodulationComponentsVisual2019, chenIntermodulationFrequencyComponents2024}. 
Functionally, AM at slow–fast interfaces is consistent with cross‑frequency coupling accounts of computation/communication \cite{canoltyFunctionalRoleCrossfrequency2010,hyafilNeuralCrossFrequencyCoupling2015} and with nonlinear mixing known from RF/microwave systems \cite{maasNonlinearMicrowaveRF2003}.

 
We next examine a single LaNMM column to evaluate how it can encode (modulate), demodulate and decode a message via HAM. See Figure~\ref{fig:LaNMM_Mod_Demod} bottom for the spectra of the P1 and P2 membrane potentials (code for the example is provided at \href{https://github.com/giulioruffini/LaNMM_predictive_coding_paper}{Github}). 

\paragraph{Modulation.} In the example in Figure~\ref{fig:LaNMM_Mod_Demod}, an external slow oscillatory input in the alpha band (mimicking a “prior” or top-down signal) was injected into the deep-layer pyramidal population P1 while the superficial layer P2 was configured to produce a sustained fast oscillation. The slow input modulates the fast oscillation in P2. In this example, the correlation between external input and P1 was $\sim$0.52, between P1 and P2 envelope $\sim$0.98, and between input and P2 envelope $\sim$0.51. 

Modulation can be undone. It has been recently shown that the LaNMM provides a candidate mechanism for the neural function of error evaluation—central to predictive coding—by comparing modulated signals across cortical layers \cite{Ruffini2025Comparator}. 
This implementation of error evaluation is in fact a demonstration of modulation removal --- v. Fig. 7 in Ruffini et al. (2025) \cite{Ruffini2025Comparator}. Modulation removal (or error evaluation) is successfully carried out by providing an appropriate prior sign-reversed version of the modulating signal. The incoming slow-band-modulated high-frequency carrier is ``unmodulated'' by an incoming slow signal with the right signal shape (a sign shift with respect to the high-frequency envelope). Prediction with the right prior demodulates the incoming bottom-up signal, leaving only the carrier.

Second and higher order modulation can be implemented in different ways. One is to again use the modulation approach above, but with a LaNMM model in which the P1 population has a slower natural frequency. P1 can then be driven by an appropriate slower signal. The other is to use envelope-envelope coupling as discussed in Ruffini et al (2025) \cite{Ruffini2025Comparator}, where it was used to encode precision in predictive coding.

\paragraph{Demodulation (envelope extraction).} Finally, it is important to be able to extract the envelope of processed signals (the message). This can be achieved with the architecture described in Figure~\ref{fig:LaNMM_Mod_Demod} (b). In our implemented example, we find that the carrier envelope can be transferred to the P1 membrane potential with a correlation $\sim$.45.  
 

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.99\linewidth]{figures/composite_modemod.png}

  \includegraphics[width=0.99\linewidth]{figures/spectra.png}
    %\includegraphics[width=0.9\linewidth]{figures/lanmm_signals.png}
  
  % \includegraphics[width=0.5\linewidth]{figures/envelope_extraction.png}
  \caption{\textbf{LaNMM architecture for modulation and demodulation.} \textbf{Top (modulation and amplitude normalization)}:
  a) The prior modulates the amplitude/frequency of a faster carrier input. The operation is undone to first order by another node with the modulator's sign-reversed signal (\textit{unmodulation}). b) Modulation and demodulation (envelope extraction). 
  Spectra of LaNMM P1 and P2 (membrane potential) driven by an alpha band input to P1. The alpha-band peak and the sidebands around the gamma peak are clearly visible, as are higher-order terms. \textbf{Bottom}: Spectra of P1 and P2 populations during modulation (as in (a)). }
  
 
  \label{fig:LaNMM_Mod_Demod}
\end{figure}

 


\section{Discussion}


 

\paragraph{Oscillations in the brain.}
Oscillations are ubiquitous in neural systems and organize computation across space and time \cite{buzsaki_brain_2023, buzsakiNeuronalOscillationsCortical2004, friesRhythmsCognitionCommunication2015,chenOscillatoryControlCortical2025}. Slow rhythms (delta–alpha/beta) tend to coordinate large-scale context and top-down control, whereas faster beta/gamma rhythms carry local content and bottom-up signals; laminar and inter-areal physiology support this spectral dissociation \cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015,spaakLayerspecificEntrainmentGammaband2012,bastosCanonicalMicrocircuitsPredictive2012,bastos_laminar_2018}. Cross-frequency coupling (CFC) provides the bridge between these scales, with slow phases modulating fast-band envelopes and timing \cite{canoltyFunctionalRoleCrossfrequency2010,lismanThetaGammaNeuralCode2013}. 


Across EEG/MEG/ECoG and intracranial recordings, canonical rhythm classes
(delta, theta, alpha, beta, gamma) occur at progressively higher center
frequencies whose separations are roughly constant on a log–frequency axis.
In practice, adjacent bands are typically separated by a factor of about
two to three (e.g., $\theta\!\sim\!4$–7\,Hz, $\alpha\!\sim\!8$–12\,Hz,
$\beta\!\sim\!15$–30\,Hz, $\gamma\!\sim\!40$–80\,Hz), yielding an
approximately geometric hierarchy of timescales \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiNeuronalOscillationsCortical2004,buzsakiRhythmsBrain2006,friesRhythmsCognitionCommunication2015,klimeschEEGAlphaOscillations2007,donoghueParameterizingNeuralPower2020}.
Penttonen and Buzsáki explicitly quantified near–log spacing in rodents
\cite{penttonenNaturalLogarithmicRelationship2003}; convergent human and
non‑human primate reviews and textbooks report comparable band centers and
boundaries, which imply ratios $r\!\approx\!2$–3 between neighboring classes
\cite{buzsakiNeuronalOscillationsCortical2004,buzsakiRhythmsBrain2006,klimeschEEGAlphaOscillations2007,friesRhythmsCognitionCommunication2015}.
Methodological work that separates periodic peaks from the aperiodic
background further confirms that these peaks recur at roughly logarithmic
intervals across individuals and tasks \cite{donoghueParameterizingNeuralPower2020}.


Recent frequency-tagging work makes this hierarchy directly observable: when two inputs are presented at distinct tagging rates, neural responses exhibit \emph{intermodulation} (IM) components at sums and differences of the tags—clear fingerprints of multiplicative mixing/demodulation predicted by HAM. This has been leveraged in steady-state visual paradigms to probe integration and attention \cite{norciaSteadystateVisualEvoked2015}. Comprehensive reviews document robust IM sidebands across visual cognition \href{https://doi.org/10.1016/j.neuroimage.2019.06.008}{\cite{gordonIntermodulationComponentsVisual2019}} and characterize their generators and applications \href{https://doi.org/10.1016/j.neuroimage.2024.120937}{\cite{chenIntermodulationFrequencyComponents2024}}. In our framework, these sidebands arise naturally from envelope–carrier architectural interactions: slow predictive rhythms (alpha/theta) multiplicatively gate fast carriers (beta/gamma), yielding IM structure around each band center while preserving separability under approximate logarithmic spacing (constant-$Q$). This same logic explains why, after constant-$Q$ preprocessing, both periodic peaks and an aperiodic $1/f^\alpha$ backbone can be read without mutual bias \cite{donoghueParameterizingNeuralPower2020,heScalefreeBrainActivity2014}.

\paragraph{Computation with oscillators.}
Beyond description, oscillations can \emph{compute}. Analog and hybrid models use coupled oscillators as primitives for signal processing and decision dynamics \cite{rietman_analog_2003}. A broader theory and hardware stack for \emph{computing with oscillators} now connects phase, frequency, and amplitude modulation to classification, constraint satisfaction, and optimization \href{https://www.nature.com/articles/s44335-024-00015-z}{\cite{todri-sanialComputingOscillatorsTheoretical2024}}. In AI, recurrent oscillator reservoirs (e.g., Kuramoto-type networks) provide nonlinear fading-memory dynamics and rich readouts for sequence learning and control \cite{yeungReservoirComputationNetworks2025,miyatoArtificialKuramotoOscillatory2025}. This may provide a route to the large energy savings realized in computational biological systems compared to artificial ones \cite{effenbergerFunctionalRoleOscillatory2025}.
Converging theory in systems neuroscience argues that neocortex itself employs recurrent oscillator networks whose computational capacity depends on the \emph{patterning and heterogeneity} of local oscillators—and that increasing heterogeneity (while avoiding overlap) enhances separability and pushes the network toward powerful operating regimes near criticality \cite{singerOscillationsNaturalNeuronal2025}. Large-scale modeling further shows how oscillatory motifs implement routing, working-memory control, and flexible task switching, with function tied to band-specific coupling and multiplexing \href{https://www.pnas.org/doi/10.1073/pnas.2412830122}{\cite{effenbergerFunctionalRoleOscillatory2025}}. 

HAM fits squarely within this computational view: it \emph{requires} oscillator heterogeneity and benefits from log-uniform (constant-$Q$) spacing to prevent IM collisions, enabling layered modulation /demodulation pipelines across cortex. In practice, this predicts that (i) modest increases in frequency diversity should improve multiplexed readout, and (ii) approximately logarithmic spacing maximizes usable channel capacity—two testable signatures in laminar/ECoG/MEG experiments and neuromorphic oscillator arrays.
In HAM, relatively fast rhythms serve as carriers whose amplitudes are multiplicatively structured by slower processes; staged demodulation then makes these envelopes readable downstream. This implements a neural analogue of multi‑layer AM in communication engineering, but realized in laminar circuits. 

\paragraph{From analog/digital representations to hierarchical envelopes.}
Information in engineered systems may be encoded digitally (symbolic, discrete) or analogically (continuous waveforms). Neural systems are fundamentally analog at the field level, with information carried both by oscillatory signals themselves and by their amplitude envelopes—the latter being a natural vehicle for slow context that modulates faster content \cite{buzsakiNeuronalOscillationsCortical2004}. Auditory ECoG demonstrates this duality: high‑gamma envelopes track the stimulus envelope and support direct speech reconstruction \cite{pasleyReconstructingSpeechHuman2012,tamuraCorticalRepresentationSpeech2023}. HAM formalizes a coarse‑to‑fine organization in which information can reside in the signal and in successive envelopes (envelope‑of‑envelope, etc.), yielding a principled multi‑scale code. 
Debates on neural coding (rate, temporal/ spike‑timing, phase, and burst codes) converge on the view that multiple encoding schemes coexist and are multiplexed across scales \cite{zeldenrustNeuralCodingBurstsCurrent2018,carianiTimeEssenceNeural2022,lismanThetaGammaNeuralCode2013}. In this light, HAM specifies \emph{one} concrete way slow rhythms coordinate fast content: theta/alpha phases gate gamma‑band carriers (rate/temporal information) via gain control, with nested envelopes enabling compositional, layered messages. This aligns with ``communication‑through‑coherence'' and the spectral dissociation of feedforward/feedback pathways \cite{friesRhythmsCognitionCommunication2015,vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}. More broadly, oscillator‑based computation is increasingly recognized as a viable computing paradigm; HAM connects that literature to cortical field dynamics \cite{todri-sanialComputingOscillatorsTheoretical2024, effenbergerFunctionalRoleOscillatory2025,singerOscillationsNaturalNeuronal2025}.

\paragraph{A rationale for log spacing.}
Intermodulation at each stage produces sideband clusters around every center frequency. Avoiding spectral collisions across levels requires the span of slower combinations to fit inside the guard band of faster clusters. A strict super‑increasing hierarchy $f_k>2 \sum_{j>k}f_j$ suffices to keep clusters disjoint; geometric spacing $f_{k+1}=f_k/r$ satisfies this with margin for $r\gtrsim $ 2--3 and yields constant fractional bandwidths (constant-$Q$), making demodulation scale‑invariant and mirroring classical filterbanks and cochlear tiling \cite{brownCalculationConstantSpectral1991}. The minimal ratio depends on the number of modulating layer ($r=2$ is the minimum for a single carrier/modulator AM arrangement, see Figure~\ref{fig:minimal_r} and Appendix~\ref{app:math} for details). Exact ratios are not essential—a weaker non‑overlap inequality already pushes the ensemble toward a log‑uniform density of centers. Empirically, $r \approx 2$--$3$ lies near the theoretical $r > $ 2--3 threshold, explaining why real bands show limited but tolerable overlap. 

\paragraph{Two routes to the aperiodic $1/f^\alpha$.}
HAM predicts $1/f^\alpha$ via two complementary mechanisms. (i) \emph{Cascade route:} with log spacing and small, quasi‑uniform modulation depth $m$, $k$‑modulator terms scale $\sim(m/2)^k$; because $k\!\sim\!\log_r(f_1/f)$, $P(f)\propto f^{-\alpha}$ with $\alpha=2\ln(2/m)/\ln r$. This links slope to circuit variables, predicting how $\alpha$ should shift with changes in $m$ or $r$. (ii) \emph{Mixture route:} a log‑uniform bank of narrowband (constant‑$Q$) oscillators already yields $\sim 1/f$ in expectation (equal power per octave). Random AM pairings preserve this backbone in the interior but induce predictable edge deviations: a small‑difference pedestal at low $f$ and a finite‑support roll‑off at high $f$. Analysis choices (edges; constant‑Hz vs.\ constant‑$Q$ smoothing) determine whether fitted $\hat\alpha$ flattens or steepens over finite windows \cite{donoghueParameterizingNeuralPower2020,heScalefreeBrainActivity2014}.

\paragraph{Mechanistic implementation.} Here we provided an implementation proof-of-concept of amplitude modulation in the LaNMM, with base frequencies in tha alpha and gamma band, although other frequencies can be implemented in this model \cite{Ruffini2025Comparator} and related ones \cite{davidNeuralMassModel2003} by adjusting parameters. 
In LaNMM, PING‑like fast generators and JR‑like slow generators map naturally onto ``carrier'' and ``envelope'' roles, with demodulation operationalized by analytic‑signal envelopes \cite{boashashEstimatingInterpretingInstantaneous1992}. These choices are consistent with laminar spectral asymmetries—gamma favoring feedforward content and alpha/beta favoring feedback predictions \cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}. 
The generation of other frequencies may involve larger scale mechanisms, e.g., thalamo-cortical loops, but neural mass models have been proposed for most the canonical bands (see Table~\ref{tab:frequencies}.

 

\paragraph{Predictions.}
A core prediction of multiplicative envelopes besides approximate geometric spacing is \emph{intermodulation arithmetic}: sum/difference terms ($f_1\pm f_2$, $2f_1\pm f_2$, \dots) should emerge when two tagged inputs interact nonlinearly or when demodulation occurs. This is precisely what steady‑state visual paradigms report and exploit \cite{norciaSteadystateVisualEvoked2015,chenIntermodulationFrequencyComponents2024}. Intermodulation components (IMs) are now used to assay integrative neural mechanisms from low‑ to high‑level vision, attention, and multisensory binding, strengthening the case for HAM‑like processing \href{https://doi.org/10.1016/j.neuroimage.2019.06.008}{\cite{gordonIntermodulationComponentsVisual2019}}. In laminar circuits, HAM further predicts superficial carrier‑dominant activity coexisting with deeper envelope‑dominant readouts, and cross-frequency coupling, matching reported interactions \cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}.

HAM makes direct, perturbation‑level predictions. (i) \emph{Spacing:} approximately geometric spacing should minimize cluster collisions and facilitate staged demodulation. (ii) \emph{IM scaling:} the amplitude of IM terms scales with products of modulation depths, providing a calibration handle in frequency‑tagged designs. (iii) \emph{Laminar dissociation:} superficial layers should carry fast carriers; deeper layers should track their envelopes in prediction‑heavy contexts. (iv) \emph{Perturbations:} reducing a slow rhythm’s amplitude should reduce the envelope variance of faster activity it modulates (steepening the PSD), whereas entraining a slow rhythm should increase that variance (flattening the PSD). These can be tested with closed‑loop stimulation and preregistered laminar/ECoG/MEG paradigms, coupled to LaNMM‑based parameter inference to link controlled changes in $m$ and $r$ to shifts in $\alpha$.  Constant‑$Q$ preprocessing and explicit separation of periodic vs.\ aperiodic components are essential to stabilize slope estimates and avoid spurious CFC \cite{donoghueParameterizingNeuralPower2020}. 

\paragraph{Relation to alternative accounts.}
Several mechanistic accounts have been proposed for observed frequency spacing. One line argues that approximately logarithmic spacing follows from hierarchical anatomy and finite conduction delays: excitability windows and integration times scale with the oscillation period, allowing circuits with heterogeneous axonal/synaptic delays and sizes to coordinate \cite{vereshchaginAlgorithmicStatisticsForty2017}. In this view, faster rhythms remain local while larger networks synchronize more slowly, consistent with evolutionary preservation of a hierarchy of timescales \cite{buzsakiScalingBrainSize2013}, a posterior–anterior cortical frequency gradient in humans \cite{mahjooryFrequencyGradientHuman2020}, and whole‑brain models in which distance‑dependent conduction delays generate alpha‑band phase networks \cite{williamsInfluenceInterregionalDelays2023}. Complementarily, spacing as equal steps on a log‑frequency axis has been argued to minimize mutual entrainment/cross‑talk and to reflect hierarchical network architecture—shown in reverse‑engineered networks that reproduce log‑spaced spectral peaks \cite{steinkeBrainRhythmsReveal2011}. Recent formal work refines this optimization idea: “golden rhythms” place neighboring centers at the golden ratio to maximize multiplexing (minimal interference) while preserving efficient triadic cross‑frequency interaction \cite{kramerGoldenRhythmsTheoretical2022}; related analyses propose a binary \(1{:}2\) hierarchy (and a “golden‑mean rule”) linking brain and body oscillations and predicting harmonic/nested coupling across scales.  Klimesch’s binary hierarchy theory posits that cross‑frequency interactions are governed by two principles—phase–to–amplitude (envelope) modulation and phase–phase (harmonic) coupling—applied to brain and body oscillations arranged on a 1:2 ladder \cite{klimeschFrequencyArchitectureBrain2018a}.

A parallel mechanistic route is self‑organization: with activity‑dependent plasticity, neural populations can split into a few frequency clusters whose slower envelopes modulate faster components, offering a circuit basis for band “quantization” \cite{rohrFrequencyClusterFormation2019,socolovskyRobustRhythmogenesisSpiketimingdependent2021,luzOscillationsSpikeTimingDependent2016}. Methodologically, the apparent discreteness strengthens after modeling out the aperiodic \(1/f\) background and analyzing burst‑wise dynamics—revealing approximately constant‑\(Q\) spacing of putative peaks \cite{donoghueParameterizingNeuralPower2020}—and recent state‑space reconstructions report a low‑dimensional geometric “core” that organizes band interactions \cite{pourdavoodEEGSpectralAttractors2024}). '

HAM differs from the above by focusing on the consequences of hiearchical amplitude modulation for information encoding. It does not derive the spacing primarily from biophysical delays or carrier interference‑avoidance; instead, it treats the band hierarchy as a normative, codebook‑like quantization emerging from the model’s objective (clean, scale‑invariant multiplexing under explicit constraints), yielding a specific multiplicative scheme and falsifiable predictions about when/where centers shift while preserving constant‑\(Q\) structure. Our results show how nested envelopes can carry specific information and yield testable spectral consequences.
 HAM starts from the central role of amplitude modulation but generalizes the spacing: intermodulation sidebands impose guard‑band constraints that are satisfied by \emph{any} approximately geometric (constant‑$Q$) spacing with ratio $r\gtrsim$ 2-3 . 
%Critically, HAM predicts explicit intermodulation fingerprints (sum/difference components) in frequency‑tagging paradigms, a prediction borne out across visual cognition \cite{gordonIntermodulationComponentsVisual2019, chenIntermodulationFrequencyComponents2024}, and maps naturally onto laminar carriers vs.\ envelopes (gamma feedforward; alpha/beta feedback) \cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}.
It also offers a constructive path to fractal‑like spectra by structurally favoring criticality. It is compatible with signatures of near‑critical dynamics (avalanches, scale‑invariant correlations). Combining HAM, time‑constant, or E–I explanations will require perturbational tests and careful finite‑size controls—not spectral fits alone \cite{milotti1NoisePedagogical2002,bedardMacroscopicModelsLocal2009}. 

 Finally, prior work in signal processing has noted that constant‑$Q$ (log‑spaced) filterbanks with equal power per octave imply an overall $1/f$ (“pink‑noise”) backbone, and that cascade constructions (e.g., mixtures of relaxation times or multiplicative cascades) naturally generate $1/f^\alpha$ spectra \cite{brownCalculationConstantSpectral1991,keshner1Noise1982,duttaLowfrequencyFluctuationsSolids1981,milotti1NoisePedagogical2002,kaulakysStochasticNonlinearDifferential2004,bacryMultifractalRandomWalk2001}.
We place that in a neural context and further link it to predictive coding dynamics (with deep layers modulating superficial layers).  HAM can be viewed as a mechanistic complement to descriptive models of $1/f$ noise (e.g., fractional Gaussian noise, self-organized criticality). Rather than assuming an ad hoc fractal process, HAM builds $1/f$ from interacting oscillatory components with clear physiological mapping. 
Beyond qualitative arguments for $1/f$, HAM derives two mechanistic routes—(i) a multiplicative cascade linking slope $\alpha$ to modulation depth $m$ and spacing $r$, and (ii) a mixture route from aggregating log‑spaced narrowband oscillators \cite{donoghueParameterizingNeuralPower2020}. 

\noindent\textbf{Predictive coding link.}
HAM provides a hierarchical backbone for predictive coding. As prposed in Ruffini et al. (2025) \cite{Ruffini2025Comparator}, in hierarchical predictive coding, deep populations provide slow predictions $p_k(t)$ while superficial populations carry content in their envelopes. Error evaluation at level $k$ is a signed \emph{unmodulation}:
\begin{equation}
\varepsilon_k(t)\;=\;\underbrace{\mathcal U_{f_k}\!\bigl[x(t)\bigr]}_{\text{estimated envelope at }k}\;-\;p_k(t),
\label{eq:pc-error}
\end{equation}
where $x(t)$ is the bottom-up signal, $p_k(t)$ the prediction for the $k$-th layer, and $\mathcal U$ stands for unmodulation at frequency $f_k$ --- i.e.,\ envelope extraction followed by subtraction (the “Comparator”). Laminar data support this mapping: gamma‑band activity dominates feedforward channels and
alpha/beta envelopes dominate feedback, consistent with deep‑layer envelope readout
and staged demodulation across the hierarchy
\cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}.
Thus, modulation order is immaterial (product structure), whereas unmodulation
should proceed fast$\rightarrow$slow (or in parallel constant‑$Q$ channels) to expose the nested envelopes that predictive circuits compare to their top‑down forecasts (see Appendix~\ref{sec:hierarchical_am}).


\paragraph{Outlook.}
Hierarchical Amplitude Modulation offers a coherent explanation linking multiple scales of neural dynamics. By requiring logarithmic frequency spacing and limited modulation depth, HAM ensures that oscillatory bands can nest without losing their identity, providing a multi-layer carrier for information. The framework elegantly accounts for the origin of 1/f spectra as a consequence of either deep cascades or aggregated log-oscillators. It connects to laminar circuit anatomy, suggesting that the cortex’s layered structure is not just a feedforward/feedback arrangement but also a modulation pipeline for neural signals. There are many avenues to build on this work: incorporating more realistic spiking dynamics, exploring how noise and oscillations interplay in HAM (since external noise could either disrupt or be filtered by such a system), and extending to other frequency domains (e.g., could very slow $<1$~Hz fluctuations modulate high-frequency oscillations across brain-wide networks?). The HAM model provides clear predictions and a language to discuss how the brain’s rhythms might interact to encode information. As neuroscience moves toward understanding the brain as a multi-scale communications network, HAM may offer a valuable piece of the puzzle, bridging theories from engineering with the biological reality of brain oscillations.

\section*{Funding}
 Giulio Ruffini  and Francesca Castaldo are funded by the European Commission under European Union’s Horizon 2020 research and innovation programme Grant Number 101017716 (Neurotwin) and European Research Council (ERC Synergy Galvani) under the European Union’s Horizon 2020 research and innovation program Grant Number~855109. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 

\bibliographystyle{unsrt}

\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% A P P E N D I X %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage
\clearpage
\appendix

\section{Mathematical details}
\label{app:math}

 AM radio operates by encoding information in the amplitude of a high-frequency carrier wave. In this context, “information” typically refers to the audio signal we want to transmit, such as music or voice. This information is represented as a low-frequency signal, which modulates or varies the amplitude of the high-frequency carrier wave. The carrier itself oscillates at a frequency much higher than the signal, enabling it to be transmitted over long distances efficiently.
During modulation, the carrier’s amplitude is adjusted to follow the waveform of the audio signal, embedding the desired information within the carrier’s envelope. When received, this combined signal undergoes demodulation to extract the original audio information from the carrier. Demodulation detects variations in amplitude and reconstructs the original low-frequency signal. This process highlights how AM radio relies on distinct components: a steady carrier, the modulating information signal, and demodulation to recover the original information for playback. 
The \textit{carrier signal} \( C(t) \) can be expressed as $
C(t) = A_c \cos(2 \pi f_c t)
$, 
where \( A_c \) is the amplitude of the carrier, and \( f_c \) is the carrier frequency.
  To encode the information, the amplitude of the carrier is varied according to the lower-frequency \textit{information signal} \( m(t) \), producing the \textit{modulated signal }\( S(t) \),
  \begin{equation}\label{eq:am_model}
  S(t) = A_c\,\bigl[\,1 + a\,m(t)\bigr]\cos\bigl(2\pi f_c t\bigr), 
  \end{equation}
  where \( a\, m(t) \) is scaled such that \( 1 + a\, m(t) \) remains positive, ensuring no distortion occurs during modulation.
  Here, $A_c$ is the carrier amplitude,
    $a$ is the \emph{modulation index},
    $m(t)$ is the baseband (modulating) signal with frequency content up to $f_m \ll f_c$, and 
    $f_c$ is the carrier frequency.
This generic model subsumes both single-tone AM (where $m(t)=\cos(2\pi f_m t)$) and multi-tone/wideband modulations, as long as the maximum modulating frequency is less than $f_c$.
At the receiver, a \textit{demodulation} process retrieves the original information signal \( m(t) \) by detecting variations in the amplitude (or envelope) of \( S(t) \).
% \end{tcolorbox}
% }
%\end{figure}


 
While standard AM encoding relies on a single layer of modulation, where the information signal \( m(t) \) modulates the amplitude of a high-frequency carrier, this concept can be extended to a \textit{hierarchical framework in which multiple layers of modulation occur at different timescales (HAM.} In HAM, each modulating signal not only modifies the carrier but also modulates the envelope of another modulating signal at a slower timescale, forming a cascade of amplitude modulations across hierarchical frequency bands.


 

\subsection{HAM and logarithmic spacing of modulating bands} 
\label{sec:hierarchical_am}

Hierarchical Amplitude Modulation (HAM) refers to a nested modulation scheme in which a high-frequency carrier is amplitude-modulated by a signal, whose amplitude in turn is modulated by another slower signal, and so on across multiple layers. In essence, information is encoded at multiple hierarchical levels of the amplitude envelope. We describe this concept step by step, analyze the resulting spectra at each stage, and show why a logarithmic (geometric) spacing of carrier and modulation frequencies naturally emerges to prevent spectral overlap, as observed in electrophysiological signals \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiNeuronalOscillationsCortical2004} (see Figure~\ref{fig:logarithmic_hierarchy}). Finally, we discuss general insights into how such a structure can be demodulated and generalized.



\paragraph{Definition (single HAM cascade).}
In neural circuits, there is no singled-out ``carrier''. We therefore treat the fastest oscillator as any other building block in a multiplicative cascade around a positive baseline.
Let the highest frequency (``carrier") signal be
\[
c(t)=A_c [1+\cos(2\pi f_0 t + \phi_0)],
\]
and let \(f_0>f_1>\cdots>f_N\) be modulating tones. A \textit{depth‑\(N\) hierarchical
amplitude‑modulated signal} is
\begin{equation} \boxed{
s_N(t)
= A_c \prod_{i=0}^{N}\bigl[1+m_i\cos(2\pi f_i t+\phi_i)\bigr] 
\qquad 0<m_i<1 .}
\label{eq:ham_product}
\end{equation}

 
By expanding~\eqref{eq:ham_product} and using 
  the trigonometric identity $\cos A \cos B = \frac{1}{2} [\cos(A+B) + \cos(A-B)]$, this introduces additional sidebands.
The (one‑sided) spectrum contains lines at
\begin{equation}
f \in \Bigl\{\, \sum_{i=0}^{N} a_i f_i \;:\; a_i\in\{-1,0,1\}\Bigr\}.
\label{eq:support}
\end{equation}
At lowest order (small $m_i$), the first two layers ($i=0,1$) contribute
$
\pm f_0$, $\pm f_1$, and $\pm(f_0\pm f_1),
$
and in general, a term involving $k$ nonzero coefficients has amplitude proportional to $\prod_{i:\,a_i\neq 0}(m_i/2)$.
 

 
At depth \(N\), the number of algebraic combinations is at most \(3^N\);
counting only distinct positive‑frequency lines (ignoring DC and mirror symmetry)
gives an upper bound \(\le (3^N-1)/2\). Degeneracies occur when
frequencies are commensurate.

\begin{figure}[t]
\centering
\begin{tikzpicture}[>=Latex, node distance=20mm]
 % Nodes (top to bottom; roughly log-spaced)
 \node (f0) at (0,0)   {$f_0\!=\!128\,$Hz};
 \node (f1a) at (-2.2,-1.6) {$f_{1a}\!=\!42.7\,$Hz};
 \node (f1b) at ( 2.2,-1.6) {$f_{1b}\!=\!36.0\,$Hz};
 \node (f2a) at (-3.2,-3.2) {$f_{2a}\!=\!14.2\,$Hz};
 \node (f2b) at ( 0.0,-3.2) {$f_{2b}\!=\!12.0\,$Hz};
 \node (f3) at (-1.2,-4.8) {$f_{3}\!=\!4.7\,$Hz};

 % Edges (allowed modulations)
 \draw[->] (f0) -- (f1a);
 \draw[->] (f0) -- (f1b);
 \draw[->] (f1a) -- (f2a);
 \draw[->] (f1a) -- (f2b);
 \draw[->] (f1b) -- (f2a);
 \draw[->] (f2a) -- (f3);
 \draw[->] (f2b) -- (f3);

 % One highlighted path (a particular HAM cascade)
 \draw[line width=1.6pt] (f0) -- (f1a);
 \draw[line width=1.6pt] (f1a) -- (f2a);
 \draw[line width=1.6pt] (f2a) -- (f3);

 % Legend-like labels
 \node[anchor=west] at (2.8,-4.6) {Path $p$: $f_0\!\to\!f_{1a}\!\to\!f_{2a}\!\to\!f_3$};
 \node[anchor=west] at (2.8,-5.1) {$x_p(t)=A_0\prod_{v\in p}\!\bigl[1+m_v\cos(2\pi f_v t+\phi_v)\bigr]$};
\end{tikzpicture}
\caption{Directed graph of oscillators (nodes) and allowed modulations (arrows). 
A highlighted path $p$ represents one hierarchical AM cascade; the full signal 
is the sum over all such paths (Eq.~\eqref{eq:pathsum}).} \label{fig:graph}
\end{figure}


\paragraph{Graph-based HAM.} Here we analyze the more general case where multiple HAM chains interact. 
Let $G=(V,E)$ be a directed acyclic graph (DAG) of oscillators (Fig.~\ref{fig:graph}).
An edge $(v\!\to\!u)\in E$ means that the slower node $v$ \emph{modulates} the faster node $u$.
Each node $u\in V$ has a carrier $c_u(t)=\cos(2\pi f_u t+\phi_u)$ and gain $A_u>0$.
Multiple incoming modulators \emph{sum first} and then modulate the carrier at $u$.

\emph{Presynaptic drive and node output.}
Define the (dimensionless) presynaptic drive
\begin{equation}
I_u(t)\;=\;\sum_{v\in\mathrm{In}(u)} m_{v\to u}\, s_v(t),
\qquad
\mathrm{In}(u)=\{\,v:(v\!\to\!u)\in E\,\}.
\label{eq:drive}
\end{equation}
Here $s_v(t)$ is the signal exported by $v$ that serves as a modulator (e.g., a narrowband
$s_v(t)=\cos(2\pi f_v t+\phi_v)$ or, more generally, the envelope at $v$).
The node output is
\begin{equation}
x_u(t)\;=\;A_u\,[\,1+\mathcal{H}_u(I_u(t))\,]\,c_u(t),
\qquad
\mathcal{H}_u(\xi)=\sum_{q\ge 1} h^{(q)}_u\,\xi^q,\ \ h^{(1)}_u=1,
\label{eq:node_output}
\end{equation}
and the network signal is the superposition $S(t)=\sum_{u\in V} x_u(t)$.
Using \eqref{eq:node_output} and the Taylor series of $\mathcal{H}_u$,
\begin{equation}
x_u(t)
\;=\;
A_u\,c_u(t)\,
\sum_{q\ge 0} \tilde h^{(q)}_u
\!\!\!\sum_{(v_1,\dots,v_q)\in \mathrm{In}(u)^q}\!\!\!
\Bigl(\prod_{j=1}^{q} m_{v_j\to u}\Bigr)\,
\Bigl(\prod_{j=1}^{q} s_{v_j}(t)\Bigr),
\qquad
\tilde h^{(0)}_u=1,\ \tilde h^{(q)}_u=h^{(q)}_u\ (q\!\ge\!1).
\label{eq:pathsum}
\end{equation}
Recursively substituting the same form for each $s_{v_j}(t)$ yields a convergent
sum over \emph{finite directed forests} in the DAG (finite because $G$ is acyclic
and $m_{v\to u}$ are small), i.e., a global sum of time‑domain products whose coefficients
are monomials in the edge depths $m_{e}$.

\textit{Spectral support.}
Assume as before that each exported modulator has a narrowband component at $f_v$ 
(e.g., $s_v(t)=\cos(2\pi f_v t+\phi_v)$ at leaves, and more generally $s_v$ inherits the
narrowband content of its ancestors). Then every term in \eqref{eq:pathsum} is a product of cosines.
Using $\cos A\cos B=\tfrac12[\cos(A+B)+\cos(A-B)]$ iteratively gives again a spectrum as in Equation~\ref{eq:support}. To see this, order the DAG topologically. For each $u\in V$, let $\mathrm{Anc}(u)$ be its ancestors.
Then all non‑DC spectral lines of $S(t)$ lie in
\[
\Omega
\;\subseteq\;
\bigcup_{u\in V}\Bigl\{
\ \bigl|\,f_u+\sum_{v\in F}\sigma_v f_v\,\bigr|
:\ F\subseteq \mathrm{Anc}(u)\ \text{finite},\ \sigma_v\in\{-1,+1\}
\Bigr\}
\]
(with mirrored negative frequencies), i.e., \emph{linear combinations with coefficients in $\{-1,0,+1\}$}. That is, Equation~\ref{eq:support}
 holds. Amplitudes are polynomials in the $m_{e}$ (products of edge depths on the contributing
forests) times node factors $A_u,\tilde h^{(q)}_u$.
 

% \emph{Proof.} By induction on topological depth. If $u$ is a source, $x_u(t)=A_uc_u(t)$
% has a line at $f_u$. Suppose all $x_v$ for $v\in\mathrm{In}(u)$ only contain signed sums of their
% ancestors. Multiplying by $c_u(t)$ shifts each narrowband component by $\pm f_u$, and
% summing across parents preserves the signed‑sum structure. Higher powers in \eqref{eq:series}
% produce additional \(\pm\) sums but no new coefficient set beyond $\{-1,0,1\}$.
\paragraph{Spacing constraints: necessary and sufficient.}
Let $f_1>f_2>\cdots>f_N>0$ and let the sideband cluster around $f_k$ be
$\mathcal{C}_k \subset [\,f_k-S_k,\; f_k+S_k\,]$ with
$S_k=\sum_{j>k} f_j$. We now show that $\{\mathcal{C}_k\}$ are pairwise disjoint
iff $
f_k \;>\; 2\sum_{j>k} f_j \quad \text{for all } k.
$

\vspace{1em}
% In the geometric case $f_{k+1}=f_k/r$, this is equivalent to $r>3$, as we show next.


\begin{theorem}
\textbf{Necessary and sufficient condition for non-overlap of clusters.}
Let $f_1>f_2>\cdots>f_N>0$ and define the half-width
$S_k:=\sum_{j>k} f_j$, and the (one-sided, positive-frequency) sideband cluster
\[
\mathcal{C}_k \;:=\; [\,f_k - S_k,\; f_k + S_k\,] \subset \mathbb{R}_{\ge 0}.
\]
Then the family $\{\mathcal{C}_k\}_{k=1}^N$ is pairwise disjoint (strictly, i.e., no touching) if and only if
\begin{equation}
\label{eq:necessary_sufficient}
\boxed{\quad f_k \;>\; 2\sum_{j>k} f_j \quad \forall k. \quad}
\end{equation}
\end{theorem}
\begin{proof}
\emph{Support envelope.}
By construction of the HAM spectrum, every line in the ``$k$-th cluster'' has the form
$f = f_k + \sum_{j>k} a_j f_j$ with $a_j\in\{-1,0,1\}$. Hence the smallest/largest possible
values in that cluster are obtained by taking all signs negative/positive, and thus
\[
\mathcal{C}_k \subseteq [\,f_k - S_k,\; f_k + S_k\,], \qquad S_k=\sum_{j>k} f_j.
\]
(For the interval proof it suffices to work with these extremal bounds.)

\smallskip
\noindent
\emph{Sufficiency.}
Assume \eqref{eq:necessary_sufficient}. Then $\forall k$, $f_k>2S_k$ and
\[
f_k - S_k \;>\; S_k \;=\; f_{k+1} + S_{k+1}
\]
Therefore the entire $k$-th interval lies strictly to the right of the $(k+1)$-st interval:
$\mathcal{C}_k\cap\mathcal{C}_{k+1}=\varnothing$.
Since $f_1>\cdots>f_N$ orders the intervals on the real line, disjointness of all adjacent pairs
implies the whole family $\{\mathcal{C}_k\}$ is pairwise disjoint.

\smallskip
\noindent
\emph{Necessity.}
Conversely, suppose the clusters are pairwise disjoint. In particular,
$\mathcal{C}_k$ and $\mathcal{C}_{k+1}$ do not intersect, so
\[
f_k - S_k \;>\; f_{k+1} + S_{k+1}.
\]
Using $S_k = f_{k+1} + S_{k+1}$ this reduces to $f_k - S_k > S_k$, i.e.
$f_k > 2 S_k = 2\sum_{j>k} f_j$. Since this holds for each adjacent pair, it holds for all $k$,
proving \eqref{eq:necessary_sufficient}.
\end{proof}

\paragraph{Corollary (Geometric spacing).}
If for all $k$ (log-uniform or geometric spacing), 
\begin{equation}
f_{k+1}=\frac{f_k}{r} \quad\Longleftrightarrow\quad f_k=\frac{f_0}{r^k}.%\qquad r>1.
\label{eq:geom}
\end{equation}
then
\[
S_k=\sum_{j>k} f_j \;=\; \frac{f_k}{r-1}\quad\text{(in the infinite-depth limit),}
\]
hence the condition $f_k>2S_k$ is equivalent to $r>3$
$$
f_k > 2 \frac{f_k}{r-1}
$$
or 
$r-1>2$. 
For finite depth $N$,
$S_k=\frac{f_k}{r-1}\bigl(1-r^{-(N-k)}\bigr)$, so $r\gtrsim 3$ suffices in practice with slack set
by depth, bandwidths, and modulation indices. More precisely,
$$
f_k > 2 \frac{f_k}{r-1}\bigl(1-r^{-(N-k)}\bigr)
$$
% or
% $$
% r-1 > 2-2r^{-(N-k)}
% $$
or
$$
r>3-2r^{-(N-k)}
$$

\paragraph{Minimum geometric spacing for finite cascades.}
For a depth-$N$ geometric ladder $f_{k+1}=f_k/r$ ($k=0,\dots,N$), the strongest
cluster non-overlap and signed-representation uniqueness condition
$f_k>2\sum_{j>k}f_j$ reduces to a single tight inequality at the top level:
\[
r \;>\; 3 - 2\,r^{-N}.
\]
Hence the minimum admissible ratio $r_{\min}(N)$ is the unique root $>1$ of
$r=3-2r^{-N}$. In the infinite-depth limit this yields $r>3$; for finite depth,
$r_{\min}(N)$ approaches $3$ from below and is well approximated by
$r_{\min}(N)\approx 3-2/3^{N}$ (error $<10^{-2}$ for $N\ge4$).
Explicitly $r_{\min}(1) =2$, $r_{\min}(2)=1+\sqrt{3}\approx 2.732$, $r_{\min}(3)\approx 2.920$,
$r_{\min}(4)\approx 2.975$, and $r_{\min}(5)\approx 2.992$ (see Figure~\ref{fig:minimal_r}).
For comparison, the weaker super-increasing constraint $f_k>\sum_{j>k}f_j$
admits the bound $r_{\min}^{\text{SI}}(N)$ solving $r=2-r^{-N}$, which tends to $r>2$ as $N\to\infty$.

\begin{figure}
  \centering
  \includegraphics[width=0.75\linewidth]{figures/minimal_r.png}
  \caption{Minimal spacing ratio to avoid overalap as a function of the number of modulation layers. The case $N=1$ represents the simples, with a carrier and modulation signal (modulation depth is 1).
  \label{fig:minimal_r}}
  %https://colab.research.google.com/drive/1s7Jsy-yagTC32LMnx6sxXWphrAreW_xF#scrollTo=Hl9s6OFDlQl7
\end{figure}
 

\paragraph{Remark (Why $f_k>\sum_{j>k}f_j$ is \emph{not} enough).}
The weaker ``super-increasing'' condition $f_k>\sum_{j>k}f_j$ guarantees that no \emph{purely
slower} combination reaches $f_k$, but it does not exclude overlap of the \emph{clusters} because
$\mathcal{C}_k$ extends down to $f_k-S_k$ while $\mathcal{C}_{k+1}$ extends up to $f_{k+1}+S_{k+1}=S_k$.
When $f_k\le 2S_k$, these bounds meet or overlap. For example, with $f_1=6.25, f_2=2.5, f_3=1$
($r=2.5$) we have $S_1=3.5$, so $\mathcal{C}_1=[2.75,\,9.75]$ and $\mathcal{C}_2=[1.5,\,3.5]$ overlap
on $[2.75,\,3.5]$ despite $f_1>S_1$.


\paragraph{Constant-\(Q\) and practical benefits.}
Geometric spacing yields equal steps on a log-frequency axis and approximately constant fractional bandwidth across levels (constant-\(Q\)), which (i) preserves separability of sideband clusters, and (ii) enables scaleable filter/demodulator design. This mirrors classical constant-\(Q\) filterbanks and cochlear spacing \cite{brownCalculationConstantSpectral1991}, and matches the empirical near‑logarithmic arrangement of canonical neural bands \cite{penttonenNaturalLogarithmicRelationship2003,buzsaki_brain_2023}.


 

Log spacing in~\eqref{eq:geom} affords constant‑$Q$ filtering --- meaning each band’s $Q = \frac{\text{center frequency}}{\text{bandwidth}}$ is identical ---, so the signal is naturally demodulable in stages: bandpass around $f_0$ and apply an envelope detector to recover a composite modulation dominated by $\{f_1,f_2,\dots\}$; iterating (envelope of envelope) successively reveals $\{f_1,f_2,\ldots\}$ at lower and lower centers. The analytic‑signal formalism
\[
z(t)=s(t)+j\,\mathcal{H}\{s(t)\},\qquad a(t)=|z(t)|
\]
provides a standard implementation of amplitude extraction at each stage \cite{boashashEstimatingInterpretingInstantaneous1992}.




 




 

 

 
 




 

This hierarchical modulation structure results in nested envelopes that impose amplitude fluctuations at different timescales. As seen in the generated hierarchical modulation plots (see Figure~\ref{fig:ham_example}), a low-frequency modulator introduces a broad envelope variation, a higher-frequency modulator superimposes faster oscillations within that envelope, and so forth. The final signal thus encapsulates a multi-timescale structure where rapid oscillations are embedded within progressively slower amplitude modulations.




\paragraph{Bandwidth and modulation index considerations:}
In a multi-layer design, each modulation layer $k$ will effectively use up a certain bandwidth around each spectral line it modulates. If $m_k$ is large (deep modulation), the sidebands carry more power and one might allow a slightly larger bandwidth occupancy for that layer (for example, if the modulating signal is not a pure tone but has its own small bandwidth). However, too large $m_k$ can cause nonlinear distortion (for AM, $m_k>1$ leads to signal clipping/inversion) which can create additional unwanted spectral lines. Thus, typically one keeps $m_k \le 1$ and often $m_k \ll 1$ for higher layers so that those sidebands remain small and confined. The no-overlap condition imposes a trade-off: higher-layer modulations can occupy only a fraction of the spacing left by the layer above. 
% In practice, one might design the modulation frequencies such that
% \[
% \frac{f_{k+1}}{f_k} < \gamma
% \]
% for some $\gamma < 1$ (e.g.\ $\gamma \approx 0.5$), and choose $m_k$ accordingly small, to ensure spectral clusters from different layers do not intrude on each other. This approach mirrors the strategy used in frequency planning to avoid intermodulation interference: one leaves guard bands so that no intermodulation product lands on top of an existing signal. Indeed, in RF systems manufacturers often recommend a minimum frequency margin between any two transmitters such that third-order intermodulation products cannot coincide with another carrier frequency. Similarly, our hierarchical modulation frequencies should be spaced with sufficient margin (relative to their bandwidths) to avoid any overlap of sideband ``images.''

% Mathematically, if we treat all modulation frequencies as independent (incommensurate), the spectral lines in Eq.\,\eqref{eq:hierarchical_freqs} will be distinct. However, distinct is not enough – we also want them well-separated to be easily filterable. That is why a geometric progression of frequencies is advantageous, as we discuss next.

% \paragraph{Derivation of logarithmic spacing:}
% A key result from the above conditions is that an approximately logarithmic spacing of the carrier and modulation frequencies naturally arises as the optimal way to maintain a consistent separation between sideband clusters at each layer. By logarithmic spacing, we mean that the center frequencies at successive layers are in a constant ratio (geometric progression). Let us denote the ratio between a carrier and its modulation frequency as $r$. For example, in a two-layer scheme, $r_1 = f_c/f_1$ and $r_2 = f_1/f_2$. If we desire self-similar spectral separation at each layer, we can set these ratios equal (and similarly for further layers),
% \[
% r_1 = r_2 = \cdots = r,
% \]
% so that 
% \[
% f_1 = \frac{f_c}{r},\quad f_2 = \frac{f_1}{r} = \frac{f_c}{r^2},\quad f_3 = \frac{f_c}{r^3},
% \]
% and in general $f_k = f_c/r^k$. This geometric series of frequencies corresponds to equal spacing on a logarithmic frequency axis (e.g.\ each layer might be one octave or one decade apart if $r=2$ or $r=10$, respectively). Logarithmic spacing directly implies a constant proportional bandwidth or Quality-factor: the ratio of frequency to bandwidth remains the same for each layer. In fact, using a logarithmic frequency scale produces a set of bands that form a constant-$Q$ filter bank, meaning each band’s $Q = \frac{\text{center frequency}}{\text{bandwidth}}$ is identical. Equivalently, all bands have the same fractional bandwidth when spaced geometrically.

% In the context of hierarchical AM, this is highly desirable. If each modulation frequency $f_{k+1}$ is, say, a fixed fraction of $f_k$ (i.e.\ $f_{k+1} = \alpha\,f_k$ for some $0<\alpha<1$), then each layer occupies the same fraction of its carrier’s spectrum. For instance, if $\alpha=0.5$ (spacing of one octave per layer), each new pair of sidebands sits at $\pm 50\%$ of the previous layer’s carrier frequency. This yields a uniform margin: the sidebands from layer $k+1$ will reach at most $\pm 50\%$ of $f_k$ away from each $f_c \pm \cdots \pm f_k$ line. As a result, none of those sidebands can touch the adjacent cluster (centered at $f_c \pm \cdots \pm f_{k-1}$), because the gap to the next cluster is also about 50\% of $f_k$ on each side. Geometric scaling thus guarantees a fixed relative separation between sideband groups at every layer. In contrast, if one attempted linear spacing (e.g.\ $f_k = f_c - (k-1)\Delta f$ for some fixed $\Delta f$), the highest-frequency layers would be cramped together (small absolute gaps near $f_c$) while the lowest layers would be excessively far apart in relative terms. The logarithmic choice equalizes these proportional gaps.

% Another way to derive the need for log-spacing is by considering the cumulative frequency span of the lower layers. Suppose we use $N$ layers. If each layer $k$ is a fraction $\alpha$ of the previous, then the total fraction of $f_1$ occupied by all lower layers is $\alpha + \alpha^2 + \cdots + \alpha^N < \frac{\alpha}{1-\alpha}$ (for $\alpha<1$). If $\alpha$ is small (strongly logarithmic spacing), the sum is bounded and the highest layer ($f_1$) dominates the spacing. As $N$ grows (more layers), there is still room because the series converges. In the extreme case of infinitely many layers, avoiding overlap would require $\alpha \le 0.5$ so that $\alpha/(1-\alpha) \le 1$ (the worst-case limit where the sum of all lower sideband spans equals $f_1$ but never exceeds it). This aligns with our earlier heuristic that each frequency should exceed the sum of all lower frequencies. Thus, from first principles of avoiding intermodulation overlap, one is naturally led to a geometric series of frequencies. It is no coincidence that many physical and engineered systems use log-spaced frequency bands to handle multi-scale signals. For example, the human auditory system’s frequency sensitivity is roughly logarithmic; the cochlea can be modeled as a bank of bandpass filters whose center frequencies follow the Greenwood equation (approximately exponential distribution along the cochlear length), resulting in roughly constant-$Q$ filtering across the audible spectrum. Indeed, analysis of speech and music often employs a logarithmic frequency axis for amplitude modulation spectra, capturing slow prosodic modulations and faster phonetic modulations in separate bands. In the same vein, a hierarchical modulator with geometric spacing ensures that each layer can be demodulated with a similar relative bandwidth and tuning, simply scaled in frequency. This self-similarity greatly simplifies the design of demodulation filters and circuits, as discussed below.




% \subsubsection*{Avoiding Distortion with $m < 1$} 
% The modulation index $m$ (modulation depth) governs how strongly each oscillator modulates the next. Crucially, \textbf{when $m < 1$}, each factor remains positive for all $t$ (since $1 + m\cos(\cdot) > 0$ as the minimum value is $1 - m > 0$). In this \textbf{under-modulation} regime, the slower oscillations simply scale the amplitude of faster ones without inverting or distorting them. The envelope of the signal accurately follows the shape of the modulating wave, maintaining a one-to-one correspondence between modulator and envelope. Each modulation layer can thus be treated separately as a linear amplitude scaling of the next layer.

% If \textbf{$m = 1$} (100\% modulation), the lowest value of $1 + m\cos(\cdot)$ reaches zero. At those moments the amplitude of the affected oscillator is zero, causing \textbf{critical modulation} where the carrier (or faster wave) is completely suppressed at troughs. At \textbf{$m > 1$}, \textbf{overmodulation} occurs -- the term $1 + m\cos(\cdot)$ becomes negative during part of the cycle. This means the instantaneous amplitude inversion of that layer (the carrier’s phase flips by 180$^\circ$ when the amplitude goes negative). The result is a \textbf{distorted envelope} and a breakdown of the simple multiplication structure into a nonlinear regime. Instead of a clean amplitude modulation, the slower wave now also induces phase reversals in the faster wave.

 
% A key virtue of keeping the modulation depth $m$ below 1 is that it preserves an orderly spectral structure. \textbf{Moderate modulation ($m < 1$)} results in a \textbf{spectrum with a clear power-law or hierarchical distribution} of energy: the majority of signal power remains concentrated at the fundamental frequencies of each oscillator (and their primary sidebands), and progressively less power appears at higher-order combination frequencies. Intuitively, each successive modulation layer contributes smaller amplitude fluctuations (scaled by powers of $m$), so the spectral contributions diminish rapidly for higher-order terms. This often manifests as a \textbf{1/f-like decay} in power: lower-frequency modulators (being first-order terms) have the strongest amplitudes, and higher-frequency modulations or sidebands (higher-order products) have much lower power.

% When \textbf{$m$ is too high (approaching or exceeding 1)}, the spectral purity is compromised. \textbf{Excessive modulation depth ($m \geq 1$)} drives strong nonlinearities that \textbf{generate a plethora of harmonics and intermodulation frequencies}. Instead of a neat pair of sidebands or a few small extra peaks around each fundamental frequency, the signal develops a wide range of frequency components. The end result is \textbf{spectral clutter}: many frequencies carry significant power. This disrupts any power-law decay; higher-order terms no longer fade out but instead can carry comparable energy.
 
Natural systems that exhibit hierarchical oscillations appear to enforce the $m < 1$ principle to maintain functional integrity. The brain produces many oscillatory nested rhythms (delta, theta, alpha, beta, gamma, etc.).  Maintaining $m < 1$ in these neural modulations preserves \textbf{well-separated oscillatory bands}. Each brainwave band retains its characteristic frequency range and identity, which is important if different bands are to carry separable information streams.

In summary, \textit{overmodulation is avoided in both engineered HAM systems and natural oscillatory hierarchies} because it leads to distortion and loss of information. The condition $m < 1$ keeps each modulation layer linear and the overall signal decomposable into distinct frequency components.

\subsection{Power scaling} \label{app:powerlaw}

\paragraph{Sideband spectra.} We first examine sideband spectra. With equal depths \(m_i\equiv m\in(0,1)\) and log spacing \(f_{k+1}=f_k/r\) (\(r>1\)),
the amplitude of a component involving \(k\) modulators scales as \((m/2)^k\).
Because \(f_k \propto r^{-k}\), we have \(k \approx \log_r(f_1/f_k)\). Hence
\[
A(f_k) \propto \left(\frac{m}{2}\right)^{\log_r (f_1/f_k)}
= \left(\frac{f_k}{f_1}\right)^{\,\log_r(m/2)} ,
\]
and the power scales as
\[
P(f_k)\propto \left(\frac{f_k}{f_1}\right)^{\,2\log_r(m/2)}
= \frac{1}{f_k^{\,\alpha}},\qquad \text{ with }
 \ \alpha \;=\; \frac{2\ln(2/m)}{\ln r}\ >0\ .
\]
Thus, for fixed \(r\), \(\alpha\) increases as \(m\) decreases (weaker modulation \(\Rightarrow\)
steeper slope), and for fixed \(m\), \(\alpha\) decreases as \(r\) increases (wider spacing
\(\Rightarrow\) shallower slope, see Figure~\ref {fig:power_law}). %This is a multiplicative‑cascade result for HAM under log spacing, not a universal account of aperiodic structure.



\paragraph{Full spectrum.} Next, we model a random ensemble of logarithmic oscillators interacting via modulation. 
Consider an ensemble of narrowband oscillators with center frequencies \(f\in[F_{\min},F_{\max}]\).
Assume (i) \emph{log‑uniform} center density \(\rho(f)=C/f\) (equal expected number per
log‑frequency bin, i.e., equal energy per octave), where \(C=[\ln(F_{\max}/F_{\min})]^{-1}\);
(ii) each oscillator contributes the same unit power, shaped by a unit‑area spectral kernel
\(K_\varepsilon(\cdot)\) that is narrow relative to its center (ideally constant‑\(Q\), i.e. fractional
bandwidth approximately constant across \(f\)).%
Equal energy per octave $\Rightarrow$ pink noise ($1/f$) is a classical result in audio/time–frequency
analysis; constant‑$Q$ filterbanks formalize this \emph{logarithmic} tiling.


\medskip
\noindent\textbf{Lemma 1 (Log‑spaced ensembles yield a $1/f$ backbone).}
Let \(S_0\) denote the ensemble power spectral density (PSD) obtained by summing all oscillators:
\[
S_0(f)=\int_{F_{\min}}^{F_{\max}}\rho(u)\,K_\varepsilon(f-u)\,du.
\]
If \(K_\varepsilon\) is narrow (or constant‑\(Q\)), then, for \(f\) away from band edges,
\[
S_0(f)\;\approx\;\rho(f)\underbrace{\int K_\varepsilon}_{=1}\;=\;\frac{C}{f}\,.
\]
\emph{Intuition.} Convolving a slowly varying density \(\rho\) with a narrow kernel evaluates
\(\rho\) locally. With \(\rho(f)\propto 1/f\), equal oscillator mass per octave gives a pink (\(1/f\))
baseline.

We now assume that the oscillators are coupled randomly as carrier and modulator (second-order pairing). 
We draw unordered pairs \((f_c,f_m)\) i.i.d.\ from \(\rho\) with the \emph{AM constraint} \(f_m<f_c\).
First‑order AM generates sidebands at \(f_c\pm f_m\) with weights \(w_\pm\) (for small modulation
depth, \(w_\pm\propto m/2\)).%

% \footnote{Intermodulation/AM sidebands at $f_c\pm f_m$ (and higher‑order sums) are standard in
% nonlinear/SSVEP analyses 
% \cite{norciaSteadystateVisualEvoked2015, gordonIntermodulationComponentsVisual2019,chenIntermodulationFrequencyComponents2024}}

\paragraph{Lemma 2 (Sidebands from a log-uniform population).}
Let $f_c,f_m$ be i.i.d.\ on $[F_{\min},F_{\max}]$ with density $\rho(f)=C/f$, conditioned on $f_c>f_m$. Define lower/upper sideband frequencies $f_L=f_c-f_m$ and $f_U=f_c+f_m$. Then
\[
D(f_L)=\int_{F_{\min}+f_L}^{F_{\max}} \frac{C^2}{a(a-f_L)}\,da
\ =\ \frac{C^2}{f_L}\,\ln\!\left(\frac{(F_{\max}-f_L)(F_{\min}+f_L)}{F_{\max}F_{\min}}\right),
\]
\[
U(f_U)=\!\!\int_{\max(F_{\min},\,f_U-F_{\max})}^{\min(F_{\max},\,f_U-F_{\min})}
\frac{C^2}{a(f_U-a)}\,da
\ =\ \frac{C^2}{f_U}\,\ln\!\Bigg(\frac{a_1[\,f_U-a_0\,]}{a_0[\,f_U-a_1\,]}\Bigg),
\]
where $a_0,a_1$ are the integration limits above.
For $f$ in the interior of $[F_{\min},F_{\max}]$ both marginals satisfy
\[
D(f)=\frac{\kappa_-(f)}{f},\qquad U(f)=\frac{\kappa_+(f)}{f},
\]
with $\kappa_\pm(f)$ bounded and slowly varying (logarithmic) functions of $f$.
As $f\to 0$, $D(f)\to C^2(\frac{1}{F_{\min}}-\frac{1}{F_{\max}})$ (a constant pedestal),
while $U(f)$ exhibits a high-frequency roll-off near $F_{\max}$ due to finite support.
Thus the \emph{expected} sideband contribution preserves the $1/f$ backbone in the interior,
with deviations driven by a low-$f$ pedestal (flattening fitted slopes) and a high-$f$ truncation
(steepening locally).%
%\footnote{The same conclusions follow by changing variables to ratio $r=f_c/f_m$:
%$f_L=(r-1)f_m$, $f_U=(r+1)f_m$, giving $D(f)\propto(1/f)\int (dr/r)$ and
%$U(f)\propto(1/f)\int (dr/r)$ with edge-dependent bounds.}




\medskip
\noindent\textbf{Corollary 1 (Total PSD).}
With carrier lines \(S_0(f)\propto 1/f\) and sidebands \(S_{\pm}(f)\) as above, the expected total PSD is
\[
S_{\mathrm{tot}}(f)
\;=\; S_0(f) + w_-\,D(f) + w_+\,U(f)
\;=\; \frac{\kappa_1}{f}\ +\ \kappa_0\ +\ \text{edge/log corrections}.
\]
Hence the ensemble remains approximately \(1/f\) with an \emph{additive constant} \(\kappa_0\)
determined by small‑difference sidebands and finite‑band edges. Larger \(\kappa_0\) flattens the
apparent slope \(\alpha\) when fitting over restricted ranges.

\medskip
\noindent\textbf{Finite‑range and bandwidth effects.}
(i) \emph{Finite support} \([F_{\min},F_{\max}]\) produces curvature near edges:
high‑frequency sum‑sidebands are truncated at \(F_{\max}\), and very small differences accumulate
near \(F_{\min}\), both tending to \emph{flatten} slopes. (ii) Using a \emph{constant absolute}
kernel width (e.g., Gaussian \(\sigma=\) 0.5\,Hz) is not constant‑\(Q\) and biases the high‑frequency
region. Adopting \(\sigma=\eta f\) (fractional width) restores scale invariance and improves linearity
of log–log fits. These two points explain why, in our simulation, log‑spaced centers produce a
clear \(1/f\) (\(\alpha\approx 1.0\)) without sidebands, while adding sidebands slightly reduces the
fitted \(\alpha\) (constant pedestal added), as observed in Fig.~\ref{fig:simulationfullspectrum}.%
%\footnote{See your Fig.~6 for slopes with/without sidebands; constant‑$Q$ improves linearity and stabilizes $\alpha\approx 1$ over decades.}
% (Refer to figure cross‑reference or page number as available in the manuscript.)


\paragraph{Does geometric spacing matter for the $1/f$ law?}
The derivation of $S(f)\!\sim\!1/f$ relies only on the \emph{density} of oscillator
centers $\rho(f)$, not on exact geometric ratios. Geometric spacing
($f_{k+1}=f_k/r$) yields $\rho(f)\propto 1/f$ exactly, but any monotone sequence
that satisfies the \emph{super-increasing} condition
\[
f_k>2 \sum_{j>k}f_j
\]
implies that the ratios \(r_k=f_k/f_{k+1}\) are all \(>3\), though not necessarily
constant. This is because
$$
r_k=\frac{f_k}{f_{k+1}}> \frac{2 \sum_{j>k}f_j}{f_{k+1}}= 1 + \frac{2 \sum_{j>k+1}f_j}{f_{k+1}} >3
$$
Writing $\log f_{k+1}=\log f_k-\log r_k$, the sequence of log-frequencies
performs a random walk with mean step $-\langle \log r\rangle$. By the law of
large numbers,
\[
\log f_k \approx \log f_0 - k\,\langle \log r\rangle,
\qquad
k \approx \frac{\log(f_0/f_k)}{\langle \log r\rangle}.
\]
The cumulative count of oscillators above frequency $f$ is therefore
\[
N(f)=\#\{k: f_k>f\}\approx
\frac{\log(F_{\max}/f)}{\langle \log r\rangle},
\]
so the differential density is
\[
\rho(f)=-\frac{dN}{df}=\frac{1}{\langle \log r\rangle}\frac{1}{f}.
\]
Hence any super-increasing sequence with bounded ratios
and finite $\langle \log r\rangle$ yields an approximately log-uniform
distribution of oscillator centers. The strict geometric case
($r_k\!\equiv\!r$) is a special instance where $\rho(f)=C/f$ exactly.
Thus, the weaker non-overlap condition suffices—geometric spacing merely
provides a clean analytical form.
Thus, geometric spacing is a \emph{sufficient} but not
\emph{necessary} condition; the weaker non-overlap constraint already implies an
asymptotically log-uniform distribution and therefore the same $1/f$ scaling in
expectation --- geometric spacing merely
provides a clean analytical form.

 
As we saw, for a \emph{single multiplicative cascade} with geometric spacing \(r>1\) and modulation depth \(m\),
the toy analysis gives \(P(f)\sim f^{-\alpha}\) with
\(
\alpha={2\ln(2/m)}/{\ln r}
\)
(derived earlier). That mechanism ties \(\alpha\) to \((r,m)\) within one chain. By contrast, the
\emph{population mixture mechanism} above yields \(\alpha\approx 1\) from the log‑spaced
\emph{distribution of centers} alone, even without sidebands. Both mechanisms can coexist in
neural data; careful parameterization is needed to separate aperiodic from oscillatory peaks.%
On disentangling aperiodic and periodic structure, see Donoghue et al. (2020) \cite{donoghueEvaluatingComparingMeasures2024}. 
Other accounts of $1/f^\alpha$ (mixtures of timescales, synaptic filtering, E/I balance, etc.) are
reviewed in Bédard \& Destexhe (2009) and Milotti (2002) \cite{milotti1NoisePedagogical2002}.
 

In summary,
(1) A bank of log‑spaced oscillators with equal power per band produces \(S(f)\approx 1/f\).
(2) Random AM pairings add a constant pedestal; stronger modulation (\(m\)) or more pairs
increase \(\kappa_0\) and flatten the fitted \(\alpha\) over finite windows. (3) Using constant‑\(Q\)
kernels (or filters) should push the fitted slope in Fig.~\ref{fig:logarithmic_hierarchy} toward \(\alpha\approx 1\) and
increase the fitting range.
The assumption \(\rho(f)\propto 1/f\) parallels empirical \emph{logarithmic spacing} of neural bands
(delta\(\to\)gamma) \cite{penttonenNaturalLogarithmicRelationship2003} and is mathematically equivalent to a constant‑\(Q\)
tiling (equal spacing on a log‑frequency axis) \cite{brownCalculationConstantSpectral1991}. 


\paragraph{Modulation and demodulation.}
Let the transmitted field be a narrowband carrier $c_0(t)$ around $f_0$ whose
amplitude is multiplicatively structured by slower processes,
\begin{equation}
x(t)\;=\;A_0\,c_0(t)\,\prod_{i=1}^{N}\bigl[\,1+m_i\,s_i(t)\,\bigr],
\qquad 0<m_i<1,
\label{eq:ham-signal}
\end{equation}
with $s_i(t)$ narrowband around $f_i\!\ll\! f_0$. Because \eqref{eq:ham-signal} is a
\emph{product}, the \emph{order of modulation} is immaterial: multiplication
commutes, hence any staging of the factors yields the same $x(t)$ (up to higher‑order
$\mathcal O(m^2)$ cross‑terms). In network form, convergent inputs \emph{sum then
modulate} a faster carrier at a node, but in the small‑signal regime the linear expansion
$[1+\sum_i m_i s_i(t)]$ reproduces the same product across stages to first order.

 
To \emph{recover} the nested envelopes from a single wideband signal, order now
\emph{does} matter. Write a constant‑$Q$ demodulator centered at $f$ as
\[
\mathcal D_{f}[x]\;=\;\bigl|\,\mathcal H\{\,B_f[x]\,\}\bigr|,
\]
where $B_f[\cdot]$ band‑passes the (non‑overlapping) cluster around $f$
and $\mathcal H\{\cdot\}$ is the analytic (Hilbert) transform
envelope \cite{boashashEstimatingInterpretingInstantaneous1992}. Under the spacing
condition $f_k>2\sum_{j>k} f_j$ (equivalently geometric $f_{k+1}=f_k/r$ with $r>3$),
clusters do not overlap. Then demodulating the \emph{fastest} band first yields the
composite slow envelope,
\[
E_0(t)\;=\;\mathcal D_{f_0}[x]\;\approx\;A_0
\Bigl[\,1+\sum_{i=1}^{N} m_i \cos(2\pi f_i t)\Bigr]+\mathcal O(m^2),
\]
and iterating “fast $\rightarrow$ slow” peels off deeper layers,
\[
E_k(t)\;=\;\mathcal D_{f_k}[E_{k-1}] \;\approx\;
A_0\Bigl[\,1+\sum_{i=k+1}^{N} m_i \cos(2\pi f_i t)\Bigr].
\]
Attempting “slowest‑first” on the raw signal is effectively a low‑pass and misses the
intended envelope until the faster band that \emph{carries} it has been demodulated.
Equivalently, a \emph{parallel} constant‑$Q$ filterbank can demodulate all layers at
once; under non‑overlap these envelopes match the staged fast$\rightarrow$slow
recovery to first order.





% \paragraph{Logspacing simulation.} As an example of how logspacing leads to power laws, we first simulated 5,000 uncoupled harmonic oscillators sampled either uniformly or logarithmically between 0.5–200 Hz. Sampled oscillator pairs (a carrier and a modulator) were constrained such that the modulator frequency \( f_m \) was less than the carrier frequency \( f_c \). For each valid pair, we added power at \( f_m \) and \( f_c \) using a Gaussian kernel ($\sigma$ = 0.5 Hz). In a second condition, we also added power at the AM-generated sidebands \( f_c \pm f_m \). The resulting aggregate spectrum was normalized, and its slope \( \alpha \) estimated via linear regression in log-log space for \( f > 5 \) Hz. Figure \ref{fig:logspectra} provides an overview of the findings: 
% For a \textit{uniform frequency distribution,} there is no power-law scaling (\( \alpha \approx 0.08 \), flat spectrum). For 
% \textit{log-spaced frequencies without sidebands}, a clear power-law (\( \alpha \approx 0.84 \)) demonstrates that log-distributed source density and hierarchical pairing are sufficient. Finally, for 
%  \textit{log-spaced frequencies, with sidebands,} a shifted scaling is found (\( \alpha \approx 0.95 \)).

% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=0.8\linewidth]{figures/Power-Law Fits: Effect of Sidebands on Spectral Slope.png}
%   \caption{ Power spectra and fitted power-law slopes for hierarchical amplitude modulation (AM) simulations with $f_m < f_c$. 
% The \textit{uniform} oscillator distribution shows no meaningful scaling ($\alpha \approx 0.08$). 
% The \textit{log-spaced} distribution yields a clear $1/f^\alpha$ structure. 
% Including AM sidebands ($f_c \pm f_m$) results in a slightly steeper slope ($\alpha \approx 0.95$) than when only the modulator and carrier frequencies are included ($\alpha \approx 0.84$), reflecting enhanced low-frequency accumulation. 
% Dashed lines indicate linear fits in log-log space over $f > 5$ Hz.}
%   \label{fig:logspectra}
% \end{figure}
% %https://colab.research.google.com/drive/1Pi7JAZTRPXa7-UuQ-hx75XIW4hYB8_qr




% We further tested how the geometric spacing ratio between oscillator frequencies affects the emergence of the power law. Using between 5 and 100 logarithmically spaced bins to populate the frequency range (0.5–200 Hz), we found that the slope $\alpha$ increases rapidly between 5 and 20 bins (corresponding to spacing ratios $r \approx 3.31$ to $1.35$), but plateaus beyond 30 bins ($r \approx 1.22$). Increasing the number of bins beyond 30 had no meaningful impact on the spectral slope, which stabilized near $\alpha \approx 0.96$. This indicates that while logarithmic spacing is essential, only moderate resolution is needed to fully express the scaling. Finer band divisions contribute little additional structure once the modulation space is densely sampled.
 
%------------------------------------------------------------
% TEXT POLICY: Guard-bands, spacing, and non-overlap (paste once)
% Requires: amsmath, amssymb, xcolor, tcolorbox (already in your preamble)
%------------------------------------------------------------

% --- Short, reusable macros (use these in the text to stay consistent) ---
\newcommand{\geomspace}{\emph{geometric spacing}}
\newcommand{\constQ}{constant-$Q$}
\newcommand{\ratio}{\ensuremath{r}}      % spacing ratio
\newcommand{\theoryr}{\ensuremath{r>2\text{--}3}}    % theory threshold for disjoint clusters
\newcommand{\weakr}{\ensuremath{r>2}}     % weaker super-increasing threshold
\newcommand{\empiricalr}{\ensuremath{r\approx 2\text{--}3}} % empirical observation
\newcommand{\gbnoc}{\textsc{GBNO}}       % Guard–Band Non–Overlap (short name)
\newcommand{\si}{\textsc{SI}}         % Super–Increasing (short name)

% Inequalities as named snippets (for equations or inline)
\newcommand{\GBNOineq}{\ensuremath{f_k \;>\; 2\sum_{j>k} f_j \quad \forall k}}
\newcommand{\SIineq}{\ensuremath{f_k \;>\; \sum_{j>k} f_j \quad \forall k}}
\newcommand{\GeomDef}{\ensuremath{f_{k+1} = f_k/\ratio}}
\newcommand{\GeomGBNO}{\ensuremath{\GeomDef\ \Rightarrow\ \ratio>2\text{--}3}}
\newcommand{\GeomSI}{\ensuremath{\GeomDef\ \Rightarrow\ \ratio>2}}

% --- The policy box shown once in the manuscript ---
\begin{tcolorbox}[title={\textbf{Terminology: guard-bands, spacing, and non-overlap}},
         colback=gray!2, colframe=gray!40, left=1mm, right=1mm, top=1mm, bottom=1mm]

\textbf{Terminology.}
\begin{itemize}
 \item \textbf{Non-overlap (default meaning).} Throughout the paper, \emph{“non-overlap”} means
    \textbf{Guard–Band Non–Overlap} (\gbnoc): 
    \[
     \GBNOineq.
    \]
    Under \geomspace, i.e., \( \GeomDef \), this is equivalent to \( \theoryr \).
 \item \textbf{Weaker condition.} The \textbf{Super–Increasing} (\si) hierarchy
    \[
     \SIineq
    \]
    (geometric case \( \GeomSI \)) prevents slower combinations from \emph{reaching} \(f_k\) but
    \emph{does not} guarantee that the downward spread of the \(k\)th sideband cluster cannot
    overlap the upward spread of the \((k{+}1)\)st. Hence \si\ is \emph{insufficient}
    for strict cluster separation and uniqueness of signed combinations.
 \item \textbf{Geometric/log spacing and \constQ.} We use \geomspace\ (\(f_{k+1}=f_k/\ratio\)) to describe
    log‑uniform centers and a \constQ\ tiling. When we discuss theoretical guarantees
    (non‑overlap, uniqueness, staged demodulation), we state \(\theoryr\). When we describe
    empirical bands, we state \( \empiricalr \) and note that modest overlap can appear in
    practice for finite depth and small modulation.
\end{itemize}

\textbf{Demodulation requirement.}
\begin{itemize}
 \item \emph{Sequential demodulation without interference} assumes \gbnoc\ (\(\theoryr\) under geometry),
    so that each carrier’s sideband cluster is disjoint from adjacent clusters.
 \item When referring to “adequate separation” in simulations or data where depth is finite
    and modulation is moderate (\(m<1\)), we may note that \si\ (\(\weakr\)) can suffice
    operationally, but we reserve the term \emph{“non‑overlap”} \emph{strictly} for \gbnoc.
\end{itemize}

\textbf{Summary of statements.}
\begin{itemize}
 \item \emph{Theory:} Cluster non‑overlap (GBNO) requires \( \GBNOineq \), i.e., \( \GeomGBNO \).
 \item \emph{Weaker statement:} The super‑increasing hierarchy \( \SIineq \) (geometric: \( \GeomSI \))
    prevents slower combinations from reaching \(f_k\) but does not preclude adjacent cluster overlap.
 \item \emph{Empirical:} Canonical bands are roughly log‑spaced with \( \empiricalr \), close to but sometimes
    below the theoretical \( \theoryr \), which explains limited yet tolerable overlap in practice.
\end{itemize}

\end{tcolorbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Requires: \usepackage{booktabs,array,hyperref}
\begin{table}[t]
\scriptsize
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{@{}p{0.12\textwidth} p{0.22\textwidth} p{0.62\textwidth}@{}}
\toprule
\textbf{Band (Hz)} & \textbf{Putative generators} & \textbf{Mechanistic account (abridged) with links (incl. neural mass implementations)} \\
\midrule
\textbf{Very slow / Slow} ($<\!1$) 
& Neocortex $\rightarrow$ thalamus (global) 
& Cortical Up/Down dynamics from recurrent E–I circuits with slow adaptation (e.g., $I_\mathrm{M}$, Ca$^{2+}$‑activated K$^+$) and synaptic depression give bistability and traveling slow waves; cortex alone can generate SOs, with thalamus coordinating SO–spindle interactions. In vivo/in vitro and modeling support this view \href{https://www.cell.com/neuron/fulltext/S0896-6273(17)30416-6}{Sánchez‑Vives\,(2017)}, \href{https://www.sciencedirect.com/science/article/abs/pii/S2468867320300365}{Sánchez‑Vives\,(2020)}, \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC10351697/}{Dalla\,Porta\,(2023)}, \href{https://www.frontiersin.org/articles/10.3389/fnsys.2021.609645/full}{Torao‑Angosto\,(2021)}. 
\emph{Neural mass implementations:} corticothalamic NMMs/fields that reproduce SOs and K‑complexes \href{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005022}{Schellenberger\,Costa\,(2016)}, \href{https://pubmed.ncbi.nlm.nih.gov/25659479/}{Zhao\,(2015)}; SO–spindle coupling in a thalamocortical NMM \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC9120371/}{Jajcay\,(2022)}. \textit{JR note:} the standard Jansen–Rit (JR) column model exhibits an $\sim\!$alpha limit cycle via Hopf and can also express lower‑frequency (delta‑like) cycles under parameter changes \href{https://pubmed.ncbi.nlm.nih.gov/17052158/}{Grimbert\,(2006)}, \href{https://pubmed.ncbi.nlm.nih.gov/20045068/}{Spiegler\,(2010)}; however, canonical $<\!1$\,Hz SOs (Up/Down) typically require slow variables and/or explicit corticothalamic embedding. \\[0.35em]

\textbf{Delta} (1–4) 
& Thalamocortical loop (TC$\leftrightarrow$TRN) $\pm$ cortex
& Hyperpolarized thalamic relay (TC) neurons deinactivate T‑type Ca$^{2+}$ channels; interplay with $I_h$ and TRN GABAergic inhibition yields rebound LTS bursts and cyclical TC–TRN pacing in $\delta$; cortex gates/entrains and links $\delta$ to SOs/spindles. Reviews and mechanisms: \href{https://europepmc.org/articles/pmc6364803}{Crunelli\,(2018)}, \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC3018590/}{Crunelli\,(2006)}. \emph{Neural mass implementations:} corticothalamic NMMs/fields producing delta/spindles via relay/TRN dynamics (T‑currents, $I_h$) \href{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005022}{Schellenberger\,Costa\,(2016)}, \href{https://link.aps.org/doi/10.1103/PhysRevE.85.011910}{Roberts\,(2012)}; JR can enter a slow (delta‑like) limit cycle under specific gains/inputs \href{https://pubmed.ncbi.nlm.nih.gov/17052158/}{Grimbert\,(2006)}, \href{https://pubmed.ncbi.nlm.nih.gov/20045068/}{Spiegler\,(2010)}. Cortical $\delta$ can also arise locally in EC via HCN resonance \href{https://www.cell.com/cell-reports/fulltext/S2211-1247(23)01279-2}{Haam\,(2023)}. \\[0.35em]

\textbf{Theta} (4–8) 
& Medial septum $\rightarrow$ hippocampus; hippocampal–entorhinal circuits; links to frontal cortex
& Medial septal GABAergic/cholinergic cells pace hippocampus; rhythmic septal inhibition of interneurons yields disinhibition windows for principal cells; CA3/EC inputs and intrinsic resonance shape phase/frequency. Septal cooling slows $\theta$ and disrupts coordination \href{https://www.cell.com/neuron/fulltext/S0896-6273(20)30392-5}{Petersen\,(2020)}; system‑level reviews \href{https://pubmed.ncbi.nlm.nih.gov/35926456/}{Kocsis\,(2022)}. \emph{Neural mass implementations:} septo‑hippocampal NMMs capturing $\theta$ modulation and phase precession \href{https://pubmed.ncbi.nlm.nih.gov/25284339/}{Cona\,(2015)}; recent septo‑hippocampal NMM with ACh‑$\theta$ interactions \href{https://pubmed.ncbi.nlm.nih.gov/40664336/}{Pirazzini\,(2025)}. (Many hippocampal $\theta$ models are spiking/resonator based rather than NMMs.) \\[0.35em]

\textbf{Alpha} (8–12) 
& Cortex (supragranular) $\leftrightarrow$ thalamus (pulvinar/visual nuclei)
& Human laminar/iEEG show superficial cortical generators that \emph{lead} thalamic alpha and propagate as traveling waves \href{https://www.pnas.org/doi/10.1073/pnas.1913092116}{Halgren\,(2019)}; in mice, visual thalamus is necessary for an alpha‑like quiet‑wake rhythm and gates cortical activity \href{https://www.sciencedirect.com/science/article/pii/S089662732100773X}{Nestvogel \& McCormick\,(2022)}; pulvinar–cortex feedback can generate/gate alpha \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC8685293/}{Cortés\,(2021)}. \emph{Neural mass implementations:} cortical JR column (alpha via Hopf) \href{https://pubmed.ncbi.nlm.nih.gov/17052158/}{Grimbert\,(2006)}; corticothalamic NMMs/fields producing alpha and split‑alpha \href{https://www.fil.ion.ucl.ac.uk/~karl/A%20neural%20mass%20model%20for%20MEG.pdf}{David\&Friston\,(2003)}, \href{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004352}{Becker\,(2015)}, \href{https://link.aps.org/doi/10.1103/PhysRevE.85.011910}{Roberts\,(2012)}. \\[0.35em]

\textbf{Beta} (13–30) 
& Sensorimotor cortex; basal ganglia–thalamocortical loops (STN–GPe)
& In cortex, structured E–I connectivity and long‑range feedback support transient $\beta$ bursts/traveling $\beta$ waves \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC10112891/}{Kang\,(2023)}; in BG, reciprocal STN–GPe forms an E–I resonant loop generating $\sim\!$20\,Hz; cortico–BG feedback synchronizes and can pathologically amplify beta in PD \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC4684204/}{Pavlides\,(2015)}; layer‑V cortical $\beta_2$ (20–30\,Hz) in vitro depends partly on gap junction coupling \href{https://www.pnas.org/doi/10.1073/pnas.0607443103}{Roopun\,(2006)}. \emph{Neural mass implementations:} cortical E–I NMMs reproducing beta waves/bursts \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC10112891/}{Kang\,(2023)}; cortico‑BG NMMs and mean‑field models for pathological beta and DBS effects \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC4684204/}{Pavlides\,(2015)}, \href{https://pubmed.ncbi.nlm.nih.gov/24344162/}{Nevado‑Holgado\,(2014)}. \\[0.35em]

\textbf{Gamma} (30–100+) 
& Local cortical/hippocampal microcircuits (columns)
& PING/ING mechanisms: fast E–I loops with PV$^{+}$ basket cells and GABA\textsubscript{A} decay set $\gamma$ period; drive and delays tune slow/fast $\gamma$; typically local and nested by $\theta/\alpha$. Mechanistic reviews: \href{https://pubmed.ncbi.nlm.nih.gov/22443509/}{Buzsáki \& Wang\,(2012)}, \href{https://www.cell.com/trends/neurosciences/fulltext/S0166-2236(24)00082-1}{Craig \& McBain\,(2024)}. \emph{Neural mass implementations:} canonical microcircuit NMM/DCM families produce $\gamma$ via superficial E–I loops and fast synaptic kinetics \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC3664834/}{Moran\,(2013)}, \href{https://www.fil.ion.ucl.ac.uk/~karl/A%20neural%20mass%20model%20for%20MEG.pdf}{David\&Friston\,(2003)}, \href{https://dspace.mit.edu/bitstream/handle/1721.1/150022/1-s2.0-S105381192300085X-main.pdf?isAllowed=y&sequence=2}{Sánchez‑Todo\,(2023)} \cite{ruffiniP118BiophysicallyRealistic2020, sanchez-todoPhysicalNeuralMass2023, lopez-solaPersonalizableAutonomousNeural2022,wendlingMultiscaleNeuroinspiredModels2024}. \\
\bottomrule
\end{tabular}

\caption{\textbf{Generators and mechanisms across canonical bands, with neural mass models.} Cortical columns can locally generate $\gamma$ (PING/ING) and parts of $\alpha$; thalamo‑cortical loops contribute to $\alpha$ and dominate $\delta$; septo‑hippocampal loops pace $\theta$; SOs ($<\!1$\,Hz) primarily emerge from cortical E–I networks with slow processes and can be coordinated by thalamus. Neural mass/field models (JR; corticothalamic mean‑field; canonical microcircuits; cortico‑basal ganglia) have been proposed for each band: JR yields $\sim$alpha and, under parameter shifts, slower (delta‑like) rhythms \href{https://pubmed.ncbi.nlm.nih.gov/17052158/}{Grimbert\,(2006)}; $<\!1$\,Hz SOs generally require slow adaptation/STD and/or explicit corticothalamic modules \href{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005022}{Schellenberger\,Costa\,(2016)}, \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC9120371/}{Jajcay\,(2022)}.}
\label{tab:frequencies}
\vspace{0.5em}

\textit{Notes.} “Very slow” here refers to slow oscillations (SOs, $<\!1$\,Hz). Infraslow ($\lesssim\!0.1$\,Hz) fluctuations often reflect neurometabolic/vascular components modulating neural excitability and band‑limited envelopes rather than a distinct neuronal rhythm (e.g., \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC3173874/}{Hughes\,(2011)}, \href{https://europepmc.org/article/pmc/pmc7886622}{Drew\,(2020)}).
\end{table}


% \paragraph{Simulation.} As an example, we simulated 5,000 uncoupled harmonic oscillators sampled either uniformly or logarithmically between 0.5–200 Hz. Sampled oscillator pairs (a carrier and a modulator) were constrained such that the modulator frequency \( f_m \) was less than the carrier frequency \( f_c \). For each valid pair, we added power at \( f_m \) and \( f_c \) using a Gaussian kernel ($\sigma$ = 0.5 Hz). In a second condition, we also added power at the AM-generated sidebands \( f_c \pm f_m \). The resulting aggregate spectrum was normalized, and its slope \( \alpha \) estimated via linear regression in log-log space for \( f > 5 \) Hz. Figure \ref{fig:logspectra} provides an overview of the findings: 
% For a \textit{uniform frequency distribution,} there is no power-law scaling (\( \alpha \approx 0.08 \), flat spectrum). For 
% \textit{log-spaced frequencies without sidebands}, a clear power-law (\( \alpha \approx 0.84 \)) demonstrates that log-distributed source density and hierarchical pairing are sufficient. Finally, for 
%  \textit{log-spaced frequencies, with sidebands,} a shifted scaling is found (\( \alpha \approx 0.95 \)).

% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=0.95\linewidth]{figures/Power-Law Fits: Effect of Sidebands on Spectral Slope.png}
%   \caption{ Power spectra and fitted power-law slopes for hierarchical amplitude modulation (AM) simulations with $f_m < f_c$. 
% The \textbf{uniform} oscillator distribution shows no meaningful scaling ($\alpha \approx 0.08$). 
% The \textbf{log-spaced} distribution yields a clear $1/f^\alpha$ structure. 
% Including AM sidebands ($f_c \pm f_m$) results in a slightly steeper slope ($\alpha \approx 0.95$) than when only the modulator and carrier frequencies are included ($\alpha \approx 0.84$), reflecting enhanced low-frequency accumulation. 
% Dashed lines indicate linear fits in log-log space over $f > 5$ Hz.}
%   \label{fig:logspectra}
% \end{figure}
% %https://colab.research.google.com/drive/1Pi7JAZTRPXa7-UuQ-hx75XIW4hYB8_qr


% The emergence of $1/f^\alpha$ spectral structure in hierarchical AM systems is fully explained by two minimal structural constraints: (1) a log-spaced distribution of oscillators and (2) a modulation rule enforcing \( f_m < f_c \). Sidebands are not necessary for scale-free behavior but do enhance low-frequency accumulation and steepen the slope.

% We further tested how the geometric spacing ratio between oscillator frequencies affects the emergence of the power law. Using between 5 and 100 logarithmically spaced bins to populate the frequency range (0.5–200 Hz), we found that the slope $\alpha$ increases rapidly between 5 and 20 bins (corresponding to spacing ratios $r \approx 3.31$ to $1.35$), but plateaus beyond 30 bins ($r \approx 1.22$). Increasing the number of bins beyond 30 had no meaningful impact on the spectral slope, which stabilized near $\alpha \approx 0.96$. This indicates that while logarithmic spacing is essential, only moderate resolution is needed to fully express the scaling. Finer band divisions contribute little additional structure once the modulation space is densely sampled.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


 
 
 
 
% \section{Hierarchical Neural Encoding in the Brain}
% \label{sec:hierachical}



% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{The role of fast and slow oscillations}
% Cross-frequency phase-amplitude coupling between gamma and other rhythms within the same and different brain regions has been well documented, including modulation by theta, alpha, spindle, slow, and ultraslow oscillations \cite{buzsakiMechanismsGammaOscillations2012}. This modulation often occurs via phase-amplitude coupling, where the phase of a slow oscillation influences the amplitude of a faster oscillations \cite{buzsakiRhythmsBrain2006}.
% Gamma oscillations, occurring at frequencies between 30--100 Hz, play crucial roles in various brain functions. They are essential for integrating information within neural circuits, supporting complex processes such as perception, cognition, and memory \cite{buzsakiMechanismsGammaOscillations2012, jensenHumanGammafrequencyOscillations2007, friesNeuronalGammabandSynchronization2009}. In the motor cortex, gamma oscillations are linked with movement and learning, where they enhance motor control and skill acquisition \cite{nowakMotorCorticalGamma2018}. 
% Gamma oscillations also facilitate the binding of perceptual features in the cortex, as well as the integration of diverse information in the hippocampus, contributing to episodic memory formation and retrieval \cite{nyhusFunctionalRoleGamma2010}. Disruptions in gamma rhythms have been associated with neurological and psychiatric disorders, including schizophrenia, Alzheimer's disease, and Parkinson's disease \cite{shinGammaOscillationSchizophrenia2011, guanRoleGammaOscillations2022}. The network of inhibitory interneurons is critical for generating gamma oscillations, and disturbances within this network can lead to pathological conditions \cite{guanRoleGammaOscillations2022}.
% Recent studies have explored gamma entrainment through sensory stimulation as a potential therapeutic approach for neuropsychiatric diseases, showing promising avenues for treatment \cite{manippaUpdateUseGamma2022,blackTherapeuticPotentialGamma2024}. As these findings suggest, gamma oscillations are not only fundamental to normal brain function but also present important clinical implications, highlighting the need for further research into gamma-based therapeutic strategies.

% Gamma oscillations, particularly in the 30-100 Hz range, are widely proposed as mechanisms for information encoding in the brain. Fries (2009) \cite{friesNeuronalGammabandSynchronization2009} suggests that gamma-band synchronization allows for \textit{communication through coherence}, aligning excitability phases across neural populations to enhance selective information routing and integration between cortical areas. This alignment is thought to provide a temporal framework within which neural populations can encode and transfer information efficiently. Singer and Gray (1995) \cite{singerVisualFeatureIntegration1995} proposed that gamma oscillations play a crucial role in \textit{feature binding}, encoding sensory information by synchronizing distributed neural assemblies, effectively linking spatially separated representations into coherent perceptual constructs. Jensen et al. (2007) \cite{jensenHumanGammafrequencyOscillations2007} further emphasized gamma oscillations as \textit{carriers of task-relevant information} across neural networks, particularly in contexts involving attention and working memory. By selectively amplifying specific representations, gamma rhythms enhance signal-to-noise ratios, facilitating the encoding and prioritization of important information. These studies collectively underscore gamma oscillations as essential temporal structures for dynamic encoding, enabling effective communication and information processing across brain networks. 

% Results in recent animal studies suggest that unpredicted stimuli—i.e. “deviants” in an oddball paradigm—evoked robust responses in supragranular layer 2/3, arising after 100-ms post-stimulus, with processing of contextually deviant stimuli in the oddball paradigm involving increases in theta- and gamma-band oscillations in layer 2/3 \cite{gallimoreSpatiotemporalDynamicsVisual2023}. These frequency bands and this layer of cortex are believed to carryout feed-forward processing, which is consistent with deviance detection reflecting a “prediction error” that is fed-forward in cortical circuits. 
% In summary, gamma rhythms are fundamental carriers of information in the brain with a central multifunctional role in neural systems for perception, selective attention, memory, motivation, and behavioral control \cite{bosmanFunctionsGammabandSynchronization2014}.

% %\subsection{Alpha and theta}
% Alpha (8–12 Hz) and theta (4–8 Hz) oscillations have been associated with distinct but complementary roles in neural processing. Alpha rhythms are primarily linked to inhibition and attentional gating, where increased alpha power helps to filter out irrelevant sensory information by suppressing activity in areas not directly involved in a task, thus aiding selective attention \cite{klimeschEEGAlphaOscillations2007,jensenShapingFunctionalArchitecture2010}. This inhibitory function allows alpha to act as a ``gate" in sensory processing areas, modulating cortical activity and stabilizing cognitive processing \cite{hanslmayrRoleOscillationsTemporal2011}. In contrast, theta oscillations are closely tied to cognitive control, working memory, and inter-regional synchronization, particularly within frontal-midline regions during tasks requiring sustained attention and decision-making \cite{klimeschEEGAlphaTheta1999,sederbergThetaGammaOscillations2003,voytekShiftsGammaPhase2010}.
% Furthermore, gamma-oscillations propagate in the feedforward direction, whereas alpha-oscillations propagate in the feedback direction \cite{kerkoerleAlphaGammaOscillations2014}.
% Recent studies have explored how predictions are encoded in neural oscillations, particularly in the alpha and beta frequency bands. Alpha oscillations have been found to carry stimulus-specific visual predictions before stimulus onset, influencing subsequent perceptual performance \cite{hetenyiPrestimulusAlphaOscillations2024a}. Similarly, alpha/low-beta oscillations in the occipital cortex have been shown to predictively encode the position of moving stimuli, supporting the view of these rhythms as a spectral ``fingerprint" of hierarchical predictive processing \cite{turnerVisualInformationPredictively2023}. Prestimulus alpha oscillations in multisensory networks representing grapheme/phoneme associations increase with predictions and correlate with early sensory component amplitudes, suggesting a role in selective amplification of predicted information \cite{mayerExpectingSeeLetter2016}.

% Similarly, theta’s role in cross-frequency coupling, especially phase-amplitude coupling (PAC) with gamma oscillations, enables it to organize and integrate information across neural networks \cite{lismanThetaGammaNeuralCode2013}.

% Theta and gamma oscillations PAC, where the phase of the slower theta oscillation (4–8 Hz) modulates the amplitude—specifically, the envelope—of the faster gamma oscillation (30–80 Hz). In PAC, the gamma envelope, representing the overall magnitude or intensity of the gamma signal, waxes and wanes in sync with the theta phase, creating a coupling that helps encode and structure complex information, such as the rhythmic and phonemic components of speech \cite{scheffer-teixeiraCrossfrequencyPhasephaseCoupling2016}. This coupling enables theta oscillations to segment speech into syllabic units, while the gamma envelope encodes finer details within each segment, supporting hierarchical language processing \cite{giraudCorticalOscillationsSpeech2012}.

% EEG and MEG studies show that during selective listening, low-frequency cortical responses in the delta (1–3 Hz) and theta (3–7 Hz) bands preferentially track the temporal envelope of attended speech, allowing the brain to selectively tune into relevant auditory information \cite{dingNeuralCodingContinuous2012,osullivanAttentionalSelectionCocktail2015}. ECoG studies further reveal that the slowly varying envelopes of high-frequency responses in the high-gamma range ($>$70 Hz) also track attended speech, highlighting the role of high-gamma power in auditory attention \cite{mesgaraniSelectiveCorticalRepresentation2012,golumbicMechanismsUnderlyingSelective2013}. In particular, in \cite{golumbicMechanismsUnderlyingSelective2013}, brain activity is seen to dynamically track speech streams using both low-frequency phase and high-frequency amplitude fluctuations


% In Viswanathan (2019) \cite{viswanathanElectroencephalographicSignaturesNeural2019}, a key focus was on feature selection in neural signal processing, specifically in the context of selective attention to auditory stimuli. They reported that for low-frequency bands (delta, theta, alpha, and beta), the filtered EEG signal itself was treated as a feature. However, for higher-frequency gamma bands, they used only the amplitude envelopes as features and discarded phase information, a decision inspired by findings from electrocorticography (ECoG) studies indicating that high-gamma amplitude envelopes track attended speech selectively over ignored speech \cite{mesgaraniSelectiveCorticalRepresentation2012,golumbicMechanismsUnderlyingSelective2013}.

% Furthermore, for the alpha (8–12 Hz) and beta (12–30 Hz) bands, Viswanathan included the amplitude envelopes as additional features separate from the filtered EEG signal. This choice was motivated by evidence that alpha power fluctuates coherently with attended stimuli (Wöstmann et al., 2016) and that beta power varies systematically with task demands across a range of cognitive and motor tasks (Engel and Fries, 2010). By incorporating both filtered signals and amplitude envelopes in these bands, Viswanathan aimed to capture both the steady-state and fluctuating attentional effects within the neural data, supporting a richer, more nuanced understanding of the neural correlates of selective auditory attention.

% This feature selection approach, outlined by Viswanathan (2019), includes treating the filtered EEG signal in delta, theta, alpha, and beta bands as a feature while using only the amplitude envelope for higher-frequency gamma bands, motivated by findings from ECoG studies. In the alpha (8–12 Hz) and beta (12–30 Hz) bands, both the filtered signal and the amplitude envelope are considered, as alpha power coheres with attentional focus (Wöstmann et al., 2016), and beta-band power varies across cognitive and motor tasks (Engel and Fries, 2010). Wöstmann et al. (2021) further show that alpha oscillations serve a dual role as a spatio-temporal attentional filter: alpha power lateralizes with spatial attention, increasing in the hemisphere ipsilateral to the attended side and decreasing in the contralateral hemisphere. This lateralization can be temporally modulated, as demonstrated by MEG recordings where alpha power lateralization was stronger for spatially attended cues when participants had foreknowledge of a target’s timing. The lateralization precisely at temporally cued onsets of attended numbers indicates alpha’s role in dynamically filtering spatial and temporal attention to optimize perceptual performance.


% Together, alpha and theta rhythms create a dynamic balance between local inhibition and long-range coordination, supporting efficient neural processing and flexible cognitive functioning across various demands \cite{kaplanMedialPrefrontalTheta2014, friesRhythmsCognitionCommunication2015}.


% We previously introduced a laminar neural mass model (LaNMM) designed to capture both superficial-layer fast and deeper-layer slow oscillations along with their interactions, such as phase-amplitude coupling (PAC) and amplitude-amplitude anticorrelation (AAC)\cite{sanchez-todoPhysicalNeuralMass2023}. The spectrolaminar motif is ubiquitous across the primate cortex \cite{mendoza-hallidayUbiquitousSpectrolaminarMotif2024}. This framework allows for a dynamic instantiation of the Comparator, facilitating the analysis of how prediction errors and top-down predictions may be performed across different cortical depths. LaNMM combines conduction physics with neural mass models (NMMs) to simulate depth-resolved electrophysiology. Cortical function emerges from multi-scale networks, and NMMs provide a high-level representation of the mean activity of large neuronal populations. LaNMM builds on this by incorporating realistic conduction properties, enabling a detailed simulation of how slow and fast oscillatory sources interact across layers.
% By applying LaNMM to laminar recordings from the macaque prefrontal cortex, we generated a minimal model capable of reproducing coupled oscillations across layers. This allowed us to optimize LaNMM parameters to match functional connectivity (FC) patterns derived from empirical data. Here, FC was defined by the covariance between bipolar voltage measurements across cortical depths, with optimal configurations reproducing observed FC patterns, specifically generating fast activity in superficial layers and slow oscillations across deeper layers.
% This framework not only aligns with recent findings in cortical oscillatory dynamics but also offers a practical platform for investigating the role of the Comparator. By analyzing the PAC and AAC patterns in LaNMM simulations, we gain insight into how fast and slow oscillatory interactions may underpin prediction error generation, routing, and inhibition in hierarchical cortical processing.

% LaNMM fits comfortably in the context of hierarchical processing and predictive coding. There exist clear asymmetries in hierarchical cortical connectivity, particularly in the distinct roles of feedforward (FF) and feedback (FB) pathways in sensory processing \cite{bastosDCMStudySpectral2015}.  Anatomical studies established that FF pathways typically originate from superficial layers and project to granular layers, while FB pathways arise from deeper layers, connecting to non-granular areas, supporting a hierarchical processing model. This anatomical arrangement is reflected physiologically: FF pathways exhibit strong excitatory signals, using ionotropic receptors, while FB pathways are more modulatory, involving both ionotropic and metabotropic receptors and acting at dendritic arbors. Functionally, FF and FB connections also differ in frequency domains. FF signals often use gamma-band frequencies and are associated with prediction error transmission, while FB signals utilize lower alpha and beta frequencies, hypothesized to convey predictions and aligning with predictive coding models. In this framework, FF connections deliver prediction errors to higher-order areas, while FB signals transmit top-down predictions to explain sensory input. Experimental studies support this model, showing that gamma coherence is stronger in FF pathways, while alpha/beta coherence is more prominent in  FB pathways, suggesting a mechanism where faster frequencies convey discrepancies and slower frequencies signal model-driven predictions \cite{bastosDCMStudySpectral2015}. Finally, in hierarchical processing, the selection of ascending information by adjusting the ‘volume’ or gain of prediction errors that compete for influence over higher levels of processing \cite{fristonLFPOscillationsWhat2015}. 


% \subsection{General model for information encoding and processing}
% In digital communication systems, information is usually represented as a sequence of binary digits (0’s and 1’s). The physical encoding, transportation, and processing of information in computers are achieved through the manipulation of electrical currents within transistors. These components leverage semiconductor properties to control current flow, thereby enabling the execution of complex computational tasks. However, in analog systems and biological networks, information is conveyed through continuous signals. Amplitude modulation (AM) radio is a classic example, where information is encoded in the amplitude envelope of a high-frequency carrier wave. Similarly, in neural systems, the amplitude envelope of oscillatory activity can carry meaningful information. In the framework of the brain as an oscillatory computational system, information may be encoded in different ways, for example, in the power, envelope, or phase \cite{buzsakiNeuronalOscillationsCortical2004} of signals. More generally, we may think of information as encoded by the signal themselves (not some features of it), just as audio waves encode information. There is evidence that slow-wave signals encode information in this manner, with high gamma-band activities exhibiting amplitude-modulation at the same rate as the stimulus envelope of speech \cite{tamuraCorticalRepresentationSpeech2023}. The seminal study by Pasley et al. (2012) \cite{pasleyReconstructingSpeechHuman2012} successfully reconstructed speech by decoding the high gamma power envelope from ECoG recordings and mapping it to the spectrogram of the speech signal. The high gamma envelope was crucial because it tracked the amplitude and rhythm of the speech signal closely, allowing the researchers to recreate a spectrogram that could then be transformed back into an audible approximation of the original speech.
% This demonstrated that gamma-band activity in the auditory cortex contains rich, temporally precise information about the auditory signal, which can be leveraged for reconstructing intelligible sounds. 


 

% Crucially, we will consider the idea of coarse-grained multiscale information encoding. %Figure~\ref{fig:multiscale} provides an example of a multi-scale nested signal generated in Python, spanning 6 seconds. The fastest oscillation is a 60 Hz “gamma” carrier, whose amplitude is modulated by a 4 Hz “theta” wave. In turn, the theta wave’s amplitude is further modulated by a 1 Hz low-frequency envelope. This hierarchy illustrates how slower signals can shape the amplitude fluctuations of faster rhythms, reflecting a nested, multiscale structure.




% Thus, in our framework, information is not only carried by the raw signal (for instance, an alpha band oscillation) but also by its successively extracted envelopes across multiple time scales. Building on the notion of hierarchical temporal organization in brain signals \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiScalingBrainSize2013} (Figure~\ref{fig:logarithmic_hierarchy}), we propose that meaningful structure can be revealed when we move from the primary oscillation to its envelope, and then to the envelope of that envelope, and so on. Each “layer” of the envelope captures fluctuations at progressively slower time scales, enabling us to disentangle the nested rhythmic features that are otherwise hidden in the raw signal. In this framework, the laminar model acts as a machine that systematically extracts these envelopes—layer by layer—and compares them, thus exposing the rich, multi-level temporal structure of neural activity. This framework is further discussed below.

 









 

 

% \subsection{Neural Coding Schemes: Rate, Temporal, Phase, and Burst Codes}

% Researchers have long debated how neurons encode information, contrasting \textbf{rate coding} with various \textbf{temporal coding} strategies. In rate coding, information is carried by a neuron’s average firing rate (spikes per second) over some time window. In contrast, temporal codes rely on the precise timing or patterns of spikes. This includes \textit{spike timing codes} (where the exact millisecond timing or the order of spikes matters) and \textit{synchrony codes} (where coordinated firing across neurons can signal relationships or “binding” of stimulus features). Indeed, an \textit{ongoing debate} asks how much information is conveyed by precise spike timing versus mean firing rates \cite{zeldenrustNeuralCodingBurstsCurrent2018}. Temporal codes can enable extremely rapid information readout -- for example, firing-order or first-spike codes can support fast sensory discriminations that would be too quick for pure rate codes \cite{carianiTimeEssenceNeural2022}. In such schemes, which neurons fire first (and at what millisecond) carries meaning, allowing decisions on the order of tens of milliseconds (as observed in vision and audition).

% \textbf{Phase coding} is a special case of temporal coding where spikes convey information by their phase with respect to an ongoing neural oscillation. For instance, hippocampal place cells not only increase firing rate at a particular location (rate code for position), but also fire at progressively earlier phases of the local theta rhythm as an animal moves through the place field. This \textit{phase precession} is thought to provide additional information about position or timing that complements the rate code. More generally, spike timing relative to oscillatory phase can multiplex information: neurons can encode one aspect of a stimulus in their firing rate and another in the phase at which they fire \cite{lismanThetaGammaNeuralCode2013}. Evidence in the hippocampus supports the idea that dual oscillations (theta and gamma) form a \textit{theta--gamma code} for multiple items in sequence -- each theta cycle ($\sim$100 ms) is subdivided into gamma subcycles that represent different pieces of information in an ordered manner \cite{lismanThetaGammaNeuralCode2013}. In this way, phase coding by nested rhythms can create an internal sequencing of information (as seen in spatial navigation and possibly in multi-item working memory).

% \textbf{Burst coding} adds another layer to neural encoding. Neurons often fire \textit{bursts} -- brief clusters of spikes -- in addition to isolated spikes. Bursts have been hypothesized to serve distinct functional roles. One view is that bursts simply enhance reliability at synapses: a barrage of spikes in quick succession can overcome synaptic transmission failures. However, bursts might also carry specific information \textit{in their internal structure}. Researchers ask whether the number of spikes in a burst, or the precise inter-spike intervals within a burst, encode unique information beyond a simple “there was a burst” signal \cite{zeldenrustNeuralCodingBurstsCurrent2018}. Recent reviews highlight that bursts \textit{do} convey information and are not only for reliability \cite{zeldenrustNeuralCodingBurstsCurrent2018}. For example, bursts and single spikes may form \textbf{parallel codes} in the same neuron’s output, each relating to different stimulus features \cite{zeldenrustNeuralCodingBurstsCurrent2018}. A burst could represent a slower or more significant event, while single spikes embedded between bursts represent higher-frequency details \cite{zeldenrustNeuralCodingBurstsCurrent2018}. Bursts also have a disproportionate impact on targets -- acting as a “wake-up call” that can reset or enhance responsiveness in downstream neurons \cite{zeldenrustNeuralCodingBurstsCurrent2018}. This has led to the idea that bursts might flag important events (an attentional \textit{“searchlight”} signal \cite{zeldenrustNeuralCodingBurstsCurrent2018}) or indicate the coincidence of multiple inputs (when generated by dendritic spike events \cite{zeldenrustNeuralCodingBurstsCurrent2018}). In summary, contemporary perspectives view neural coding as \textit{multi-faceted}: firing rates, spike timing, phases of oscillations, and bursts all provide complementary channels of information.

% \subsection{Oscillations and Information Processing}

% Brain oscillations (rhythmic fluctuations of neural activity) are now recognized as central to neural information processing. Oscillations provide a temporal structure -- a scaffold of cycles -- that can organize when neurons fire. One consequence is \textbf{phase-dependent excitability}: the probability or impact of a spike can depend on \textit{when} in an oscillatory cycle it occurs \cite{canoltyFunctionalRoleCrossfrequency2010}. For example, if a neuron is most excitable at the trough of a local field potential oscillation, a spike or input arriving at that phase will be processed more effectively than one arriving at the peak \cite{canoltyFunctionalRoleCrossfrequency2010}. In this way, oscillations can gate information flow by timing it to phases of high responsiveness. Oscillatory synchrony also enables effective communication across brain regions: when groups of neurons oscillate in phase, their spikes arrive at target areas in a coordinated manner, summing to have a larger impact. This is the basis of the \textbf{communication-through-coherence} hypothesis, which proposes that coherence (phase alignment) between sender and receiver neural groups facilitates selective information transmission \cite{friesRhythmsCognitionCommunication2015}. Notably, different frequency bands may subserve different signaling roles. \textbf{Gamma-band (30--100 Hz)} oscillations often reflect local, fast information processing (e.g., encoding sensory details or content), while slower rhythms like \textbf{alpha/beta (8--20 Hz)} can carry top-down influences (e.g., attentional or predictive signals) that modulate the gamma activity \cite{friesRhythmsCognitionCommunication2015}. Indeed, in one model, feed-forward (bottom-up) signals are carried by high-frequency gamma synchrony, whereas feed-back (top-down) signals are conveyed by slower alpha/beta oscillations that can modulate the gamma timing \cite{friesRhythmsCognitionCommunication2015}. Even the theta band ($\sim$4--8 Hz) has been implicated in sampling or sequencing information -- for instance, attention might effectively “sample” inputs at a theta rhythm \cite{friesRhythmsCognitionCommunication2015}. Thus, multiple oscillations interact to make neural communication \textit{effective, precise, and selective} \cite{friesRhythmsCognitionCommunication2015}.

% Beyond their role in coordinating timing, oscillations also allow the brain to \textbf{multitask or multiplex} information streams. Because oscillations occur at various frequencies, neural signals can be segregated in the frequency domain. A striking observation is that brain activity organizes into a \textbf{hierarchy of oscillatory bands} -- from slow ($<$1 Hz, delta) up through theta, alpha, beta, gamma, and even faster “ripple” oscillations -- and this hierarchy is preserved across brain regions and species \cite{penttonenNaturalLogarithmicRelationship2003, buzsakiBrainRhythmsNeural2012}. These rhythms are not independent; they \textbf{interact across frequencies}, supporting cross-scale communication. In other words, the brain appears to use a layered timing structure, where slow fluctuations can align or modulate faster ones. This is evident in measures of \textbf{oscillatory amplitude modulation}: for example, the amplitude (power) of a high-frequency gamma oscillation often waxes and wanes in step with the phase of a slower theta wave. Such \textit{phase-amplitude coupling} suggests that slower oscillations can control the gain of faster oscillatory activity. Because slow oscillations tend to involve larger, more distributed networks (operating on behavioral or cognitive timescales), and fast oscillations involve local circuits (processing fine details), their coupling provides a way to bridge \textbf{multiple spatial and temporal scales of processing} \cite{canoltyFunctionalRoleCrossfrequency2010}. In fact, accumulating evidence indicates that information processing in the brain is intrinsically \textbf{multi-scale}, and a \textit{hierarchy of coupled oscillations} is well-suited to regulate this integration across scales \cite{canoltyFunctionalRoleCrossfrequency2010}. Through oscillations, the brain can synchronize distant regions, segment continuous streams into discrete “frames,” and dynamically route signals -- all essential for complex cognitive functions such as attention, memory, and perception [\cite{carianiTimeEssenceNeural2022}.

% \subsection{Cross-Frequency Coupling and Nested Hierarchies}

% \textbf{Cross-frequency coupling (CFC)} refers to interactions between oscillations at different frequencies, and it is a key mechanism for hierarchical encoding in neural circuits. In nested \textbf{oscillatory hierarchies}, a slower rhythm can organize the occurrence or amplitude of faster rhythms [\cite{buzsakiBrainRhythmsNeural2012}. One well-studied example is theta--gamma coupling: the phase of a 5 Hz theta oscillation modulates the amplitude or timing of $\sim$50 Hz gamma bursts. This nesting effectively creates “packets” of high-frequency activity within each cycle of the slow wave, which can be viewed as a higher-order coding unit. Lisman and colleagues have argued that theta--gamma coupling constitutes a neural code capable of ordering multiple items (as mentioned above for hippocampal sequences) \cite{lismanThetaGammaNeuralCode2013}. More generally, CFC has been observed across many brain areas and frequency combinations, suggesting a \textbf{general hierarchical organization} of brain rhythms \cite{buzsakiBrainRhythmsNeural2012}. The \textbf{strength} of phase-amplitude coupling often reflects behavioral or cognitive states: it varies with task demands and learning, and higher coupling can correlate with better performance \cite{canoltyFunctionalRoleCrossfrequency2010}. For example, during learning tasks, the degree of theta--gamma coupling in relevant circuits changes rapidly with new information and can predict memory retention or decision success \cite{canoltyFunctionalRoleCrossfrequency2010}. In working memory paradigms, stronger coupling between prefrontal low-frequency rhythms and high-frequency activity has been linked to greater memory capacity or focus, implying that coupling helps coordinate the maintenance of multiple items in memory \cite{buzsakiBrainRhythmsNeural2012}. Thus, CFC is not just an epiphenomenon -- it appears to \textbf{serve functional roles} in computation, communication, and plasticity \cite{canoltyFunctionalRoleCrossfrequency2010}.

% One way to understand nested oscillations is as a \textbf{layered coding scheme}, akin to a compositional syntax. Just as letters form words and words form sentences, fast oscillatory cycles can be grouped by slower cycles into larger assemblies. Buzs\'aki and colleagues describe this as a potential \textbf{“neural syntax”}, where cross-frequency coupling imposes syntactic rules on neural firing -- i.e., which spike patterns (or gamma bursts) go together in time \cite{buzsakiBrainRhythmsNeural2012}. The slower rhythm provides a reference frame that segments and orders the faster activity, making it easier for downstream “readers” (neuronal targets) to interpret the spikes. In practical terms, if individual spikes or high-frequency bursts are like letters conveying fine details, a slower oscillation can concatenate these into intelligible “words” or “phrases” that carry a compound meaning \cite{buzsakiBrainRhythmsNeural2012}. This hierarchical \textbf{packaging of information} could underlie complex pattern generation and parsing in the brain. Notably, cross-frequency coupling comes in different forms -- not only phase--amplitude (slow phase modulates fast amplitude) but also phase--phase locking (integer frequency relationships) and amplitude--amplitude correlations between bands \cite{buzsakiBrainRhythmsNeural2012}. Each type of coupling might support different computations or interactions. Phase--phase locking (often an $n : m$ frequency relationship) can temporally align neural events across scales (e.g., one gamma burst every cycle of a beta rhythm, etc.), whereas phase--amplitude coupling effectively uses the slow wave as a carrier signal that gates fast content. Across the cortex and hippocampus, virtually all co-occurring rhythms can exhibit CFC \cite{buzsakiBrainRhythmsNeural2012}, indicating \textbf{ubiquitous nesting} from very slow ($<$0.1 Hz) oscillations up to fast ripples ($\sim$150--200 Hz). This nested organization means the brain operates with a deep \textbf{temporal hierarchy}, where higher-order cortical areas or network states (often oscillating slower) can modulate and orchestrate the fine-scale activity of local circuits (oscillating faster). Such dynamics have been hypothesized to support cognitive functions that require integration of information over multiple timescales -- for instance, parsing speech (syllables within words within phrases) or forming episodic memories (encoding sequences of events in order).

% \subsection{Integration of Coding Schemes in Hierarchical Layers}

% Rather than using any single coding scheme in isolation, neural systems likely \textbf{integrate multiple codes} that interact across layers of processing. An emerging view is that the brain multiplexes information much like a communication system, using different “channels” (whether defined by neuron populations, time windows, or frequency bands) to carry different aspects of information simultaneously [\cite{carianiTimeEssenceNeural2022} L159-L167]. The architecture of the brain -- with its layered cortex and recurrent loops -- supports such parallel and hierarchical coding. For example, a single neuron could use a combination of rate and temporal codes: it might increase firing rate to signal the intensity of a stimulus, while the precise spike timing relative to a network oscillation conveys qualitative information about that stimulus. Similarly, as mentioned, \textbf{single spikes vs. bursts} from the same cell may form a dual code \cite{zeldenrustNeuralCodingBurstsCurrent2018}. Experimental evidence in sensory systems shows that bursts often code for \textit{slow} or context features, whereas isolated spikes code for \textit{rapid detail} features, effectively splitting the information stream by timescale \cite{zeldenrustNeuralCodingBurstsCurrent2018}. This can be seen as a form of \textbf{time-division multiplexing} -- using the timing pattern of spikes (clustered vs.\ singular) to send two messages on one output line.

% At the network level, oscillations of different frequencies provide distinct \textbf{communication channels} that can operate concurrently. This resembles \textbf{frequency-division multiplexing} in engineered systems: distinct frequency bands can carry separate streams without interference \cite{carianiTimeEssenceNeural2022}. Brain rhythms might dynamically assign certain computations to gamma band activity and others to beta or theta bands, all happening in parallel. Critically, these channels are not independent; they \textbf{interact} to ensure the streams are recombined appropriately for unified perception and behavior. The interplay of gamma, beta, alpha, theta, etc., can thus implement a layered processing hierarchy. For instance, in attention and perception, \textbf{fast gamma oscillations} in visual cortex can encode the features of a currently attended object, while \textbf{alpha oscillations} regulate the timing of these gamma bursts and segregate other objects in the scene by suppressing or desynchronizing their gamma activity. A recent framework for visual attention proposes exactly this kind of hierarchical multiplexing: different elements of an object are processed in separate high-frequency bursts (gamma) \textit{within} a single cycle of an alpha rhythm, ensuring they are bound together as one object, whereas different objects are processed in successive alpha cycles (thereby kept separate in time) \cite{bonnefondVisualProcessingHierarchical2024}. Furthermore, even slower oscillations like theta (perhaps tied to eye movements or attentional shifts) could align these alpha cycles when scanning a scene \cite{bonnefondVisualProcessingHierarchical2024}. This \textbf{nested multiplexing} idea illustrates how multiple encoding schemes (phase coding, bursting, oscillatory gating) can dovetail: a slow rhythm sets up a temporal frame for each object (like a “time slot”), within which rapid bursts encode the object’s features by phase coding along the alpha cycle \cite{bonnefondVisualProcessingHierarchical2024}. Such a system is flexible and hierarchical -- akin to a multi-layer computer architecture where signals are modulated and demodulated across different layers of a network.

% From a theoretical standpoint, researchers have drawn analogies between neural coding and the layered protocols of communication systems. Cariani and others suggest that the brain might implement forms of \textbf{time-division, frequency-division, and code-division multiplexing} to handle the immense complexity of information it processes \cite{carianiTimeEssenceNeural2022}. In this view, neural oscillations and precisely timed spike patterns act like carriers and modulators, allowing neurons to \textbf{broadcast multiplexed, temporally-patterned signals} that can be selectively read by receivers tuned to the appropriate timing or frequency \cite{carianiTimeEssenceNeural2022}. Just as radio transmitters broadcast at different frequencies to avoid mutual interference, different neural assemblies might oscillate at distinct frequencies to avoid cross-talk and then use moments of synchrony (phase alignment) to communicate when needed. The concept of \textbf{polychronous ensembles} is another example of a layered temporal code: Izhikevich and colleagues proposed that neurons with specific axonal conduction delays can fire in reproducible spatiotemporal patterns (with precise delays between spikes across neurons), creating a high-dimensional coding space for patterns (a form of \textit{time-offset code}). These ensembles can be active in overlapping ways as long as their spike timing patterns differ, analogous to code-division multiplexing. Such polychronous firing patterns essentially embed a hidden layer of temporal code that only a circuit with matching delay properties could interpret -- hinting at a kind of combinatorial coding that goes beyond simple rate or synchrony. While experimental evidence for polychronization is still emerging, it aligns with the broader idea that \textbf{neuronal networks exploit temporal diversity for coding capacity}.

% In summary, modern perspectives conceive of neural encoding as inherently \textbf{hierarchical and multi-layered}. Low-level neural events (spikes) are organized in time by intermediate patterns (bursts, synchrony, phase-locking), which are further structured by emergent global rhythms (oscillations, couplings). This layered structure allows the nervous system to encode \textbf{multiple features and contexts simultaneously} and to flexibly route information as task demands change. The interplay of different codes -- rate, temporal, phase, and bursts -- in a hierarchical manner enables something analogous to a \textbf{neural information architecture}. It ensures that local details and global context are integrated, much like how, in a computer, bits are organized into bytes, packets, and higher-level data structures. Crucially, a growing body of theoretical models and empirical data supports this view. We see evidence from hippocampal theta--gamma coding of sequences \cite{lismanThetaGammaNeuralCode2013}, to cross-frequency coupling correlating with learning and memory performance \cite{canoltyFunctionalRoleCrossfrequency2010,buzsakiBrainRhythmsNeural2012}, to human neuroimaging studies showing nested oscillations tracking multi-level linguistic structures during speech comprehension \textsuperscript{\dag}. All of these findings converge on the idea that \textbf{neuronal information is encoded across multiple temporal scales} in a coordinated fashion. This hierarchical encoding may be fundamental to how the brain achieves efficient, robust, and context-sensitive computation. 

\end{document}




% \subsection{PEIX: A Normalized Slope Measure of Excitation–Inhibition Balance}
% \label{sec:PEIX}
% \newcommand{\peix}{\mathrel{\raisebox{-0.1ex}{\scalebox{1.5}{\reflectbox{$\propto$}}}}}

% Neural populations are often modeled by a sigmoid function,
% \[
% S(V) = \frac{2e_0}{1+\exp\big(r(V_0-V)\big)},
% \]
% where \(V_0\) is the half-maximum potential (the linear operating point), \(e_0\) is the maximum firing rate, and \(r\) sets the slope. The derivative,
% \[
% S'(V) = \frac{2e_0\, r\,\exp\big(r(V_0-V)\big)}{\Bigl(1+\exp\big(r(V_0-V)\big)\Bigr)^2},
% \]
% peaks at \(V=V_0\) with \(S'(V_0)=\frac{e_0r}{2}\) and decreases as the system enters its excitatory (\(V \gg V_0\)) or inhibitory (\(V \ll V_0\)) saturation regimes.

% To quantify the deviation from the ideal linear regime, we define the dimensionless variable
% \[
% x = r\big(\langle V\rangle - V_0\big),
% \]
% where \(\langle V\rangle\) is the average membrane potential. By normalizing the local slope \(S'(V)\) with its maximum value at \(V_0\), we obtain a measure of nonlinearity that distinguishes between excitation and inhibition. The \textbf{Population Excitation-Inhibition Index (PEIX)} is defined as
% \[
% \peix(x) = -\operatorname{sign}(x) \left(\frac{4\,\exp(-x)}{\Bigl(1+\exp(-x)\Bigr)^2} - 1\right).
% \]
% This formulation ensures that:
% \begin{itemize}
%   \item \(\peix(x) \approx 0\) when \(\langle V\rangle \approx V_0\) (linear regime),
%   \item \(\peix(x) > 0\) when \(\langle V\rangle > V_0\) (excitatory state),
%   \item \(\peix(x) < 0\) when \(\langle V\rangle < V_0\) (inhibitory state),
%   \item \(|\peix(x)| \approx 1\) in the saturated regimes.
% \end{itemize}
% Thus, by focusing on the normalized slope of the sigmoid, PEIX provides a compact, dimensionless index of both the deviation from the optimal operating point and the underlying excitation–inhibition balance.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 


% \section{Comparator (Innovation) Role in a Hierarchical Kalman Filter}

% A standard Kalman filter is a sequential algorithm for estimating a hidden state 
% $\mathbf{x}_k$ of a system at each discrete time step $k$. 
% It operates in two main steps:

% \begin{enumerate}
%   \item \textbf{Prediction:} Use the previous state estimate and the system model to 
%   predict the next state.
%   \item \textbf{Update:} Incorporate new measurements to correct the predicted state, 
%   guided by the \emph{innovation} (or \emph{comparator output}), which is the discrepancy 
%   between predicted and observed measurements.
% \end{enumerate}

% \paragraph{Standard Formulation.}
% We typically assume a linear state-space model:
% \[
% \begin{cases}
% \mathbf{x}_k \;=\; \mathbf{A}_k\,\mathbf{x}_{k-1} + \mathbf{B}_k\,\mathbf{u}_{k-1} + \mathbf{w}_{k-1},\\
% \mathbf{z}_k \;=\; \mathbf{H}_k\,\mathbf{x}_k + \mathbf{v}_k,
% \end{cases}
% \]
% where $\mathbf{x}_k$ is the state at time $k$, $\mathbf{u}_k$ is a known control input, 
% and $\mathbf{z}_k$ is the measurement at time $k$. The matrices 
% $\mathbf{A}_k$, $\mathbf{B}_k$, and $\mathbf{H}_k$ define how the system evolves and 
% how measurements map from the state. The noise terms $\mathbf{w}_{k-1}$ and $\mathbf{v}_k$ 
% represent process and measurement noise, respectively, with covariances 
% $\mathbf{Q}_{k-1}$ and $\mathbf{R}_k$.

% At each step $k$, the filter does:

% \[
% \textbf{(1) Prediction Step:}
% \quad
% \begin{cases}
% \mathbf{x}_{k}^{\mathrm{pred}} \;=\; \mathbf{A}_k\,\mathbf{x}_{k-1}^{\mathrm{upd}}
% \;+\; \mathbf{B}_k\,\mathbf{u}_{k-1},\\
% \mathbf{P}_{k}^{\mathrm{pred}} \;=\; \mathbf{A}_k\,\mathbf{P}_{k-1}^{\mathrm{upd}}\,\mathbf{A}_k^{T}
% \;+\; \mathbf{Q}_{k-1},
% \end{cases}
% \]
% where $\mathbf{x}_{k}^{\mathrm{pred}}$ and $\mathbf{P}_{k}^{\mathrm{pred}}$ are the 
% \emph{predicted} state and its error covariance, respectively. They depend only on 
% information up to time $k-1$.

% \[
% \textbf{(2) Innovation (Comparator) Output:}
% \quad
% \boldsymbol{\nu}_k \;=\; \mathbf{z}_k \;-\; \mathbf{H}_k\,\mathbf{x}_{k}^{\mathrm{pred}}.
% \]
% This quantity $\boldsymbol{\nu}_k$ is the \emph{difference} between the 
% \emph{predicted measurement} $\mathbf{H}_k\,\mathbf{x}_{k}^{\mathrm{pred}}$ 
% and the \emph{actual} measurement $\mathbf{z}_k$; a large discrepancy indicates 
% a large prediction error.

% \[
% \textbf{(3) Kalman Gain:}
% \quad
% \mathbf{K}_k \;=\; \mathbf{P}_{k}^{\mathrm{pred}}\,\mathbf{H}_k^{T}\,
% \Bigl(\mathbf{H}_k\,\mathbf{P}_{k}^{\mathrm{pred}}\,\mathbf{H}_k^{T} + \mathbf{R}_k\Bigr)^{-1}.
% \]
% \[
% \textbf{(4) Update Step:}
% \quad
% \begin{cases}
% \mathbf{x}_{k}^{\mathrm{upd}} \;=\; \mathbf{x}_{k}^{\mathrm{pred}} + \mathbf{K}_k\,\boldsymbol{\nu}_k,\\[4pt]
% \mathbf{P}_{k}^{\mathrm{upd}} \;=\; \bigl(\mathbf{I} - \mathbf{K}_k\,\mathbf{H}_k\bigr)\,\mathbf{P}_{k}^{\mathrm{pred}}.
% \end{cases}
% \]
% Hence, $\mathbf{x}_{k}^{\mathrm{upd}}$ and $\mathbf{P}_{k}^{\mathrm{upd}}$ become the \emph{posterior} 
% (or “corrected”) estimates at time $k$, which are then used to \emph{predict} time $k+1$ in the same fashion.

% \medskip

% \paragraph{Hierarchical Extension.}
% In this work, we adopt a \emph{hierarchical} perspective akin to multi-scale or 
% multi-layer processing: rather than applying the Kalman filter to a single timescale, 
% we apply a version at \emph{each} level in a cortical hierarchy (or each frequency 
% band/envelope). In particular:

% \begin{figure} [t]
%   \centering
%   \includegraphics[width=0.99\linewidth]{figures/collisions.png}
%   \caption{Sequential filtering.}
%   \label{fig:collisions}
% \end{figure}


% \begin{itemize}
% \item \textbf{Coarse-Grained Predictions.} 
%  At each layer, the state $\mathbf{x}_{k}^{(\ell)}$ represents coarser features of the 
%  signal (e.g., slower dynamics or envelope information). The update aims to cancel out 
%  prediction error \emph{at that layer’s timescale}.
% \item \textbf{Comparator Mechanism.} 
%  Each layer receives an “incoming observation” $\mathbf{z}_{k}^{(\ell)}$ (potentially 
%  derived from the layer below) and compares it to the \emph{layer’s own} predicted 
%  measurement $\mathbf{H}^{(\ell)}\mathbf{x}_{k}^{(\ell), \mathrm{pred}}$. This 
%  innovation drives the layer-specific correction.
% \item \textbf{Up/Down Interactions.} 
%  Higher layers process information at lower temporal frequencies (or coarser resolution), 
%  while lower layers operate at finer scales. The mismatch (innovation) at each level 
%  feeds back to refine predictions at that level and potentially provides feedback to 
%  adjacent layers.
% \end{itemize}

% Thus, although the \emph{mathematical} form of each layer’s filter update appears similar 
% to a standard Kalman filter, the \textbf{intent} and \textbf{information flow} differ: 
% \emph{each layer’s comparator “kills the error” at its own spatiotemporal granularity.} 
% In neural terms, we could interpret this as a hierarchy of cortical areas, each predicting 
% the activity (or envelopes) in the layer below, with the local “innovation” indicating the 
% prediction error that drives learning or adjustment across layers. 
% This multi-level scheme helps the system track and predict signals with \emph{layer-specific} 
% dynamics and noise characteristics, rather than a single Kalman filter struggling with 
% all temporal scales at once.


% \textbf{Kalman Filtering and Active Inference: A Least-Squares Perspective}

% \textbf{1. Kalman Filter in Least-Squares Form}

% Assumptions: We consider a linear state-space model with Gaussian noise. Let $x_k$ be the hidden state at time $k$, and $y_k$ the observation. The dynamics are $x_k = A,x_{k-1} + w_k$ and the observation model $y_k = H,x_k + v_k$, where $w_k \sim \mathcal{N}(0, Q)$ and $v_k \sim \mathcal{N}(0, R)$ are process and measurement noise (zero-mean Gaussian) with covariances $Q$ and $R$. The goal is to estimate $x_k$ given observations up to time $k$.

% Least-squares cost function: Under these linear Gaussian assumptions, the maximum a posteriori (MAP) estimate of $x_k$ (which coincides with the minimum mean-square error estimate for Gaussian noise) can be obtained by minimizing a weighted least-squares cost. Intuitively, we want $x_k$ to be close to the prior prediction $x_{k|k-1}$ (the predicted state from time $k-1$) and also close to the value that explains the new observation $y_k$. This leads to the cost function:

% $$
% J(x_k) = \underbrace{(x_k - \hat{x}{k|k-1})^T P{k|k-1}^{-1}(x_k - \hat{x}{k|k-1})}{\text{distance from prior (prediction error)}} ;+;
% $$
% $$ \underbrace{(y_k - H,x_k)^T R^{-1}(y_k - H,x_k)}_{\text{distance from measurement (observation error)}},
% $$

% where $\hat{x}{k|k-1}$ is the prior state estimate (prediction) and $P{k|k-1}$ its covariance. The first term is the squared prediction error (the difference between $x_k$ and the predicted state) weighted by the inverse of the prior covariance (the precision of the prediction). The second term is the squared observation prediction error (innovation) weighted by the measurement precision $R^{-1}$. This cost function is essentially the negative log-posterior (up to an additive constant), so minimizing it is equivalent to Bayesian MAP estimation.

% Update via prediction error minimization: To find the optimal $x_k$, we set $\nabla_{x_k} J = 0$. Differentiating and setting to zero gives the normal equations for the minimizer:

% $$
% P_{k|k-1}^{-1}(x_k - \hat{x}_{k|k-1}) - H^T R^{-1}(y_k - H,x_k) = 0.
% $$

% Re-arranging terms, we obtain:

% $$(P_{k|k-1}^{-1} + H^T R^{-1} H),x_k = P_{k|k-1}^{-1}\hat{x}_{k|k-1} + H^T R^{-1}y_k.$$

% Solving for $x_k$ yields the a posteriori estimate $\hat{x}_{k|k}$, which can be written in the familiar Kalman update form:

% $$
% \hat{x}{k|k} ;=; \hat{x}{k|k-1} + K_k,\big(y_k - H,\hat{x}_{k|k-1}\big),
% $$

% where

% $$
% K_k = P_{k|k-1} H^T, (H P_{k|k-1} H^T + R)^{-1}
% $$

% is the Kalman gain. The term $(y_k - H,\hat{x}_{k|k-1})$ is the prediction error or innovation, and $K_k$ specifies the optimal weight given to this error. Notably, this gain $K_k$ is chosen precisely to minimize the posterior error covariance (i.e. it minimizes the expected mean-squared estimation error). In other words, the Kalman filter update finds the state estimate that minimizes the least-squares cost, balancing the prediction and observation errors according to their uncertainties. This is an explicit realization of prediction error minimization: the new estimate is the old prediction plus a correction term proportional to the prediction error.

% 2. Active Inference in Least-Squares Form

% Variational free energy (VFE) functional: Active inference casts state estimation as a variational inference problem. One defines a variational free energy $F$ which the agent (or estimator) minimizes instead of directly computing the posterior. In filtering, a common choice is to use a Gaussian variational density (a Laplace approximation), so that the free energy essentially becomes (up to constant terms) the negative log joint probability of the hidden state and observations, evaluated at the current approximate estimate. For the linear Gaussian case (same $A, H, Q, R$ as above), the VFE can be written as a sum of squared prediction errors, just like the Kalman least-squares cost. In fact, under a variational Gaussian (Laplace) assumption, one finds:

% $$
% F(x_k) ;=; \frac{1}{2}(y_k - H,x_k)^T R^{-1}(y_k - H,x_k) ;+; \frac{1}{2}(x_k - A,\hat{x}{k-1|k-1})^T Q^{-1}(x_k - A,\hat{x}{k-1|k-1}) ;+; \text{const},
% $$

% where $\hat{x}{k-1|k-1}$ is the previous posterior mean (serving as the prior mean for $x_k$ via the linear dynamics). Here the first term is the sensory prediction error (observation minus expected observation $H x_k$) weighted by its precision $R^{-1}$, and the second term is the state prediction error (difference between $x_k$ and its prediction $A\hat{x}{k-1|k-1}$) weighted by the process precision $Q^{-1}$. This form of $F$ mirrors the Kalman least-squares cost above. Minimizing VFE thus corresponds to explaining observations and dynamics as well as possible under the model, using a quadratic penalty for deviations – exactly akin to a weighted least-squares estimation.

% Free-energy minimization and update rule: To perform inference, active inference minimizes $F$ with respect to the approximate posterior parameters (here, effectively the mean $x_k$ since we assume a Gaussian form). Setting $\nabla_{x_k}F = 0$ leads to the condition:

% $$
% H^T R^{-1}(y_k - H,x_k);+; A^T Q^{-1}(\hat{x}_{k-1|k-1} - A^{-1}x_k);=;0,
% $$

% which simplifies to the same normal equation obtained for the Kalman filter solution. In other words, the stationary point of $F$ yields the exact same update for $x_k$ as the Kalman filter posterior mean. We can also see this by taking partial derivatives of $F$ as in a gradient descent scheme: the VFE gradient includes terms proportional to $H^T R^{-1}(y_k - H,x_k)$ (the observation prediction error weighted by precision) and $Q^{-1}(x_k - A,\hat{x}_{k-1|k-1})$ (the state prediction error weighted by precision) ￼. Driving these gradients to zero (or performing gradient descent until convergence) gives an equilibrium point satisfying

% $$x_k = \hat{x}{k-1|k-1} + P{k|k-1} H^T (H P_{k|k-1} H^T + R)^{-1}(y_k - H,x_k),$$

% which is exactly the Kalman update formula. In fact, it has been noted that in the linear Gaussian case, minimizing variational free energy recovers the same update equations as the Kalman filter ￼. Active inference thus reproduces the Bayesian filtering solution via a least-squares (free-energy) minimization principle. Importantly, what the Kalman filter achieves in closed-form at each time step can also be achieved by an active inference agent through gradient-based minimization of VFE – the end result (optimal state estimate) is the same in this linear case.

% \textbf{3. Linking the Updates: Kalman Gain vs. Precision Weighting}

% The Kalman filter update and the active inference update can now be directly compared. The Kalman gain $K_k = P_{k|k-1}H^T (H P_{k|k-1} H^T + R)^{-1}$ determines how much the prediction is adjusted in light of new evidence. This expression can be understood in terms of precisions (inverse variances). The term $(H P_{k|k-1} H^T + R)$ in the denominator is the innovation covariance (the uncertainty in the prediction error); its inverse is the precision of the innovation. Thus $K_k$ essentially equals $P_{k|k-1}H^T$ times the precision of the prediction error. If the observation is very reliable (small $R$, high precision), $K_k$ will be larger, giving the observation prediction error more weight. If the prior estimate is very uncertain (large $P_{k|k-1}$), $K_k$ will also be larger, meaning the system trusts the new data more. This is exactly the logic of precision-weighted prediction errors: the update $\hat{x}{k|k} = \hat{x}{k|k-1} + K_k (y_k - H\hat{x}_{k|k-1})$ says that the change in the estimate is proportional to the prediction error, scaled by a factor that reflects the relative confidence in that error.

% In active inference, the principle of precision-weighted prediction errors plays a central role in the update rules. In our VFE expression above, each prediction error term is multiplied by the inverse covariance (precision) of that term. Consequently, when we take the gradient or update equations, the magnitude of each correction is weighted by precision in the same way as the Kalman gain does. For example, in the simple case of estimating a static variable with a Gaussian prior and likelihood, the posterior mean is the prior mean plus a fraction $\frac{\text{likelihood precision}}{\text{prior precision} + \text{likelihood precision}}$ of the prediction error – i.e. the mean is updated by a precision-weighted error. This is precisely the Kalman filter logic: the posterior estimate is a weighted combination of prior and new information, with weights proportional to their precisions (equivalently, inversely proportional to their variances).

% In summary, the Kalman filter’s gain $K_k$ embeds the precision weighting: it tells us how to trade off the prior estimate versus the new sensory evidence. Active inference makes this weighting explicit by formulating an objective (VFE) where precisions appear as coefficients. Minimizing that objective naturally leads to an update of the state estimate by precision-weighted prediction errors, which is mathematically equivalent to the Kalman update in the linear Gaussian case. Thus, the Kalman filter can be seen as implementing a Bayesian prediction error correction, and active inference captures the same mechanism under the umbrella of variational (free-energy) minimization. Both frameworks agree that the optimal update increment is proportional to the prediction error, weighted by the certainty (precision) of that error ￼.

% 4. Linearization and Nonlinear Extensions

% So far we assumed linear dynamics and observations. How do Kalman filtering and active inference handle nonlinear problems? In the Kalman filter world, one common approach is the Extended Kalman Filter (EKF), which linearizes the nonlinear system about the current estimate. Essentially, the EKF uses a first-order Taylor expansion of the dynamics $f(x)$ and observation function $g(x)$ to compute local approximations $A \approx \partial f/\partial x$ and $H \approx \partial g/\partial x$. It then performs a Kalman update with these local Jacobians. This can be viewed as a Gauss-Newton step or local least-squares linearization at each time step. The result is a Gaussian approximate posterior for $x_k$ (mean and covariance) updated iteratively, which is approximately optimal for weakly nonlinear systems.

% Active inference tackles nonlinearity through its variational inference foundation. Rather than explicitly linearizing the system equations, active inference uses an approximate posterior that can be updated by gradient descent on the VFE. A common choice is to assume a Gaussian form for the posterior (the Laplace approximation), which leads to update equations very similar to an EKF. In fact, under the Laplace assumption, the variational free energy minimization entails expanding the (log) posterior to second order around its peak (the mode), which is mathematically equivalent to forming a locally linear Gaussian approximation of the model ￼. The resulting filter (sometimes called a generalized filtering or dynamic expectation-maximization scheme) yields recognition dynamics that closely match extended Kalman filtering updates ￼ ￼. In other words, active inference with a Gaussian approximate density will internally perform a linearization (through the curvature of the log-likelihood and log-prior) much like the EKF does externally by Jacobians. Indeed, it has been noted that under certain conditions, the updates from active inference are equivalent to an (extended) Kalman filter for nonlinear systems ￼ ￼.

% The advantage of the active inference approach is that it provides a principled Bayesian framework for extension to more complex scenarios. Nonlinear and even non-Gaussian models can be handled by changing the variational approximation or using more sophisticated inference (e.g. one could use mixture densities, particles, or iterative message passing to better approximate the posterior). The Kalman filter, in contrast, must be specifically adapted (EKF, unscented KF, particle filter, etc.) for non-linear/non-Gaussian cases. Active inference naturally generalizes because it rests on minimizing VFE: for any given generative model $p(y, x)$, one can write down $F$ and attempt to minimize it. If one uses a Gaussian ansatz, this becomes analogous to an EKF (a Laplace method); if one uses particles, it approaches a particle filter, and so on. Thus, active inference extends Kalman filtering to nonlinear regimes by casting filtering as general variational Bayesian inference. The linear Gaussian case is a useful special case where everything is analytically tractable (and the Kalman filter emerges exactly), while in truly nonlinear cases active inference will resort to iterative refinements (gradients, fixed-point iterations) to find a solution, much like one would linearize at each step in an EKF.

% In summary, both Kalman filtering and active inference share the same core principle of Bayesian least squares estimation. Kalman filtering provides closed-form update equations under linear Gaussian assumptions, choosing gains that minimize squared-error cost ￼. Active inference formulates the same problem via a variational free-energy objective that, in the linear case, leads to identical update rules ￼. In nonlinear scenarios, Kalman filters require explicit linearization (extended or unscented transforms), whereas active inference handles them through variational approximations (e.g. Laplace) as part of the inference process. This means that active inference inherently performs prediction error minimization with precision weighting, just as the Kalman filter does, while also providing a route to systematically handle complexity beyond the reach of basic linear Kalman filtering. The connection between the two is that Kalman filtering is essentially a specific case of active inference (or Bayesian filtering) under Gaussian assumptions – making the link between prediction-error minimization in control theory and variational free-energy minimization in cognitive neuroscience explicit and mathematically concrete

% References: In the above, we used standard results from Bayesian estimation theory and linear filtering. The Kalman gain is derived by minimizing the estimation error variance ￼. The active inference update in the linear-Gaussian case yields the same form, as shown by deriving the VFE and its gradient ￼ ￼ ￼. The notion of precision-weighted prediction errors appears in both frameworks ￼. For further reading, see e.g. Friston et al. on generalized filtering and predictive coding, which discuss how under Laplace assumptions active inference reduces to an equivalent of the extended Kalman filter ￼ ￼. This demonstrates the deep connection between Kalman filtering and active inference as approaches to optimal state estimation.
% %%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


