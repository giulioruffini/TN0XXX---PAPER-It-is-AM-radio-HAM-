\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}

% Basic packages
\usepackage{graphicx}   % For images
\usepackage{hyperref}   % For hyperlinks
\usepackage[superscript]{cite}
\usepackage{tcolorbox}
\usepackage{parskip}
\usepackage{amsfonts}
\usepackage{tikz}
\usetikzlibrary{calc}  % <-- IMPORTANT for coordinate arithmetic
\usetikzlibrary{decorations.pathmorphing,arrows.meta}
\usepackage{feynmp-auto}
% Extra formatting packages
\usepackage{geometry}   % For setting page margins
\usepackage{fancyhdr}   % For customizing headers and footers
\usepackage{titlesec}   % For customizing section titles
\usepackage{setspace}   % For adjusting line spacing
\usepackage{lmodern}    % For modern font
\usepackage{xcolor}     % For color customization
\setlength{\headheight}{13.59999pt}
% Set up page margins
\geometry{
    a4paper,
    left=1in,
    right=1in,
    top=1in,
    bottom=1in
}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark}
% Set up line spacing
\onehalfspacing

% Customize section titles
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Set up headers and footers
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}  % Left-aligned section title
\fancyhead[R]{\thepage}   % Right-aligned page number
\fancyfoot[C]{\textit{}}  % Centered footer with your name

% Customize hyperlink colors
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=magenta,
    pdftitle={NE TN0XXX},    % PDF document title
    pdfauthor={G. Ruffini},             % PDF document author
}

\title{Neural Encoding through Hierarchical Amplitude Modulation\\
NE TN0434}
\author{Giulio Ruffini\thanks{giulio.ruffini@neuroelectrics.com} ... %, Edmundo Lopez-Sola, Borja Mercadal, and Francesca Castaldo 
\\ Neuroelectrics Barcelona}

%\newcommand{\Comparator}{$\lceil \Delta \rceil$}
\usepackage{xspace}
\newcommand{\Comparator}{$\nu$\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
% \begin{abstract}

% The Free Energy Principle and related frameworks 
% require a hierarchical information neural encoding scheme, yet a satisfactory proposal has remained elusive.  At the same time, 
% the brain’s oscillatory rhythms appear to be arranged in a hierarchy of timescales.  
% We propose \emph{Hierarchical Amplitude Modulation} (HAM) as an encoding
% scheme in which faster oscillations are iteratively amplitude‑modulated by slower
% ones, forming a demodulable cascade. The multiplicative structure generates
% intermodulation components at \(f_c+\sum a_i f_i\) and favors \emph{geometric}
% (log‑spaced) band centers with frequency ratio $r\gtrsim 3$ to avoid spectral overlap, i.e., a constant‑\(Q\) arrangement
% akin to wavelet filterbanks, matching electrophysiological observations. A simple analysis shows that a geometric distribution of generators leads to power-law $1/f$ spectra, and how modulation cascades can produce
% \(1/f^\alpha\)‑like aperiodic sideband spectra with
% \(\alpha=2\ln(2/m)/\ln r\) under log spacing,  contributing mechanisms for aperiodic structure. As a proof of concept, we implement amplitude modulation and demodulation in a laminar
% neural mass model (LaNMM) combining PING‑like fast and JR‑like slow generators and derive the relation between band spacing, modulation depth, and aperiodic
% slope in a whole-brain model.
% \end{abstract}

 \begin{abstract}
Hierarchical encoding of information is a central requirement of the Free Energy Principle and related theories of brain function, yet a concrete mechanism implementing such encoding in neural dynamics has remained elusive. Concurrently, electrophysiological data reveal power laws and a hierarchical organization of oscillations with frequency ratios $r\sim$2--3.  
We analyze this from the perspective of \emph{Hierarchical Amplitude Modulation} (HAM), a biophysically grounded encoding scheme in which faster rhythms are multiplicatively modulated by slower ones, forming a demodulable cascade of nested envelopes. This multiplicative architecture naturally generates intermodulation components at frequencies \(f_c + \sum a_i f_i\) and favors geometric (log-spaced) band centers with ratio \(r \gtrsim 3\) to ensure spectral non-overlap—a constant-\(Q\) arrangement analogous to wavelet filterbanks. This conservative guard‑band picture is consistent with the empirical organization of canonical brain rhythms. Furthermore, 
 we show that (i) a log-uniform distribution of oscillatory generators leads to \(1/f\) power spectra, and (ii) modulation cascades produce \(1/f^{\alpha}\) aperiodic sideband spectra with \(\alpha = 2\ln(2/m)/\ln r\), linking the aperiodic slope directly to circuit parameters such as modulation depth \(m\) and spacing ratio \(r\). 
As a proof of concept, we implement HAM within a laminar neural mass model (LaNMM) combining PING-like fast and JR-like slow populations, demonstrating how amplitude modulation and demodulation can emerge from intrinsic nonlinearities and cross-frequency coupling. Whole-brain simulations  using LaNMM nodes with logarithmically spaced generators reproduce the empirically observed \(1/f^{\alpha}\) scaling and clarify how the interplay of spacing, modulation, and cross-frequency coupling shapes the broadband background of neural power spectra. 
\end{abstract}
% with a ratio $r\approx 3$. 

%%%%%%%%%%%%%%%
\clearpage
\tableofcontents

\clearpage
\textbf{\large Highlights}
 
 

\begin{itemize}
\item We introduce HAM: A hierarchical encoding scheme where faster rhythms are multiplicatively modulated by slower processes; information can live in signals or their envelopes (and envelopes of envelopes).
\item The intermodulation structure $f_c \pm \sum a_i f_i$ favors geometric (log) spacing with frequency ratio $r\gtrsim3$ (strong super-increasing hierarchy), yielding constant fractional bandwidths and enabling staged demodulation.
\item We present two routes to $1/f^\alpha$: (i) a cascade link $\alpha = 2\ln(2/m)/\ln r$; (ii) a log‑uniform mixture that yields $1/f$ in expectation with constant‑Q kernels.
\item We provide a proof of concept modulation and demodulation implementation in NMM using the LaNMM with PING‑like fast generators as carriers and JR‑like slow generators for envelope extraction. 
\end{itemize}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage



\section{Introduction}

Predictive coding has emerged as a central framework for understanding neural processing, positing that the brain continuously generates predictions about incoming sensory data and updates these predictions by minimizing prediction errors \cite{raoPredictiveCodingVisual1999,fristonPredictiveCodingFreeenergy2009}. In active inference (AIF), this hierarchical exchange of top-down predictions and bottom-up errors minimizes free energy \cite{fristonFreeEnergyPrinciple2006,parr2022active}. 
Kolmogorov Theory (KT) naturally embeds this framework within Algorithmic Information Theory, suggesting that both biological brains and digital agents optimize predictions by constructing short, efficient models of the world \cite{ruffiniInformationComplexityBrains2007, ruffiniRealitySimplicity2009, ruffiniModelsNetworksAlgorithmic2016,ruffiniAlgorithmicInformationTheory2017,ruffiniAlgorithmicAgentPerspective2024,ruffiniStructuredDynamicsAlgorithmic2025}. Although the analog computational brain appears distinct from digital algorithmic agents, both systems fundamentally rely on encoding information to compute prediction errors and update internal representations.

Neural coding schemes have long been debated, with proposals ranging from rate coding (where information is carried by average firing rates) to various temporal strategies—including spike timing, synchrony, phase coding, and burst coding—to capture the richness of neural information \cite{zeldenrustNeuralCodingBurstsCurrent2018, carianiTimeEssenceNeural2022, lismanThetaGammaNeuralCode2013}. In this context, it is increasingly clear that information is encoded not only in the raw oscillatory signals but also in their amplitude envelopes \cite{Ruffini2025Comparator}. This dual coding strategy is reminiscent of amplitude modulation (AM) in radio communications, where a high-frequency carrier encodes information via its modulated envelope \cite{nahinScienceRadio1996}.

Two empirical regularities stand out in neural field recordings: an aperiodic \(1/f^{\alpha}\) background and a near‑logarithmic spacing of canonical oscillation bands. Both patterns recur across species and recording modalities, and the aperiodic component can be cleanly separated from periodic peaks in modern parameterizations \cite{millerPowerLawScalingBrain2009,heScalefreeBrainActivity2014,donoghueParameterizingNeuralPower2020,penttonenNaturalLogarithmicRelationship2003,buzsaki_brain_2023,donoghueEvaluatingComparingMeasures2024}.

 
Oscillatory activity in the brain plays a key role in organizing and transmitting information. Gamma oscillations (30–100 Hz) have been implicated in local information processing, feature binding, and communication through coherence \cite{buzsakiMechanismsGammaOscillations2012, jensenHumanGammafrequencyOscillations2007, friesNeuronalGammabandSynchronization2009, singerVisualFeatureIntegration1995}, while slower rhythms (theta 4–8 Hz and alpha 8–12 Hz) are associated with attentional gating and top-down signaling \cite{buzsakiRhythmsBrain2006, klimeschEEGAlphaOscillations2007, hanslmayrRoleOscillationsTemporal2011}. Phase-amplitude coupling (PAC) provides a mechanistic bridge between these scales, with slower oscillatory phases modulating the amplitude of faster activity \cite{canoltyFunctionalRoleCrossfrequency2010, lismanThetaGammaNeuralCode2013, scheffer-teixeiraCrossfrequencyPhasephaseCoupling2016, giraudCorticalOscillationsSpeech2012}. Empirical studies using EEG, MEG, and ECoG have demonstrated that both the filtered signal and its amplitude envelope can carry stimulus-specific information \cite{dingNeuralCodingContinuous2012, osullivanAttentionalSelectionCocktail2015, mesgaraniSelectiveCorticalRepresentation2012, golumbicMechanismsUnderlyingSelective2013, viswanathanElectroencephalographicSignaturesNeural2019}. Such findings suggest that neural information processing is inherently multiscale, with nested oscillatory hierarchies that multiplex information much like modern communication systems.
 Gamma oscillations, in particular, serve as carriers of sensory information, while slower rhythms modulate these carriers via cross-frequency coupling. 
 
   Importantly, oscillatory bands—such as delta, theta, alpha, beta, and gamma—exhibit
a geometric progression with a common ratio \(r \approx 2\)--3  (equivalently, equal spacing on a
log-frequency axis; constant‑Q filterbank, see Figure~\ref{fig:logarithmic_hierarchy}). That is, the center frequencies increase by a constant multiplier, or equivalently, they are evenly spaced when plotted on a logarithmic scale \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiNeuronalOscillationsCortical2004}.  
%Penttonen \& Buzsáki (2003) and others \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiNeuronalOscillationsCortical2004} report that behaviorally relevant brain oscillations exhibit a structured relationship that facilitates coordinated activity across neuronal networks of varying sizes and connectivity. They found that the 
%Center frequencies and frequency ranges of oscillatory bands—from ultra-slow to ultra-fast—form an arithmetic progression when plotted on a natural logarithmic scale 
%Due to the mathematical properties of the logarithm, the corresponding cycle lengths (the inverses of frequency)  form an arithmetic progression after logarithmic transformation. 
%In practical terms, 
Slower oscillations, with longer periods, provide a broad temporal window and allow integration over larger spatial extents with more variable synaptic delays, whereas faster oscillations offer more precise, spatially limited representations of information. Such evidence reinforces the notion that brain oscillations are organized into a conserved, hierarchical set of timescales, with adjacent rhythm classes separated by roughly constant frequency ratios on the order of 2 to 3 \cite{penttonenNaturalLogarithmicRelationship2003, buzsakiNeuronalOscillationsCortical2004, penttonenNaturalLogarithmicRelationship2003,klimeschFrequencyArchitectureBrain2018a,
buzsakiScalingBrainSize2013,buzsaki_brain_2023}.

 
Motivated by these observations, we propose a \textit{hierarchical amplitude modulation} (HAM) framework for neural information encoding. In HAM, information is represented in a layered fashion: the raw oscillatory signal (the carrier), its amplitude envelope, and, recursively, the envelope of that envelope.  This nested modulation approach yields a coarse-grained, multiscale, natural representation of information. As each successive envelope captures fluctuations at a slower timescale, the system exhibits a logarithmic frequency spacing and power law of oscillatory dynamics—phenomena that align with the observed geometric progression of brain rhythms. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure} [t]
    \centering
    \includegraphics[width=1\linewidth]{figures/logarithmic_hierarchy.png}
    \label{fig:logarithmic_hierarchy}
    \caption{Logarithmic oscillatory hierarchy.
Oscillation bands in rat are approximately log‑spaced with an approximately common ratio \(r\)
so that \(f_{k+1}=f_k/r\) (empirically \(r\approx 2\)--3 between neighboring
bands)  \cite {penttonenNaturalLogarithmicRelationship2003}.  This approaches the theoretical limit of $r\gtrsim 3$ for non-overlapping sideband clusters and is equivalent to equal spacing on a log‑frequency axis (corresponding to a constant‑Q arrangement).}
\end{figure}
% https://colab.research.google.com/drive/1XSmrR51GdENpHDsh4ujWh7NFp4po9NwC#scrollTo=-d9JBVYRYILM
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 

In the following sections, we present the mathematical formulation of the  model and demonstrate how it can account for the observed logarithmic spacing and power-law distribution of oscillatory bands.

  Finally, we provide an implementation of the framework's amplitude modulation building block in the Laminar Neural Mass Model (LaNMM) \cite{ruffiniP118BiophysicallyRealistic2020,  sanchez-todoPhysicalNeuralMass2023,sanchez-todoFastInterneuronDysfunction2025a,  Ruffini2025Comparator}, which simulates depth-resolved electrophysiology by capturing the interaction between superficial fast and deeper slow oscillations, including mechanisms such as phase-amplitude coupling and amplitude-amplitude anticorrelation.  LaNMM provides a candidate mechanism for the neural function for error evaluation—central to predictive coding—by comparing modulated signals across cortical layers \cite{Ruffini2025Comparator}. Here we show how it also provides a practical platform to instantiate modulation and demodulation.
 



\section{Hierarchical amplitude modulation (HAM)}
\subsection{From single-layer AM to hierarchical modulation} In standard amplitude modulation (AM) (as used in radio), information is encoded in the \emph{amplitude} of a high-frequency carrier wave. For example, an audio signal $m(t)$ (low-frequency information) can modulate a higher-frequency carrier $C(t) = A_c \cos(2\pi f_c t)$ by varying the carrier’s amplitude in proportion to $m(t)$. The result is a modulated signal 
\begin{equation}
S(t) \;=\; A_c\,[1 + a\,m(t)]\,\cos(2\pi f_c t)\,,
\label{eq:AM_single}
\end{equation}
where $a$ is the modulation index (chosen so that $1+a\,m(t)>0$ to avoid distortion). Demodulation of $S(t)$ recovers the original message $m(t)$ by detecting the envelope. In neural terms, one can think of fast oscillations (e.g., gamma rhythms) acting as carriers whose amplitudes are controlled by slower processes (e.g., theta or delta rhythms).


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/AM_signals.png}
 
   \caption{\textbf{Amplitude modulation (AM)}, where information is encoded in the envelope of a carrier, as in AM radio.   }
    \label{fig:AM}
\end{figure} 


While classical AM involves a single modulating signal (Fig.~\ref{fig:AM}), the brain exhibits \emph{nested} oscillations: faster rhythms modulated by slower rhythms, which in turn are modulated by even slower fluctuations. \textit{Hierarchical Amplitude Modulation} (HAM) generalizes the AM concept to multiple layers of amplitude modulation across different frequency bands. In a HAM cascade, a high-frequency oscillation’s amplitude is modulated by a slower oscillation; this slower oscillation’s amplitude can itself be modulated by an even slower process; and so on, creating a multilevel envelope hierarchy. In effect, information can be carried not only by the fast “carrier” signal, but also by its envelope, the envelope-of-envelope, etc., forming a multiscale encoding of information. Not all frequencies need to be involved in a cascade, but all such combinations should be possible.

\noindent \textbf{Nested modulation across oscillatory bands.} Consider a chain of $N$ nested modulators with descending center frequencies $f_0 > f_1 > \cdots > f_N$. For concreteness, let $f_0$ be a high-frequency carrier and $f_1, f_2, \dots$ be progressively slower oscillatory components. A depth-$N$ HAM signal can be written as a product of an $N$-layer envelope and the carrier,
\begin{equation}
s_N(t) \;=\; A_0 \prod_{i=0}^{N}\Big[\,1 + m_i\cos(2\pi f_i t + \phi_i)\Big]~,
\label{eq:HAM_product}
\end{equation}
with $0<m_i<1$, the modulation depths at each layer. Here, $i=0$ corresponds to the fastest oscillation (the putative “carrier”), and each $i>0$ term represents an oscillatory envelope imposed on the previous layers. All the terms in the product are positive to reflect the nature of firing rates, but this is not a crucial element. Expanding the product in Eq.~(\ref{eq:HAM_product})  and using the trigonometric identity $\cos A\,\cos B = \tfrac{1}{2}[\cos(A+B)+\cos(A-B)]$ iteratively yields a superposition of cosines at various frequencies, revealing a rich \emph{intermodulation spectrum}: new frequency components (sidebands) appear at linear combinations of the base frequencies. In fact, the spectrum of $s_N(t)$ contains sinusoidal components at frequencies 
\begin{equation}
f \;\in\; \Big\{\;\sum_{i=0}^{N} a_i\,f_i:\; a_i\in\{-1,0,1\}\;\Big\}\!,
\label{eq:sideband_set}
\end{equation}
i.e. at the carrier frequency $\pm f_0$, the modulation frequencies $\pm f_1,\pm f_2,\dots$, and all combinations such as $f_0\pm f_1$, $f_0\pm f_2$, $f_0\pm f_1 \pm f_2$, etc. Each modulation layer thus spreads the signal’s spectral energy into new sideband peaks spaced around the original frequencies. Importantly, the strength of higher-order sidebands (involving multiple $m_i$ factors) is suppressed for small $m_i$; thus, the cascade remains dominated by its fundamental bands and nearest sidebands if $m_i \ll 1$.

This multiplicative, multiband structure is illustrated in Figure~\ref{fig:ham_example}. A high-frequency carrier (Fig.~\ref{fig:ham_example}, top) is successively modulated by a slower oscillation, then by an even slower one, and so on, producing a final complex signal with amplitude fluctuations at multiple timescales (middle). The corresponding frequency spectrum (Fig.~\ref{fig:ham_example}, bottom) displays the original carrier line and a comb of sidebands generated by each modulation layer. Notably, the spectral power decays at lower frequencies, reflecting how each envelope layer contributes a factor $m_i<1$ to the amplitude (as discussed below). The nested HAM signal can be demodulated in stages to recover the slower envelope signals from the faster carrier, analogous to how a radio receiver detects a single AM envelope – but here performed iteratively across scales.


 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/ham_signals.png}
    \includegraphics[width=0.8\linewidth]{figures/ham_spectrum.png}
    \caption{\textbf{HAM example.} A high-frequency carrier is successively modulated by slower oscillations in a nested multiplicative structure. Each subplot presents a key stage in the formation of the hierarchical signal:
	(A)	Envelope Modulations (First three subplots from top): Each line represents an envelope function of the form  $1 + m_i \cos(2\pi f_i t + \phi_i)$, where each modulation frequency  $f_i$  is progressively lower. These envelopes modulate all previously applied layers, creating an intricate structure of amplitude variations at multiple timescales.
	(B)	Demeaned, final Hierarchical AM Signal (fourth plot): The resulting signal after applying all modulation layers. This waveform is the product of all envelope layers modulating a carrier, generating a complex multi-scale amplitude variation. The signal is filtered around the highest carrier frequency (125 Hz). This highlights how the original high-frequency carrier retains structured amplitude variations due to the hierarchical modulation. (C) Spectrum of the full signal. 
The hierarchical nature of modulation creates a combinatorial spectral expansion, where all integer combinations of modulating frequencies contribute to the final spectrum. This structure requires logarithmic spacing of modulation frequencies to prevent spectral overlap and ensure clear separation of modulation layers. The figure also shows a logarithmic decay in sideband power, which stems from the successive product of $m_i$ factors in each band.}
    \label{fig:ham_example}
\end{figure}


\subsection{Avoiding spectral overlap. Logarithmic band spacing and guard bands}
A critical challenge in any multi-layer modulation is avoiding spectral \textit{overlap} between components. If the newly generated sideband frequencies crowd into neighboring frequency bands, information at different layers would interfere and demodulation will not be possible. In engineered communication systems, this is solved by allocating \emph{guard bands} and using a \emph{constant-Q} spacing of carriers – meaning each carrier’s bandwidth is a fixed fraction of its center frequency (a logarithmic progression of center frequencies). Strikingly, brain oscillations also appear roughly log-spaced in frequency (each band about a constant ratio higher than the next) \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiNeuronalOscillationsCortical2004}. HAM provides a mechanistic reason for this: \emph{geometric spacing} of frequencies naturally prevents intermodulation overlap.

Mathematically, the necessary and sufficient condition to keep all sideband clusters disjoint (and hence ensure sequential demodulation) is to require that each higher-frequency $f_k$ exceeds twice the sum of all lower frequencies (see Appendix~\ref{sec:hierarchical_am}). In other words, if 
\begin{equation}
f_k > 2\sum_{j>k} f_j \quad \mbox{ for every } k, 
\end{equation}
then no combination of slower oscillators can produce a difference-frequency that reaches the $f_k$ cluster. 
%This strict \textit{super-increasing} hierarchy guarantees unique spectral placements but is stronger than necessary. 
A  practical design meeting the necessary condition is to choose a common spacing ratio $r$ such that 
\begin{equation}
f_{k+1} \approx \frac{f_k}{r}\, , \quad \text{ with } r\gtrsim 3,
\label{eq:geometric_spacing}
\end{equation}
yielding an approximately constant factor separation between successive bands. Geometric spacing (log-uniform in frequency) ensures that each modulation’s sidebands fit into the “gaps” between adjacent bands without overlap if $r$ is large enough. %: $r>2$ suffices for non-overlap, $r>3$ for strict uniqueness. 
Then, the bands behave like well-separated narrow channels. In practice, neural oscillations exhibit $r\approx 2$–3 (e.g., delta~$\sim$2–4 Hz, theta~$\sim$6–8 Hz, alpha~$\sim$10 Hz, etc.), which provides adequate separation for finite-depth or weak modulation. This log-spaced, constant-Q arrangement mirrors classical wavelet filter banks and the cochlear frequency, and it matches the empirical observation of near-logarithmic spacing in canonical brain rhythm bands.

Crucially, the log-spaced hierarchy means each oscillatory “band” retains its identity (its own frequency range) even as it participates in a larger modulation cascade. By keeping modulation depths $m_i$ moderate ($m_i<1$), each layer’s sidebands remain as small perturbations around the existing lines. This avoids excessive distortion and allows the original carrier and envelope signals to be separated by bandpass filtering or sequential demodulation. In summary, HAM’s multiplicative encoding across scales naturally favors a logarithmic frequency arrangement: only with such spacing can a deep hierarchy of oscillations coexist without mutual spectral interference.



\subsection{Power-law spectral scaling}



Empirically, many electrophysiological signals exhibit an aperiodic $1/f$-like background in their power spectra. The HAM mechanism offers two complementary explanations for this ubiquitous observation. First, a single deep cascade of nested oscillations can generate a $1/f^{\alpha}$ spectrum with $\alpha$ tuned by neuronal parameters (e.g., modulation strengths and band spacing). Second, even if individual oscillatory pairs are only weakly coupled, a whole population of log-spaced oscillators will \emph{collectively} produce a $1/f$-type spectrum in the aggregate. In the latter case, the intuition is that having equal power per octave (a flat distribution on a log-frequency axis) naturally yields a $P(f)\propto 1/f$ spectrum. In other words, if oscillatory power is spread roughly evenly across logarithmic frequency bins (as constant-Q band partitioning does), the summed spectrum follows a pink-noise $1/f$ profile (this is a well-known property in audio and wavelet analysis). The HAM framework encompasses both routes: the “cascade route” linking $\alpha$ to specific circuit parameters (Eq.~(\ref{eq:powerlaw})), and the “ensemble route” where many oscillators with log-uniform frequency spacing produce $\alpha\approx1$ in expectation. Both mechanisms may operate in neural systems, contributing to the observed aperiodic power law scaling in brain activity. 

\subsubsection*{Global Spectrum} 
 An immediate result from the geometric scaling architecture is power law spectral scaling. As shown in Appendix~\ref{app:powerlaw},   a bank of log‑spaced oscillators with equal power per band produces \(S(f)\approx 1/f\).
Morevoer, random AM pairings add a constant pedestal; stronger modulation (\(m\)) or more pairs
increase \(\kappa_0\) and flatten the fitted \(\alpha\) over finite windows. Finally, using constant‑\(Q\)
kernels (or filters) should push the fitted slope in Fig.~\ref{fig:logarithmic_hierarchy} toward \(\alpha\approx 1\) and
increase the fitting range.
The assumption \(\rho(f)\propto 1/f\) parallels empirical \emph{logarithmic spacing} of neural bands \cite{penttonenNaturalLogarithmicRelationship2003} and is mathematically equivalent to a constant‑\(Q\)
tiling (equal spacing on a log‑frequency axis) \cite{brownCalculationConstantSpectral1991}. 





\begin{figure} [t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/simulated_spectrum_PBuz.png}
    \caption{\textbf{Log--log spectra for log-spaced oscillators with AM sidebands.}
We display the HAM  spectrum using the Penttonen--Buzsáki band centers and equal amplitude carriers, which leads to $1/f$ behavior. \label{fig:simulationfullspectrum}
% https://colab.research.google.com/drive/1Pi7JAZTRPXa7-UuQ-hx75XIW4hYB8_qr#scrollTo=xm2iPf5RuBaB
The solid curve shows the per‑Hz power spectral density (PSD) obtained by summing
constant–$Q$ Gaussian lines for carriers and their first‑order AM sidebands
($f_c\pm f_m$, $f_m<f_c$).   Each base center is replicated with multiplicative jitter
$s\!\sim\!\mathcal{U}[1/2,2]$ to populate each cluster, carrier amplitudes are $A_c\equiv 1$,
and the sideband power is $\big(\tfrac{mA_c}{2}\big)^2$ with $m=0.9$.
Dotted vertical lines mark the Penttonen–Buzsáki center frequencies; faint spans indicate
their reported ranges. The dashed line is a $C/f$ reference. Fitted slopes
over 5–250\,Hz are indicated in the legend. Because generator density per Hz declines roughly as $1/f$
under log spacing, the aggregate PSD decays with frequency; AM sidebands add a low‑$f$
pedestal but preserve the $1/f$ backbone in the interior.
}

\end{figure}
% 
 

\paragraph{Synthetic spectra for log-spaced oscillators with AM sidebands.}

We tested the prediction that a log-spaced hierarchy of oscillatory generators produces a
per-Hz power density that decays with frequency on a linear axis (reflecting the decreasing
generator density per Hz), and that the aggregate spectrum approaches a power law on a
log--log axis (see Figure~\ref{fig:simulationfullspectrum}).
We considered a frequency organizations using the set of
Penttonen--Buzsáki band centers. To fill gaps on a linear frequency axis, each center
$f_i$ was replicated $K$ times with multiplicative jitter $u\sim\mathrm{Uniform}[1,2]$,
producing micro-generators at $u f_i$ (truncated to $[f_{\min},f_{\max}]$).
Each micro-generator contributes a narrowband line shaped by a constant-$Q$ Gaussian
kernel (fractional width $\sigma/f=q$) so that bandwidth scales with center frequency.
We set all carrier amplitudes to $a_i=1$ (equal power). To emulate amplitude modulation, we drew $M$ random pairs $(f_m,f_c)$ from the
micro-generators with $f_m<f_c$ and added first-order sidebands at $f_c\pm f_m$, each
weighted by $(m/2)^2$ in power, where $m\in(0,1)$ is the modulation depth.
Per-Hz power spectra were computed on a dense linear frequency grid and shown on a log--log
axis. A $1/f$ reference ($k/f$) was overlaid by matching the median of $S(f)\,f$ in an
interior band. The aperiodic slope $\alpha$ was estimated by ordinary least squares on
$\log_{10}S(f)$ vs.\ $\log_{10}f$ over a predefined range (5--250\,Hz). The design
tests the claims that (a) constant-$Q$ log spacing implies decreasing spectral power density with
frequency, and (b) the aggregate spectrum approximates $1/f^{\alpha}$, with sidebands
enhancing low-frequency accumulation while preserving the backbone.




\subsubsection*{Sideband Spectrum} 
A further consequence of the HAM cascade is the emergence of \textit{power-law} scaling in the sideband spectrum. Because each modulation layer redistributes a fraction of the signal’s power to lower frequencies, the overall spectral density tends to decline with frequency in a scale-free (fractal-like) manner. In fact, under log-spacing, a simple analysis shows that the power spectral density of a hierarchical cascade approximately follows $P(f)\sim 1/f^{\alpha}$ – a $1/f$-type law – with the exponent $\alpha$ depending on the modulation depth ($m$) and spacing ratio ($r$).


\begin{figure} [t!]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/power_law.png}
\caption{\textbf{Power‑law exponent \(\alpha\) for log‑spaced HAM sideband.}
For \(r>1\), the toy‑model exponent is
\(\alpha=2\ln(2/m)/\ln r\). Larger \(m\) (stronger modulation) and
larger \(r\) (wider spacing) both \emph{reduce} \(\alpha\) (shallower slope);
smaller \(m\) steepens the slope.}

    \label {fig:power_law}
\end{figure}


For intuition, consider an idealized case where every layer uses the same depth $m$ and ratio $r$ for clarity. The first modulation produces sidebands carrying a fraction $\sim (m/2)^2$ of the carrier power (since each first-order sideband has amplitude $\approx m/2$ of the carrier). The next modulation layer acts on those sidebands, generating second-order components of order $(m/2)^2$ times the first layer’s amplitude, i.e. $(m/2)^4$ relative to the original carrier’s amplitude, and so on. After $k$ layers, the characteristic frequency has decreased to $f_k \approx f_0/r^k$, and the power at that scale is suppressed by roughly $(m/2)^{2k}$ relative to the top. Quantitatively, one can show (see Appendix~\ref{app:powerlaw})
\begin{equation}
P(f_k) \;\propto\; \Big(\frac{f_k}{f_0}\Big)^{2\ln(m/2)/\ln(1/r)} \;=\; \frac{1}{f_k^{\,\alpha}}~,
\label{eq:powerlaw}
\end{equation}
where 
\[
\alpha \;=\; \frac{2\,\ln(2/m)}{\ln r}\,. 
\] 
This indicates a $1/f^{\alpha}$ scaling of the cascade’s spectrum. Notably, $\alpha$ increases as $m$ decreases (weaker modulation yields a steeper spectral drop-off) and $\alpha$ decreases as $r$ increases (wider band gaps flatten the spectrum). Figure~\ref {fig:power_law} illustrates this relationship: stronger modulation (larger $m$) or broader spacing (larger $r$) results in a shallower slope (smaller $\alpha$), whereas small $m$ leads to a pronounced $1/f^{\alpha}$ fall-off. 

Thus, the emergence of $1/f^\alpha$ spectral structure in hierarchical AM systems is explained by two minimal structural constraints: (1) a log-spaced distribution of oscillators and (2) a modulation rule enforcing \( f_m < f_c \). Sidebands are not necessary for scale-free behavior but do enhance low-frequency accumulation and steepen the slope.







\section{Modulation and demodulation in LaNMM}

We previously introduced a laminar neural mass model (LaNMM) designed to capture both superficial-layer fast and deeper-layer slow oscillations along with their interactions, such as phase-amplitude coupling (PAC) and amplitude-amplitude anticorrelation (AAC)\cite{sanchez-todoPhysicalNeuralMass2023}. This spectrolaminar motif is ubiquitous across the primate cortex \cite{mendoza-hallidayUbiquitousSpectrolaminarMotif2024}.  

To implement and process information in the amplitude modulation scheme, modulation and demodulation must be performed. Here, we describe how this can be achieved thanks to the nonlinearity in the model --- the sigmoid (see Figures~\ref{fig:LaNMM_Mod_Demod} and~\ref{fig:modulation}).




\textit{Signal-Envelope Coupling (SEC)}—related though not equivalent to phase-amplitude coupling (PAC) \cite{spaakLayerspecificEntrainmentGammaband2012, szczepanskiDynamicChangesPhaseamplitude2014, dvorakProperEstimationPhaseamplitude2014, mejiasFeedforwardFeedbackFrequencydependent2016, chackoDistinctPhaseamplitudeCouplings2018, bastos_laminar_2018} refers to the idea that a slow signal can couple to the envelope of a faster signal. In predictive coding, for example \cite{Ruffini2025Comparator}, a slower rhythm (encoding predictions) modulates the envelope (i.e., overall magnitude) of faster oscillations (encoding sensory data), aligning the processing of fast inputs with the slower predictive signal.  

\textit{Envelope-Envelope Coupling (EEC)}—also known as Amplitude-Amplitude Coupling (AAC) or power coupling—acts as a slower gating mechanism by modulating the envelope of fast oscillations according to the envelope of a slower rhythm. This creates periods of increased or decreased excitability that selectively allow or restrict the propagation of error signals, thus aligning predictive coding circuits with high-level task demands (e.g., attention and vigilance).





\subsection{Single node and whole-brain implementation using the LaNMM}
\label{sec:simplified_hierarchical_am}
We first examined a single model column to see if it can encode and decode a message via HAM. An external slow oscillatory input (mimicking a “prior” or top-down signal) was injected into the deep layer while the superficial layer was driven to produce a sustained fast oscillation. The result was that the slow input modulated the amplitude and instantaneous frequency of the fast oscillation in the superficial layer, just as a radio signal would modulate a carrier. The superficial population’s output now had an envelope that clearly followed the slow input’s waveform. We then asked: can the model recover the slow modulating signal from the compound output (i.e. perform demodulation)? Remarkably, the deep layer’s own activity provided this demodulation. Because the deep population received feedback from the superficial population and has intrinsic slow dynamics, it effectively tracked the envelope of the fast oscillation – the slow layer’s activity came to oscillate in phase with the envelope of the gamma layer, thus reconstructing the original slow signal (with some phase delay). In other words, the laminar circuit itself acted as both modulator and demodulator: the deep layer imposed a modulation and also (through the nonlinear feedback loop) extracted the resultant envelope. This behavior was visualized in the model by comparing the injected slow waveform with the deep layer’s output – they matched closely when the coupling was in place (illustrated in Fig.~\ref{fig:modulation}).
A simplified way to model a nested   amplitude modulation scheme with firing rates in a whole-brain model using LaNMM is to let each new signal 
$
   s_{j}(t)
$
be formed by taking the previous envelope \(s_{j-1}(t)\) and embedding it as the modulation of a higher-frequency carrier \(\cos(2\pi\,f_{j}\,t + \phi_{j})\). Including a positive offset ensures the overall signal remains non-negative (reflecting firing rates or baseline activity). Symbolically,
\[
   \boxed{
   s_{j}(t) 
   \;=\; 
   A_{j}
   \,\Bigl[
   1 
   \;+\;
   m_{j}\,s_{j-1}(t)\;\cos\bigl(2\pi\,f_{j}\,t + \phi_{j}\bigr)
   \Bigr],
   }
\]
where \(A_{j} > 0\) sets the baseline (DC offset),
  \(m_{j}\) is the modulation index for layer \(j\),
   \(f_{j}\) is the new (faster) carrier frequency at layer \(j\), and 
 \(\phi_{j}\) is a phase term.





\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/mod_demod.png}
    \caption{\textbf{LaNMM architecture for modulation and demodulation.}
    a) The prior modulates the amplitude/frequency of a faster carrier input;
    b) The input’s envelope is extracted by the slow dynamics of P1, illustrating demodulation.}
    \label{fig:LaNMM_Mod_Demod}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/modulation.png}
    \caption{\textbf{Modulation schemes.} a) Modulation of a carrier by a message through multiplication. b) Implementation in laNMM through top-down inputs from a parcel Y upstream to slow population or inhibitory neurons in the fast circuit. A steady oscillation (carrier) is received from the downstream parcel W. The modulated carrier output is sent upstream to Parcel Y. In Model 1, top-down excitatory inputs arrive at the pyramidal cell in L5. In Model 2, they arrive at the PV+ interneuron in L1, with inhibitory consequences for the PING pyramidal population in L1.}
    \label{fig:modulation}
\end{figure}




Thus each stage builds on the previous envelope:
$
   s_{j-1}(t) 
   \longrightarrow 
   s_{j}(t),
$
with the faster carrier \(\cos(2\pi f_{j} t + \phi_{j})\) added into the mix. In the time domain, \(s_{j-1}\) no longer appears as a separate low-frequency component but instead shapes the amplitude of \(\cos(2\pi f_{j} t)\). Over multiple layers \(j = 1,2,\dots\), this yields a nested hierarchy of amplitude-modulated signals, each centered on a higher-frequency band.


 
\subsection{Whole-brain model implementation}
We extended the single-column model to a whole-brain network by replicating the laminar node across multiple brain regions (parcels) and connecting them according to a simplified anatomical connectivity (e.g., a lattice or an empirical structural connectivity matrix). In this network, each region contains a slow and fast oscillator (deep and superficial populations), and inter-regional coupling can occur at different levels – for instance, a deep region’s oscillation might modulate the input of a distant superficial region if there are long-range projections targeting inhibitory interneurons or dendritic currents. We configured a scenario in which regions have intrinsic oscillations at log-spaced frequencies (to mimic, say, an anterior region oscillating at ~10 Hz and a posterior region at ~40 Hz, etc., roughly logarithmic spacing across a factor of a few). We then simulated the network’s activity and examined the power spectral density (PSD) of the summed signals. Consistent with our theoretical predictions, we observed that a network with logarithmically spaced native frequencies and inter-layer modulation couplings produces a clear $1/f$-type PSD at the macroscale. In contrast, if the same network’s oscillators were not log-spaced (e.g., uniformly spaced in frequency or all the same frequency), the PSD was much flatter or showed no consistent scaling (essentially white or only peaked at a single frequency). Figure 5 of the paper summarizes these findings: a uniform distribution of oscillators yielded ~$\alpha \approx 0.08$ (nearly flat), whereas a log-spaced set of uncoupled oscillators gave a power-law with $\alpha \approx 0.84$, and adding modulation connections between them produced $\alpha \approx 0.95$ (closer to 1/f). The slight change in slope with coupling (0.84 → 0.95) matches the expected effect of sideband pedestal discussed earlier (coupling can flatten the slope a bit by boosting low-frequency content) – in our case the effect was small and actually pushed $\alpha$ toward 1.0. Overall, the whole-brain simulation supports the notion that two ingredients – a log-frequency arrangement of oscillatory sources, and modulation (multiplicative coupling) among them – are sufficient to produce a $1/f^\alpha$ spectrum. These ingredients map onto physiological realities: the brain does have oscillators roughly log-spaced across scales, and cross-frequency coupling (both within and across regions) is abundantly observed. Our model provides a feasible mechanism by which these features give rise to the broadband background seen in EEG/MEG power spectra.

From the model, we can also inspect the fine-grained spectral features to validate the presence of intermodulation frequencies. Indeed, the regional PSDs showed not only the main peaks corresponding to each region’s fundamental rhythm, but also smaller peaks at sums and differences of those frequencies when regions interacted. For example, if one region oscillated at 10 Hz and another at 40 Hz and they were coupled, we could detect components around 40±10 = 30 Hz and 50 Hz in the spectrum (provided our frequency resolution was fine enough). Such intermodulation frequencies have been reported in experiments where two stimuli are tagged at different frequencies, and their detection in brain signals is considered evidence of nonlinear mixing or cross-frequency coupling. In our case, their presence indicates that the model’s coupling is genuinely multiplicative. The patterns of these combination frequencies could in principle be used as a spectral “fingerprint” of HAM in data – for instance, the presence of sums/differences like $f_\text{alpha}\pm f_\text{gamma}$ in a power spectrum might suggest an alpha-modulating-gamma mechanism at work. However, resolving these small peaks requires high signal-to-noise and spectral resolution, so in practice one often relies on indirect measures like PAC metrics or coherence between envelopes and phases.
In a whole-brain model of LaNMM nodes, the frequencies of each node are slightly altered, because the oscillatory behavior of the circuit is input-dependent. This heterogeneity leads to a spectral distribution with smoothed peaks around the model baseline frequencies of 10 and 40 Hz. 

When we \emph{add} all the generated signals to form a total output,
\[
  S_{\mathrm{total}}(t)
  \;=\;
  \sum_{j=1}^{J}\,s_{j}(t),
\]
the resulting spectrum contains contributions from \emph{every} layer’s carrier and sidebands, plus any offsets (DC terms). In essence, each new layer $j$ multiplies the previous layer’s signal $s_{j-1}(t)$ by $\cos(2\pi\,f_{j}t + \phi_{j})$, creating \emph{products} in the frequency domain. 

 
If the $\{f_{j}\}$ are chosen far apart (for instance, in a descending or geometric progression), most sidebands cluster around well-separated centers $\pm f_{j}$.  Under that condition, each cluster is distinct and the overall spectrum appears as a \emph{comb} of frequencies: $f_{1},\,f_{2},\dots$ plus linear combinations.  Conversely, if two carriers $f_{i}$ and $f_{j}$ are commensurate (integer multiples), certain intermodulation products coincide, merging sidebands and creating stronger or overlapping peaks.  

\paragraph{Physiological Interpretation.}
In a laminar neural model, one might measure all these signals at once (e.g.\ in an intracortical probe or scalp EEG).  The raw summed signal exhibits components at multiple frequencies, some of which reflect \emph{cross-frequency} couplings (i.e.\ sidebands).  If each layer’s output $s_{j}(t)$ is viewed in isolation, then the nesting is clearer: a layer-$j$ “carrier” is amplitude-modulated by lower layers’ envelopes.  Summing yields a rich spectral profile spanning multiple scales.

In short, the total spectrum in a whole brain model is the superposition of all layer-specific carriers and their modulation sidebands.  The structure can be kept relatively clean and non-overlapping if each frequency $f_{j}$ is sufficiently distinct and the modulation indices $m_{j}$ are small.  Otherwise, strong intermodulations can blur the spectral hierarchy.


 \begin{figure}
     \centering
     \includegraphics[width=0.6\linewidth]{figures/WBspectrum.png}
     \caption{Whole brain spectrum (LaNMM model, average over all parcels).}
     \label{fig:WBspectrum}
 \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 


% \section{Discussion}


% We have presented the Hierarchical Amplitude Modulation framework as a unifying explanation for several empirical observations: the approximate geometric spacing of brain rhythm frequencies, the emergence of $1/f$ aperiodic spectral scaling, and the prevalence of cross-frequency coupling (PAC/AAC) in neural time series. HAM provides a rigorous yet intuitive model where faster oscillations carry signals that are structured by slower processes. This resonates with multi-layer communication engineering, but here realized in neural circuits. Our mathematical analysis established how a multiplicative cascade yields a power-law spectrum and why a logarithmic arrangement of frequencies is favored to maintain signal fidelity. 
% This offers a concrete mechanism for how slower rhythms (alpha/beta/theta) can gain‑modulate faster carriers (beta/gamma) and be read out downstream, consistent with the spectral dissociation of feedforward and feedback signalling in laminar circuits, where gamma preferentially supports feedforward content and alpha/beta supports feedback predictions  \cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}. In our LaNMM instantiation, PING‑like fast generators and JR‑like slow generators map naturally onto “carrier” and “envelope” roles, with demodulation implemented by the analytic‑signal formalism \cite{boashashEstimatingInterpretingInstantaneous1992}.
% The laminar neural mass model  shows that these principles can be instantiated in a biologically plausible network, reproducing both spectral signatures and coupling phenomena akin to real brain data.



 

% A first issue is why neural bands tend to be arranged on an approximately logarithmic axis. We framed this as an engineering constraint: intermodulation creates sideband clusters around each center frequency; to avoid collisions across the hierarchy one needs the span of slower combinations to fit within the guard band of each faster cluster. The super‑increasing condition \(f_k>\sum_{j>k} f_j\) renders clusters disjoint, and geometric spacing \(f_{k+1}=f_k/r\) satisfies this with margin for \(r\gtrsim 2\). Geometric spacing also yields constant fractional bandwidths (constant‑\(Q\)), which makes demodulation scale‑invariant and mirrors classical constant‑\(Q\) filterbanks and cochlear tiling \cite{brownCalculationConstantSpectral1991}. Although geometric spacing is a convenient sufficient condition, the weaker non‑overlap inequality already pushes the ensemble toward a log‑uniform density of centers, so the main phenomenology does not rely on exact ratios.

% A second issue is the aperiodic background. Our analysis highlights two complementary routes to \(1/f^\alpha\). The first is a \emph{cascade} mechanism intrinsic to HAM: with log spacing and small, roughly uniform modulation depth \(m\), components involving \(k\) modulators scale like \((m/2)^k\), and because \(k\sim \log_r(f_1/f)\), the spectrum follows \(P(f)\propto f^{-\alpha}\) with \(\alpha = 2\ln(2/m)/\ln r\). This provides a mechanistic link between slope and circuit variables, predicting how changes in modulation depth or spacing should shift \(\alpha\). The second is a \emph{mixture} mechanism: if oscillator centers are approximately log‑uniform and each contributes narrowband power with constant‑\(Q\) width, then summing across the bank already yields a \(1/f\) backbone in expectation (equal power per octave). This accords with parameterizations that separate aperiodic from periodic structure \cite{donoghueParameterizingNeuralPower2020,heScalefreeBrainActivity2014}. Random AM pairings on top of a log‑uniform pool add sidebands that preserve the \(1/f\) backbone in the interior of the band, with predictable edge deviations: a finite low‑frequency pedestal from small differences and a high‑frequency roll‑off from finite support. Which deviation dominates—and therefore whether fitted \(\hat\alpha\) flattens or steepens over a finite window—depends on analysis choices (band edges, constant‑Hz versus constant‑\(Q\) smoothing). These considerations reconcile the intuition that lower sidebands “enrich” low frequencies with the analytic expectation that, away from edges, both lower and upper sidebands contribute \(\sim 1/f\) per absolute~Hz.

% Physiologically, the sum‑then‑modulate rule at convergent nodes is natural: presynaptic drives add, then the resulting envelope modulates a faster carrier. In the small‑signal regime, convergent modulators combine linearly at first order (yielding standard \(f_u\pm f_v\) sidebands), while higher‑order intermodulation among distinct parents appears only when the envelope nonlinearity is engaged. This aligns with frequency‑tagging observations in which sum/difference arithmetic exposes nonlinear mixing and demodulation in visual and auditory cortex \cite{norciaSteadystateVisualEvoked2015,chenIntermodulationFrequencyComponents2024}. Within laminar circuits, the framework predicts a dissociation between superficial carrier‑dominant activity and deeper envelope‑dominant readouts, in line with reported frequency‑specific directionality \cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}. 


% Methodologically, constant‑\(Q\) preprocessing and explicit separation of aperiodic and periodic components are essential to obtain unbiased slope estimates and to avoid spurious cross‑frequency coupling \cite{donoghueParameterizingNeuralPower2020}.

% HAM therefore offers a constructive route to fractal‑like spectra without invoking a phase transition, yet it is compatible with signatures of near‑critical dynamics observed elsewhere (e.g., avalanche statistics and scale‑invariant correlations). Disentangling cascade‑based accounts from alternative explanations based on mixtures of time scales, synaptic filtering, or excitation–inhibition balance will require perturbational tests and careful finite‑size controls, not spectral fits alone \cite{milotti1NoisePedagogical2002,bedardMacroscopicModelsLocal2009}. The framework nonetheless yields clear empirical leverage: guard‑band (approximately geometric) spacing should minimize cluster collisions and facilitate staged demodulation; frequency‑tagged designs should exhibit intermodulation arithmetic with amplitudes scaling with products of modulation depths; laminar recordings should dissociate carriers from envelopes in the expected directions; and iterated envelope extraction should reveal the predicted log‑spaced content with decreasing \(Q\). These predictions set up the next steps—combining preregistered laminar/ECoG/MEG tests with LaNMM‑based parameter inference to link controlled changes in modulation depth and spacing to shifts in the aperiodic slope—while using constant‑\(Q\) analyses to stabilize estimates across decades. In this way, HAM connects engineering constraints (guard bands and constant‑\(Q\) designs) with hierarchical neural computation, and provides a practical path to “spectral engineering” of both stimulation and analysis protocols that we develop further in the Conclusion.

% %\textbf{Experimental implications:} If the HAM hypothesis is correct, we expect that manipulating one oscillatory component will affect others in predictable ways. For example, pharmacological or stimulus interventions that reduce the power of a particular rhythm (say, blocking alpha oscillations) might also reduce the amplitude fluctuations of higher-frequency activity that were previously modulated by that rhythm, thereby steepening the overall PSD (since less modulation = less low-frequency power). Conversely, inducing a strong low-frequency rhythm (e.g. via stimulation or task entrainment) might increase the envelope variance of high-frequency activity and flatten the PSD. These could be tested with closed-loop stimulation: drive a slow oscillation and see if high-frequency activity exhibits entrained amplitude modulation (a sign of artificial HAM). Additionally, if HAM underlies predictive coding, one might find that during tasks involving heavy top-down predictions, the PAC between deep and superficial layers (or between frontal theta and occipital gamma, etc.) intensifies, and the spectral slope might shift (possibly flatten) as more power is shunted into broad-band envelope fluctuations. In more cognitive terms, attention and arousal states which modulate oscillatory amplitudes could be reinterpreted as tuning the modulation depths $m$ in the brain’s HAM hierarchy.

 



% Hierarchical Amplitude Modulation offers a coherent explanation linking multiple scales of neural dynamics. By requiring logarithmic frequency spacing and limited modulation depth, HAM ensures that oscillatory bands can nest without losing their identity, providing a multi-layer carrier for information. The framework elegantly accounts for the origin of 1/f spectra as a consequence of either deep cascades or aggregated log-oscillators. It connects to laminar circuit anatomy, suggesting that the cortex’s layered structure is not just a feedforward/feedback arrangement but also a modulator/demodulator pipeline for neural signals. There are many avenues to build on this work: incorporating more realistic spiking dynamics, exploring how noise and oscillations interplay in HAM (since external noise could either disrupt or be filtered by such a system), and extending to other frequency domains (e.g., could very slow $<1$~Hz fluctuations modulate high-frequency oscillations across brain-wide networks?). The HAM model provides clear predictions and a language to discuss how the brain’s rhythms might interact to encode information. As neuroscience moves toward understanding the brain as a multi-scale communications network, HAM offers a valuable piece of the puzzle, bridging theories from engineering with the biological reality of brain oscillations.


\section{Discussion}


 

\paragraph{Oscillations in the brain.}
Oscillations are ubiquitous in neural systems and organize computation across space and time \cite{buzsaki_brain_2023, buzsakiNeuronalOscillationsCortical2004,  friesRhythmsCognitionCommunication2015,chenOscillatoryControlCortical2025}. Slow rhythms (delta–alpha/beta) tend to coordinate large-scale context and top-down control, whereas faster beta/gamma rhythms carry local content and bottom-up signals; laminar and inter-areal physiology support this spectral dissociation \cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015,spaakLayerspecificEntrainmentGammaband2012,bastosCanonicalMicrocircuitsPredictive2012,bastos_laminar_2018}. Cross-frequency coupling (CFC) provides the bridge between these scales, with slow phases modulating fast-band envelopes and timing \cite{canoltyFunctionalRoleCrossfrequency2010,lismanThetaGammaNeuralCode2013}. 


Across EEG/MEG/ECoG and intracranial recordings, canonical rhythm classes
(delta, theta, alpha, beta, gamma) occur at progressively higher center
frequencies whose separations are roughly constant on a log–frequency axis.
In practice, adjacent bands are typically separated by a factor of about
two to three (e.g., $\theta\!\sim\!4$–7\,Hz, $\alpha\!\sim\!8$–12\,Hz,
$\beta\!\sim\!15$–30\,Hz, $\gamma\!\sim\!40$–80\,Hz), yielding an
approximately geometric hierarchy of timescales \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiNeuronalOscillationsCortical2004,buzsakiRhythmsBrain2006,friesRhythmsCognitionCommunication2015,klimeschEEGAlphaOscillations2007,donoghueParameterizingNeuralPower2020}.
Penttonen and Buzsáki explicitly quantified near–log spacing in rodents
\cite{penttonenNaturalLogarithmicRelationship2003}; convergent human and
non‑human primate reviews and textbooks report comparable band centers and
boundaries, which imply ratios $r\!\approx\!2$–3 between neighboring classes
\cite{buzsakiNeuronalOscillationsCortical2004,buzsakiRhythmsBrain2006,klimeschEEGAlphaOscillations2007,friesRhythmsCognitionCommunication2015}.
Methodological work that separates periodic peaks from the aperiodic
background further confirms that these peaks recur at roughly logarithmic
intervals across individuals and tasks \cite{donoghueParameterizingNeuralPower2020}.


Recent frequency-tagging work makes this hierarchy directly observable: when two inputs are presented at distinct tagging rates, neural responses exhibit \emph{intermodulation} (IM) components at sums and differences of the tags—clear fingerprints of multiplicative mixing/demodulation predicted by HAM. This has been leveraged in steady-state visual paradigms to probe integration and attention \cite{norciaSteadystateVisualEvoked2015}. Comprehensive reviews document robust IM sidebands across visual cognition \href{https://doi.org/10.1016/j.neuroimage.2019.06.008}{\cite{gordonIntermodulationComponentsVisual2019}} and characterize their generators and applications \href{https://doi.org/10.1016/j.neuroimage.2024.120937}{\cite{chenIntermodulationFrequencyComponents2024}}. In our framework, these sidebands arise naturally from envelope–carrier architectural interactions: slow predictive rhythms (alpha/theta) multiplicatively gate fast carriers (beta/gamma), yielding IM structure around each band center while preserving separability under approximate logarithmic spacing (constant-$Q$). This same logic explains why, after constant-$Q$ preprocessing, both periodic peaks and an aperiodic $1/f^\alpha$ backbone can be read without mutual bias \cite{donoghueParameterizingNeuralPower2020,heScalefreeBrainActivity2014}.

\paragraph{Computation with oscillators.}
Beyond description, oscillations can \emph{compute}. Analog and hybrid models use coupled oscillators as primitives for signal processing and decision dynamics \cite{rietman_analog_2003}. A broader theory and hardware stack for \emph{computing with oscillators} now connects phase, frequency, and amplitude modulation to classification, constraint satisfaction, and optimization \href{https://www.nature.com/articles/s44335-024-00015-z}{\cite{todri-sanialComputingOscillatorsTheoretical2024}}. In AI, recurrent oscillator reservoirs (e.g., Kuramoto-type networks) provide nonlinear fading-memory dynamics and rich readouts for sequence learning and control \cite{yeungReservoirComputationNetworks2025,miyatoArtificialKuramotoOscillatory2025}. This may provide a route to the large energy savings realized in computational biological systems compared to artificial ones \cite{effenbergerFunctionalRoleOscillatory2025}.
Converging theory in systems neuroscience argues that neocortex itself employs recurrent oscillator networks whose computational capacity depends on the \emph{patterning and heterogeneity} of local oscillators—and that increasing heterogeneity (while avoiding overlap) enhances separability and pushes the network toward powerful operating regimes near criticality \cite{singerOscillationsNaturalNeuronal2025}. Large-scale modeling further shows how oscillatory motifs implement routing, working-memory control, and flexible task switching, with function tied to band-specific coupling and multiplexing \href{https://www.pnas.org/doi/10.1073/pnas.2412830122}{\cite{effenbergerFunctionalRoleOscillatory2025}}. 

HAM fits squarely within this computational view: it \emph{requires} oscillator heterogeneity and benefits from log-uniform (constant-$Q$) spacing to prevent IM collisions, enabling layered modulation /demodulation pipelines across cortex. In practice, this predicts that (i) modest increases in frequency diversity should improve multiplexed readout, and (ii) approximately logarithmic spacing maximizes usable channel capacity—two testable signatures in laminar/ECoG/MEG experiments and neuromorphic oscillator arrays.
In HAM, relatively fast rhythms serve as carriers whose amplitudes are multiplicatively structured by slower processes; staged demodulation then makes these envelopes readable downstream. This implements a neural analogue of multi‑layer AM in communication engineering, but realized in laminar circuits. In LaNMM, PING‑like fast generators and JR‑like slow generators map naturally onto ``carrier'' and ``envelope'' roles, with demodulation operationalized by analytic‑signal envelopes \cite{boashashEstimatingInterpretingInstantaneous1992}. These choices are consistent with laminar spectral asymmetries—gamma favoring feedforward content and alpha/beta favoring feedback predictions \cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}. 

\paragraph{From analog/digital representations to hierarchical envelopes.}
Information in engineered systems may be encoded digitally (symbolic, discrete) or analogically (continuous waveforms). Neural systems are fundamentally analog at the field level, with information carried both by oscillatory signals themselves and by their amplitude envelopes—the latter being a natural vehicle for slow context that modulates faster content \cite{buzsakiNeuronalOscillationsCortical2004}. Auditory ECoG demonstrates this duality: high‑gamma envelopes track the stimulus envelope and support direct speech reconstruction \cite{pasleyReconstructingSpeechHuman2012,tamuraCorticalRepresentationSpeech2023}. HAM formalizes a coarse‑to‑fine organization in which information can reside in the signal and in successive envelopes (envelope‑of‑envelope, etc.), yielding a principled multi‑scale code. 
Debates on neural coding (rate, temporal/ spike‑timing, phase, and burst codes) converge on the view that multiple encoding schemes coexist and are multiplexed across scales \cite{zeldenrustNeuralCodingBurstsCurrent2018,carianiTimeEssenceNeural2022,lismanThetaGammaNeuralCode2013}. In this light, HAM specifies \emph{one} concrete way slow rhythms coordinate fast content: theta/alpha phases gate gamma‑band carriers (rate/temporal information) via gain control, with nested envelopes enabling compositional, layered messages. This aligns with ``communication‑through‑coherence'' and the spectral dissociation of feedforward/feedback pathways \cite{friesRhythmsCognitionCommunication2015,vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}. More broadly, oscillator‑based computation is increasingly recognized as a viable computing paradigm; HAM connects that literature to cortical field dynamics \cite{todri-sanialComputingOscillatorsTheoretical2024, effenbergerFunctionalRoleOscillatory2025,singerOscillationsNaturalNeuronal2025}.

\paragraph{Why log spacing? Guard bands and constant-$Q$.}
Intermodulation at each stage produces sideband clusters around every center frequency. Avoiding spectral collisions across levels requires the span of slower combinations to fit inside the guard band of faster clusters. A strict super‑increasing hierarchy $f_k>2 \sum_{j>k}f_j$ suffices to keep clusters disjoint; geometric spacing $f_{k+1}=f_k/r$ satisfies this with margin for $r\gtrsim 3$ and yields constant fractional bandwidths (constant-$Q$), making demodulation scale‑invariant and mirroring classical filterbanks and cochlear tiling \cite{brownCalculationConstantSpectral1991}. Exact ratios are not essential—a weaker non‑overlap inequality already pushes the ensemble toward a log‑uniform density of centers. Empirically, $r \approx 2$--$3$ lies near the theoretical $r > 3$ threshold, explaining why real bands show limited but tolerable overlap.

\paragraph{Two routes to the aperiodic $1/f^\alpha$.}
HAM predicts $1/f^\alpha$ via two complementary mechanisms. (i) \emph{Cascade route:} with log spacing and small, quasi‑uniform modulation depth $m$, $k$‑modulator terms scale $\sim(m/2)^k$; because $k\!\sim\!\log_r(f_1/f)$, $P(f)\propto f^{-\alpha}$ with $\alpha=2\ln(2/m)/\ln r$. This links slope to circuit variables, predicting how $\alpha$ should shift with changes in $m$ or $r$. (ii) \emph{Mixture route:} a log‑uniform bank of narrowband (constant‑$Q$) oscillators already yields $\sim 1/f$ in expectation (equal power per octave). Random AM pairings preserve this backbone in the interior but induce predictable edge deviations: a small‑difference pedestal at low $f$ and a finite‑support roll‑off at high $f$. Analysis choices (edges; constant‑Hz vs.\ constant‑$Q$ smoothing) determine whether fitted $\hat\alpha$ flattens or steepens over finite windows \cite{donoghueParameterizingNeuralPower2020,heScalefreeBrainActivity2014}.

\paragraph{Frequency tagging fingerprints of HAM.}
A core prediction of multiplicative envelopes is \emph{intermodulation arithmetic}: sum/difference terms ($f_1\pm f_2$, $2f_1\pm f_2$, \dots) should emerge when two tagged inputs interact nonlinearly or when demodulation occurs. This is precisely what steady‑state visual paradigms report and exploit \cite{norciaSteadystateVisualEvoked2015,chenIntermodulationFrequencyComponents2024}. Intermodulation components (IMs) are now used to assay integrative neural mechanisms from low‑ to high‑level vision, attention, and multisensory binding, strengthening the case for HAM‑like processing \href{https://doi.org/10.1016/j.neuroimage.2019.06.008}{\cite{gordonIntermodulationComponentsVisual2019}}. In laminar circuits, HAM further predicts superficial carrier‑dominant activity coexisting with deeper envelope‑dominant readouts, matching reported directionality \cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}.

\paragraph{Methodological guidance.}
Constant‑$Q$ preprocessing and explicit separation of periodic vs.\ aperiodic components are essential to stabilize slope estimates and avoid spurious CFC \cite{donoghueParameterizingNeuralPower2020}. PAC metrics should be paired with IM readouts and envelope–envelope coupling analyses to disambiguate genuine multiplicative structure from filter/edge artifacts.

\paragraph{Experimental leverage and falsification.}
HAM makes direct, perturbation‑level predictions. (i) \emph{Spacing:} approximately geometric spacing should minimize cluster collisions and facilitate staged demodulation. (ii) \emph{IM scaling:} the amplitude of IM terms scales with products of modulation depths, providing a calibration handle in frequency‑tagged designs. (iii) \emph{Laminar dissociation:} superficial layers should carry fast carriers; deeper layers should track their envelopes in prediction‑heavy contexts. (iv) \emph{Perturbations:} reducing a slow rhythm’s amplitude should reduce the envelope variance of faster activity it modulates (steepening the PSD), whereas entraining a slow rhythm should increase that variance (flattening the PSD). These can be tested with closed‑loop stimulation and preregistered laminar/ECoG/MEG paradigms, coupled to LaNMM‑based parameter inference to link controlled changes in $m$ and $r$ to shifts in $\alpha$. 

\paragraph{Relation to alternative accounts.}
HAM offers a constructive path to fractal‑like spectra by structurally favoring criticality. It is compatible with signatures of near‑critical dynamics (avalanches, scale‑invariant correlations). Combining cascade‑based from mixture, time‑constant, or E–I explanations will require perturbational tests and careful finite‑size controls—not spectral fits alone \cite{milotti1NoisePedagogical2002,bedardMacroscopicModelsLocal2009}. 

HAM can be viewed as a mechanistic complement to descriptive models of $1/f$ noise (e.g., fractional Gaussian noise, self-organized criticality). Rather than assuming an ad hoc fractal process, HAM builds $1/f$ from interacting oscillatory components with clear physiological mapping. 
Beyond qualitative arguments for $1/f$, HAM derives two mechanistic routes—(i) a multiplicative cascade linking slope $\alpha$ to modulation depth $m$ and spacing $r$, and (ii) a mixture route from aggregating log‑spaced narrowband oscillators \cite{donoghueParameterizingNeuralPower2020}. 
It also aligns with the idea of the brain as a set of coupled oscillators that trade energy across scales. 


 Prior work has noted that constant‑$Q$ (log‑spaced) filterbanks with equal power per octave imply an overall $1/f$ (“pink‑noise”) backbone, and that cascade constructions (e.g., mixtures of relaxation times or multiplicative cascades) naturally generate $1/f^\alpha$ spectra  \cite{brownCalculationConstantSpectral1991,keshner1Noise1982,duttaLowfrequencyFluctuationsSolids1981,milotti1NoisePedagogical2002,kaulakysStochasticNonlinearDifferential2004,bacryMultifractalRandomWalk2001}.
We place that in a neural context and further link it to predictive coding dynamics (with deep layers modulating superficial layers). The concept of nested oscillatory hierarchies has appeared in theories of cortical organization; our results show how nested envelopes can carry specific information and yield testable spectral consequences.
  In particular, Klimesch’s binary hierarchy theory posits that cross‑frequency interactions are governed by two principles—phase–to–amplitude (envelope) modulation and phase–phase (harmonic) coupling—applied to brain and body oscillations arranged on a 1:2 ladder  \cite{klimeschFrequencyArchitectureBrain2018a}. HAM retains the central role of amplitude modulation but generalizes the spacing: intermodulation sidebands impose guard‑band constraints that are satisfied by \emph{any} approximately geometric (constant‑$Q$) spacing with ratio $r\gtrsim3$ (binary is a convenient special case). 
  %, which accords with log‑uniform tilings inferred from cochlear/constant‑$Q$ analyses \cite{brownCalculationConstantSpectral1991}. 
  Critically, HAM predicts explicit intermodulation fingerprints (sum/difference components) in frequency‑tagging paradigms, a prediction borne out across visual cognition \cite{gordonIntermodulationComponentsVisual2019, chenIntermodulationFrequencyComponents2024}, and maps naturally onto laminar carriers vs.\ envelopes (gamma feedforward; alpha/beta feedback) \cite{vankerkoerleAlphaGammaOscillations2014,bastosDCMStudySpectral2015}.



\paragraph{Outlook.}
Hierarchical Amplitude Modulation offers a coherent explanation linking multiple scales of neural dynamics. By requiring logarithmic frequency spacing and limited modulation depth, HAM ensures that oscillatory bands can nest without losing their identity, providing a multi-layer carrier for information. The framework elegantly accounts for the origin of 1/f spectra as a consequence of either deep cascades or aggregated log-oscillators. It connects to laminar circuit anatomy, suggesting that the cortex’s layered structure is not just a feedforward/feedback arrangement but also a modulator/demodulator pipeline for neural signals. There are many avenues to build on this work: incorporating more realistic spiking dynamics, exploring how noise and oscillations interplay in HAM (since external noise could either disrupt or be filtered by such a system), and extending to other frequency domains (e.g., could very slow $<1$~Hz fluctuations modulate high-frequency oscillations across brain-wide networks?). The HAM model provides clear predictions and a language to discuss how the brain’s rhythms might interact to encode information. As neuroscience moves toward understanding the brain as a multi-scale communications network, HAM offers a valuable piece of the puzzle, bridging theories from engineering with the biological reality of brain oscillations.



\section*{Funding}
  Giulio Ruffini, Edmundo Lopez-Sola, Borja Mercadal, and Francesca Castaldo are funded by the European Commission under European Union’s Horizon 2020 research and innovation programme Grant Number 101017716 (Neurotwin) and European Research Council (ERC Synergy Galvani) under the European Union’s Horizon 2020 research and innovation program Grant Number~855109. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 

\bibliographystyle{unsrt}

\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% A P P E N D I X %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage
\clearpage
\appendix

\section{Mathematical details}
\label{app:math}

  AM radio operates by encoding information in the amplitude of a high-frequency carrier wave. In this context, “information” typically refers to the audio signal we want to transmit, such as music or voice. This information is represented as a low-frequency signal, which modulates or varies the amplitude of the high-frequency carrier wave. The carrier itself oscillates at a frequency much higher than the signal, enabling it to be transmitted over long distances efficiently.
During modulation, the carrier’s amplitude is adjusted to follow the waveform of the audio signal, embedding the desired information within the carrier’s envelope. When received, this combined signal undergoes demodulation to extract the original audio information from the carrier. Demodulation detects variations in amplitude and reconstructs the original low-frequency signal. This process highlights how AM radio relies on distinct components: a steady carrier, the modulating information signal, and demodulation to recover the original information for playback. 
The \textit{carrier signal} \( C(t) \) can be expressed as $
C(t) = A_c \cos(2 \pi f_c t)
$, 
where \( A_c \) is the amplitude of the carrier, and \( f_c \) is the carrier frequency.
    To encode the information, the amplitude of the carrier is varied according to the lower-frequency \textit{information signal} \( m(t) \), producing the \textit{modulated signal }\( S(t) \),
    \begin{equation}\label{eq:am_model}
    S(t) = A_c\,\bigl[\,1 + a\,m(t)\bigr]\cos\bigl(2\pi f_c t\bigr), 
    \end{equation}
    where \( a\, m(t) \) is scaled such that \( 1 + a\, m(t) \) remains positive, ensuring no distortion occurs during modulation.
    Here, $A_c$ is the carrier amplitude,
       $a$ is the \emph{modulation index},
       $m(t)$ is the baseband (modulating) signal with frequency content up to $f_m \ll f_c$, and 
       $f_c$ is the carrier frequency.
This generic model subsumes both single-tone AM (where $m(t)=\cos(2\pi f_m t)$) and multi-tone/wideband modulations, as long as the maximum modulating frequency is less than $f_c$.
At the receiver, a \textit{demodulation} process retrieves the original information signal \( m(t) \) by detecting variations in the amplitude (or envelope) of \( S(t) \).
% \end{tcolorbox}
% }
%\end{figure}


 
While standard AM encoding relies on a single layer of modulation, where the information signal \( m(t) \) modulates the amplitude of a high-frequency carrier, this concept can be extended to a \textit{hierarchical framework in which multiple layers of modulation occur at different timescales (HAM.} In HAM, each modulating signal not only modifies the carrier but also modulates the envelope of another modulating signal at a slower timescale, forming a cascade of amplitude modulations across hierarchical frequency bands.


 

\subsection{HAM and logarithmic spacing of modulating bands} 
\label{sec:hierarchical_am}

Hierarchical Amplitude Modulation (HAM) refers to a nested modulation scheme in which a high-frequency carrier is amplitude-modulated by a signal, whose amplitude in turn is modulated by another slower signal, and so on across multiple layers. In essence, information is encoded at multiple hierarchical levels of the amplitude envelope. We describe this concept step by step, analyze the resulting spectra at each stage, and show why a logarithmic (geometric) spacing of carrier and modulation frequencies naturally emerges to prevent spectral overlap, as observed in electrophysiological signals \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiNeuronalOscillationsCortical2004} (see Figure~\ref{fig:logarithmic_hierarchy}). Finally, we discuss general insights into how such a structure can be demodulated and generalized.



\paragraph{Definition (single HAM cascade).}
In neural circuits, there is no singled-out ``carrier''. We therefore treat the fastest oscillator as any other building block in a multiplicative cascade around a positive baseline.
Let the highest frequency (``carrier") signal be
\[
c(t)=A_c [1+\cos(2\pi f_0 t + \phi_0)],
\]
and let \(f_0>f_1>\cdots>f_N\) be modulating tones. A \textit{depth‑\(N\) hierarchical
amplitude‑modulated signal} is
\begin{equation} \boxed{
s_N(t)
= A_c \prod_{i=0}^{N}\bigl[1+m_i\cos(2\pi f_i t+\phi_i)\bigr] 
\qquad 0<m_i<1 .}
\label{eq:ham_product}
\end{equation}

 
By expanding~\eqref{eq:ham_product} and using 
    the trigonometric identity $\cos A \cos B = \frac{1}{2} [\cos(A+B) + \cos(A-B)]$, this introduces additional sidebands.
The (one‑sided) spectrum contains lines at
\begin{equation}
f \in \Bigl\{\,  \sum_{i=0}^{N} a_i f_i \;:\; a_i\in\{-1,0,1\}\Bigr\}.
\label{eq:support}
\end{equation}
At lowest order (small $m_i$), the first two layers ($i=0,1$) contribute
$
\pm f_0$, $\pm f_1$, and $\pm(f_0\pm f_1),
$
and in general, a term involving $k$ nonzero coefficients has amplitude proportional to $\prod_{i:\,a_i\neq 0}(m_i/2)$.
 

 
At depth \(N\), the number of algebraic combinations is at most \(3^N\);
counting only distinct positive‑frequency lines (ignoring DC and mirror symmetry)
gives an upper bound \(\le (3^N-1)/2\). Degeneracies occur when
frequencies are commensurate.

\begin{figure}[t]
\centering
\begin{tikzpicture}[>=Latex, node distance=20mm]
  % Nodes (top to bottom; roughly log-spaced)
  \node (f0) at (0,0)      {$f_0\!=\!128\,$Hz};
  \node (f1a) at (-2.2,-1.6) {$f_{1a}\!=\!42.7\,$Hz};
  \node (f1b) at ( 2.2,-1.6) {$f_{1b}\!=\!36.0\,$Hz};
  \node (f2a) at (-3.2,-3.2) {$f_{2a}\!=\!14.2\,$Hz};
  \node (f2b) at ( 0.0,-3.2) {$f_{2b}\!=\!12.0\,$Hz};
  \node (f3)  at (-1.2,-4.8) {$f_{3}\!=\!4.7\,$Hz};

  % Edges (allowed modulations)
  \draw[->] (f0) -- (f1a);
  \draw[->] (f0) -- (f1b);
  \draw[->] (f1a) -- (f2a);
  \draw[->] (f1a) -- (f2b);
  \draw[->] (f1b) -- (f2a);
  \draw[->] (f2a) -- (f3);
  \draw[->] (f2b) -- (f3);

  % One highlighted path (a particular HAM cascade)
  \draw[line width=1.6pt] (f0) -- (f1a);
  \draw[line width=1.6pt] (f1a) -- (f2a);
  \draw[line width=1.6pt] (f2a) -- (f3);

  % Legend-like labels
  \node[anchor=west] at (2.8,-4.6) {Path $p$: $f_0\!\to\!f_{1a}\!\to\!f_{2a}\!\to\!f_3$};
  \node[anchor=west] at (2.8,-5.1) {$x_p(t)=A_0\prod_{v\in p}\!\bigl[1+m_v\cos(2\pi f_v t+\phi_v)\bigr]$};
\end{tikzpicture}
\caption{Directed graph of oscillators (nodes) and allowed modulations (arrows). 
A highlighted path $p$ represents one hierarchical AM cascade; the full signal 
is the sum over all such paths (Eq.~\eqref{eq:pathsum}).} \label{fig:graph}
\end{figure}


\paragraph{Graph-based HAM.} Here we analyze the more general case where multiple HAM chains interact. 
Let $G=(V,E)$ be a directed acyclic graph (DAG) of oscillators (Fig.~\ref{fig:graph}).
An edge $(v\!\to\!u)\in E$ means that the slower node $v$ \emph{modulates} the faster node $u$.
Each node $u\in V$ has a carrier $c_u(t)=\cos(2\pi f_u t+\phi_u)$ and gain $A_u>0$.
Multiple incoming modulators \emph{sum first} and then modulate the carrier at $u$.

\emph{Presynaptic drive and node output.}
Define the (dimensionless) presynaptic drive
\begin{equation}
I_u(t)\;=\;\sum_{v\in\mathrm{In}(u)} m_{v\to u}\, s_v(t),
\qquad
\mathrm{In}(u)=\{\,v:(v\!\to\!u)\in E\,\}.
\label{eq:drive}
\end{equation}
Here $s_v(t)$ is the signal exported by $v$ that serves as a modulator (e.g., a narrowband
$s_v(t)=\cos(2\pi f_v t+\phi_v)$ or, more generally, the envelope at $v$).
The node output is
\begin{equation}
x_u(t)\;=\;A_u\,[\,1+\mathcal{H}_u(I_u(t))\,]\,c_u(t),
\qquad
\mathcal{H}_u(\xi)=\sum_{q\ge 1} h^{(q)}_u\,\xi^q,\ \ h^{(1)}_u=1,
\label{eq:node_output}
\end{equation}
and the network signal is the superposition $S(t)=\sum_{u\in V} x_u(t)$.
Using \eqref{eq:node_output} and the Taylor series of $\mathcal{H}_u$,
\begin{equation}
x_u(t)
\;=\;
A_u\,c_u(t)\,
\sum_{q\ge 0} \tilde h^{(q)}_u
\!\!\!\sum_{(v_1,\dots,v_q)\in \mathrm{In}(u)^q}\!\!\!
\Bigl(\prod_{j=1}^{q} m_{v_j\to u}\Bigr)\,
\Bigl(\prod_{j=1}^{q} s_{v_j}(t)\Bigr),
\qquad
\tilde h^{(0)}_u=1,\ \tilde h^{(q)}_u=h^{(q)}_u\ (q\!\ge\!1).
\label{eq:pathsum}
\end{equation}
Recursively substituting the same form for each $s_{v_j}(t)$ yields a convergent
sum over \emph{finite directed forests} in the DAG (finite because $G$ is acyclic
and $m_{v\to u}$ are small), i.e., a global sum of time‑domain products whose coefficients
are monomials in the edge depths $m_{e}$.

\textit{Spectral support.}
Assume as before that each exported modulator has a narrowband component at $f_v$ 
(e.g., $s_v(t)=\cos(2\pi f_v t+\phi_v)$ at leaves, and more generally $s_v$ inherits the
narrowband content of its ancestors). Then every term in \eqref{eq:pathsum} is a product of cosines.
Using $\cos A\cos B=\tfrac12[\cos(A+B)+\cos(A-B)]$ iteratively gives again a spectrum as in Equation~\ref{eq:support}. To see this, order the DAG topologically. For each $u\in V$, let $\mathrm{Anc}(u)$ be its ancestors.
Then all non‑DC spectral lines of $S(t)$ lie in
\[
\Omega
\;\subseteq\;
\bigcup_{u\in V}\Bigl\{
\ \bigl|\,f_u+\sum_{v\in F}\sigma_v f_v\,\bigr|
:\ F\subseteq \mathrm{Anc}(u)\ \text{finite},\ \sigma_v\in\{-1,+1\}
\Bigr\}
\]
(with mirrored negative frequencies), i.e., \emph{linear combinations with coefficients in $\{-1,0,+1\}$}. That is, Equation~\ref{eq:support}
 holds. Amplitudes are polynomials in the $m_{e}$ (products of edge depths on the contributing
forests) times node factors $A_u,\tilde h^{(q)}_u$.
 

% \emph{Proof.} By induction on topological depth. If $u$ is a source, $x_u(t)=A_uc_u(t)$
% has a line at $f_u$. Suppose all $x_v$ for $v\in\mathrm{In}(u)$ only contain signed sums of their
% ancestors. Multiplying by $c_u(t)$ shifts each narrowband component by $\pm f_u$, and
% summing across parents preserves the signed‑sum structure. Higher powers in \eqref{eq:series}
% produce additional \(\pm\) sums but no new coefficient set beyond $\{-1,0,1\}$.
\paragraph{Spacing constraints: necessary and sufficient.}
Let $f_1>f_2>\cdots>f_N>0$ and let the sideband cluster around $f_k$ be
$\mathcal{C}_k \subset [\,f_k-S_k,\; f_k+S_k\,]$ with
$S_k=\sum_{j>k} f_j$. We now show that $\{\mathcal{C}_k\}$ are pairwise disjoint
iff $
f_k \;>\; 2\sum_{j>k} f_j \quad \text{for all } k.
$

\vspace{1em}
% In the geometric case $f_{k+1}=f_k/r$, this is equivalent to $r>3$, as we show next.


\begin{theorem}
\textbf{Necessary and sufficient condition for non-overlap of clusters.}
Let $f_1>f_2>\cdots>f_N>0$ and define the half-width
$S_k:=\sum_{j>k} f_j$, and the (one-sided, positive-frequency) sideband cluster
\[
\mathcal{C}_k \;:=\; [\,f_k - S_k,\; f_k + S_k\,] \subset \mathbb{R}_{\ge 0}.
\]
Then the family $\{\mathcal{C}_k\}_{k=1}^N$ is pairwise disjoint (strictly, i.e., no touching) if and only if
\begin{equation}
\label{eq:necessary_sufficient}
\boxed{\quad f_k \;>\; 2\sum_{j>k} f_j \quad  \forall k. \quad}
\end{equation}
\end{theorem}
\begin{proof}
\emph{Support envelope.}
By construction of the HAM spectrum, every line in the ``$k$-th cluster'' has the form
$f = f_k + \sum_{j>k} a_j f_j$ with $a_j\in\{-1,0,1\}$. Hence the smallest/largest possible
values in that cluster are obtained by taking all signs negative/positive, and thus
\[
\mathcal{C}_k \subseteq [\,f_k - S_k,\; f_k + S_k\,], \qquad S_k=\sum_{j>k} f_j.
\]
(For the interval proof it suffices to work with these extremal bounds.)

\smallskip
\noindent
\emph{Sufficiency.}
Assume \eqref{eq:necessary_sufficient}. Then $\forall k$, $f_k>2S_k$ and
\[
f_k - S_k \;>\; S_k \;=\; f_{k+1} + S_{k+1}
\]
Therefore the entire $k$-th interval lies strictly to the right of the $(k+1)$-st interval:
$\mathcal{C}_k\cap\mathcal{C}_{k+1}=\varnothing$.
Since $f_1>\cdots>f_N$ orders the intervals on the real line, disjointness of all adjacent pairs
implies the whole family $\{\mathcal{C}_k\}$ is pairwise disjoint.

\smallskip
\noindent
\emph{Necessity.}
Conversely, suppose the clusters are pairwise disjoint. In particular,
$\mathcal{C}_k$ and $\mathcal{C}_{k+1}$ do not intersect, so
\[
f_k - S_k \;>\; f_{k+1} + S_{k+1}.
\]
Using $S_k = f_{k+1} + S_{k+1}$ this reduces to $f_k - S_k > S_k$, i.e.
$f_k > 2 S_k = 2\sum_{j>k} f_j$. Since this holds for each adjacent pair, it holds for all $k$,
proving \eqref{eq:necessary_sufficient}.
\end{proof}

\paragraph{Corollary (Geometric spacing).}
If for all $k$ (log-uniform or geometric spacing), 
\begin{equation}
f_{k+1}=\frac{f_k}{r} \quad\Longleftrightarrow\quad f_k=\frac{f_0}{r^k}.%\qquad r>1.
\label{eq:geom}
\end{equation}
then
\[
S_k=\sum_{j>k} f_j \;=\; \frac{f_k}{r-1}\quad\text{(in the infinite-depth limit),}
\]
hence the condition $f_k>2S_k$ is equivalent to $r>3$. For finite depth $N$,
$S_k=\frac{f_k}{r-1}\bigl(1-r^{-(N-k)}\bigr)$, so $r\gtrsim 3$ suffices in practice with slack set
by depth, bandwidths, and modulation indices.

\paragraph{Remark (Why $f_k>\sum_{j>k}f_j$ is \emph{not} enough).}
The weaker ``super-increasing'' condition $f_k>\sum_{j>k}f_j$ guarantees that no \emph{purely
slower} combination reaches $f_k$, but it does not exclude overlap of the \emph{clusters} because
$\mathcal{C}_k$ extends down to $f_k-S_k$ while $\mathcal{C}_{k+1}$ extends up to $f_{k+1}+S_{k+1}=S_k$.
When $f_k\le 2S_k$, these bounds meet or overlap. For example, with $f_1=6.25, f_2=2.5, f_3=1$
($r=2.5$) we have $S_1=3.5$, so $\mathcal{C}_1=[2.75,\,9.75]$ and $\mathcal{C}_2=[1.5,\,3.5]$ overlap
on $[2.75,\,3.5]$ despite $f_1>S_1$.

%%%%%%

% \paragraph{Sufficient condition for non-overlap.} Because of Equation~\ref{eq:support}, sideband clusters centered at \(\pm f_k\) have maximum half-width \(\sum_{j>k} f_j\) (from all \(\pm\) combinations of slower terms). A \emph{sufficient} condition for these clusters to remain disjoint across all depths is the \emph{super-increasing} constraint
% \begin{equation}
% f_k \;>\; \sum_{j>k} f_j \qquad \text{for all } k.
% \label{eq:superincreasing}
% \end{equation}
% Under \eqref{eq:superincreasing}, the cluster around \(\pm f_k\) cannot touch that around \(\pm f_{k-1}\), and the top clusters at \(\pm f_0\) do not overlap each other.

% \paragraph{Uniqueness of signed representations (stronger margin).}
% If one further requires that every element of \(\Sigma\) admit a \emph{unique} representation
% \(\sum a_i f_i\) with \(a_i\in\{-1,0,1\}\), a slightly stronger inequality suffices:
% \begin{equation}
% f_k \;>\; 2\sum_{j>k} f_j \qquad \text{for all } k,
% \label{eq:superincreasing-strong}
% \end{equation}
% since any equality \(\sum a_i f_i=\sum b_i f_i\) would lead to
% \(|d_\ell| f_\ell \le \sum_{j>\ell} |d_j| f_j \le 2\sum_{j>\ell} f_j < f_\ell\) at the largest index \(\ell\) with \(d_i=a_i-b_i\ne 0\) — a contradiction.

% \paragraph{Geometric (log) spacing as a convenient solution.}
% A simple motif that meets \eqref{eq:necessary_sufficient} for deep cascades is a geometric progression for $r>3$,
% \begin{equation}
% f_{k+1}=\frac{f_k}{r} \quad\Longleftrightarrow\quad f_k=\frac{f_0}{r^k}.%\qquad r>1.
% \label{eq:geom}
% \end{equation}
% For an \emph{infinite} cascade,
% \[
% \sum_{j>k} f_j=\frac{f_k}{r-1}.
% \]
% %so \eqref{eq:superincreasing} holds whenever \(r>2\); the stronger uniqueness margin \eqref{eq:superincreasing-strong} holds for \(r>3\).
% For \emph{finite} depth \(N\), the tail sum is
% \(\sum_{j>k} f_j=\frac{f_k}{r-1}\!\bigl(1-r^{-(N-k)}\bigr)\), hence \(r\simeq 3\) is typically sufficient in practice for non-overlap, with slack set by modulation bandwidths and indices.

% The weaker   condition $f_k>\sum_{j>k} f_j$ (equivalently $r>2$)
% ensures that slower combinations alone cannot reach $f_k$ but does \emph{not}
% prevent the downward spread of the $(k)$th cluster from overlapping the upward
% spread of the $(k\!+\!1)$st. Thus, cluster non-overlap and uniqueness of signed
% combinations $\sum a_i f_i$ with $a_i\in\{-1,0,1\}$ both require the stronger
% margin \eqref{eq:necessary_sufficient}.


\paragraph{Constant-\(Q\) and practical benefits.}
Geometric spacing yields equal steps on a log-frequency axis and approximately constant fractional bandwidth across levels (constant-\(Q\)), which (i) preserves separability of sideband clusters, and (ii) enables scaleable filter/demodulator design. This mirrors classical constant-\(Q\) filterbanks and cochlear spacing \cite{brownCalculationConstantSpectral1991}, and matches the empirical near‑logarithmic arrangement of canonical neural bands \cite{penttonenNaturalLogarithmicRelationship2003,buzsaki_brain_2023}.


 

Log spacing in~\eqref{eq:geom} affords constant‑$Q$ filtering --- meaning each band’s $Q = \frac{\text{center frequency}}{\text{bandwidth}}$ is identical ---, so the signal is naturally demodulable in stages: bandpass around $f_0$ and apply an envelope detector to recover a composite modulation dominated by $\{f_1,f_2,\dots\}$; iterating (envelope of envelope) successively reveals $\{f_1,f_2,\ldots\}$ at lower and lower centers. The analytic‑signal formalism
\[
z(t)=s(t)+j\,\mathcal{H}\{s(t)\},\qquad a(t)=|z(t)|
\]
provides a standard implementation of amplitude extraction at each stage \cite{boashashEstimatingInterpretingInstantaneous1992}.




 




 

 

  
 




 

This hierarchical modulation structure results in nested envelopes that impose amplitude fluctuations at different timescales. As seen in the generated hierarchical modulation plots (see Figure~\ref{fig:ham_example}), a low-frequency modulator introduces a broad envelope variation, a higher-frequency modulator superimposes faster oscillations within that envelope, and so forth. The final signal thus encapsulates a multi-timescale structure where rapid oscillations are embedded within progressively slower amplitude modulations.




\paragraph{Bandwidth and modulation index considerations:}
In a multi-layer design, each modulation layer $k$ will effectively use up a certain bandwidth around each spectral line it modulates. If $m_k$ is large (deep modulation), the sidebands carry more power and one might allow a slightly larger bandwidth occupancy for that layer (for example, if the modulating signal is not a pure tone but has its own small bandwidth). However, too large $m_k$ can cause nonlinear distortion (for AM, $m_k>1$ leads to signal clipping/inversion) which can create additional unwanted spectral lines. Thus, typically one keeps $m_k \le 1$ and often $m_k \ll 1$ for higher layers so that those sidebands remain small and confined. The no-overlap condition imposes a trade-off: higher-layer modulations can occupy only a fraction of the spacing left by the layer above. 
% In practice, one might design the modulation frequencies such that
% \[
% \frac{f_{k+1}}{f_k} < \gamma
% \]
% for some $\gamma < 1$ (e.g.\ $\gamma \approx 0.5$), and choose $m_k$ accordingly small, to ensure spectral clusters from different layers do not intrude on each other. This approach mirrors the strategy used in frequency planning to avoid intermodulation interference: one leaves guard bands so that no intermodulation product lands on top of an existing signal. Indeed, in RF systems manufacturers often recommend a minimum frequency margin between any two transmitters such that third-order intermodulation products cannot coincide with another carrier frequency. Similarly, our hierarchical modulation frequencies should be spaced with sufficient margin (relative to their bandwidths) to avoid any overlap of sideband ``images.''

% Mathematically, if we treat all modulation frequencies as independent (incommensurate), the spectral lines in Eq.\,\eqref{eq:hierarchical_freqs} will be distinct. However, distinct is not enough – we also want them well-separated to be easily filterable. That is why a geometric progression of frequencies is advantageous, as we discuss next.

% \paragraph{Derivation of logarithmic spacing:}
% A key result from the above conditions is that an approximately logarithmic spacing of the carrier and modulation frequencies naturally arises as the optimal way to maintain a consistent separation between sideband clusters at each layer. By logarithmic spacing, we mean that the center frequencies at successive layers are in a constant ratio (geometric progression). Let us denote the ratio between a carrier and its modulation frequency as $r$. For example, in a two-layer scheme, $r_1 = f_c/f_1$ and $r_2 = f_1/f_2$. If we desire self-similar spectral separation at each layer, we can set these ratios equal (and similarly for further layers),
% \[
% r_1 = r_2 = \cdots = r,
% \]
% so that 
% \[
% f_1 = \frac{f_c}{r},\quad f_2 = \frac{f_1}{r} = \frac{f_c}{r^2},\quad f_3 = \frac{f_c}{r^3},
% \]
% and in general $f_k = f_c/r^k$. This geometric series of frequencies corresponds to equal spacing on a logarithmic frequency axis (e.g.\ each layer might be one octave or one decade apart if $r=2$ or $r=10$, respectively). Logarithmic spacing directly implies a constant proportional bandwidth or Quality-factor: the ratio of frequency to bandwidth remains the same for each layer. In fact, using a logarithmic frequency scale produces a set of bands that form a constant-$Q$ filter bank, meaning each band’s $Q = \frac{\text{center frequency}}{\text{bandwidth}}$ is identical. Equivalently, all bands have the same fractional bandwidth when spaced geometrically.

% In the context of hierarchical AM, this is highly desirable. If each modulation frequency $f_{k+1}$ is, say, a fixed fraction of $f_k$ (i.e.\ $f_{k+1} = \alpha\,f_k$ for some $0<\alpha<1$), then each layer occupies the same fraction of its carrier’s spectrum. For instance, if $\alpha=0.5$ (spacing of one octave per layer), each new pair of sidebands sits at $\pm 50\%$ of the previous layer’s carrier frequency. This yields a uniform margin: the sidebands from layer $k+1$ will reach at most $\pm 50\%$ of $f_k$ away from each $f_c \pm \cdots \pm f_k$ line. As a result, none of those sidebands can touch the adjacent cluster (centered at $f_c \pm \cdots \pm f_{k-1}$), because the gap to the next cluster is also about 50\% of $f_k$ on each side. Geometric scaling thus guarantees a fixed relative separation between sideband groups at every layer. In contrast, if one attempted linear spacing (e.g.\ $f_k = f_c - (k-1)\Delta f$ for some fixed $\Delta f$), the highest-frequency layers would be cramped together (small absolute gaps near $f_c$) while the lowest layers would be excessively far apart in relative terms. The logarithmic choice equalizes these proportional gaps.

% Another way to derive the need for log-spacing is by considering the cumulative frequency span of the lower layers. Suppose we use $N$ layers. If each layer $k$ is a fraction $\alpha$ of the previous, then the total fraction of $f_1$ occupied by all lower layers is $\alpha + \alpha^2 + \cdots + \alpha^N < \frac{\alpha}{1-\alpha}$ (for $\alpha<1$). If $\alpha$ is small (strongly logarithmic spacing), the sum is bounded and the highest layer ($f_1$) dominates the spacing. As $N$ grows (more layers), there is still room because the series converges. In the extreme case of infinitely many layers, avoiding overlap would require $\alpha \le 0.5$ so that $\alpha/(1-\alpha) \le 1$ (the worst-case limit where the sum of all lower sideband spans equals $f_1$ but never exceeds it). This aligns with our earlier heuristic that each frequency should exceed the sum of all lower frequencies. Thus, from first principles of avoiding intermodulation overlap, one is naturally led to a geometric series of frequencies. It is no coincidence that many physical and engineered systems use log-spaced frequency bands to handle multi-scale signals. For example, the human auditory system’s frequency sensitivity is roughly logarithmic; the cochlea can be modeled as a bank of bandpass filters whose center frequencies follow the Greenwood equation (approximately exponential distribution along the cochlear length), resulting in roughly constant-$Q$ filtering across the audible spectrum. Indeed, analysis of speech and music often employs a logarithmic frequency axis for amplitude modulation spectra, capturing slow prosodic modulations and faster phonetic modulations in separate bands. In the same vein, a hierarchical modulator with geometric spacing ensures that each layer can be demodulated with a similar relative bandwidth and tuning, simply scaled in frequency. This self-similarity greatly simplifies the design of demodulation filters and circuits, as discussed below.




% \subsubsection*{Avoiding Distortion with $m < 1$} 
%  The modulation index $m$ (modulation depth) governs how strongly each oscillator modulates the next. Crucially, \textbf{when $m < 1$}, each factor remains positive for all $t$ (since $1 + m\cos(\cdot) > 0$ as the minimum value is $1 - m > 0$). In this \textbf{under-modulation} regime, the slower oscillations simply scale the amplitude of faster ones without inverting or distorting them. The envelope of the signal accurately follows the shape of the modulating wave, maintaining a one-to-one correspondence between modulator and envelope. Each modulation layer can thus be treated separately as a linear amplitude scaling of the next layer.

% If \textbf{$m = 1$} (100\% modulation), the lowest value of $1 + m\cos(\cdot)$ reaches zero. At those moments the amplitude of the affected oscillator is zero, causing \textbf{critical modulation} where the carrier (or faster wave) is completely suppressed at troughs. At \textbf{$m > 1$}, \textbf{overmodulation} occurs -- the term $1 + m\cos(\cdot)$ becomes negative during part of the cycle. This means the instantaneous amplitude inversion of that layer (the carrier’s phase flips by 180$^\circ$ when the amplitude goes negative). The result is a \textbf{distorted envelope} and a breakdown of the simple multiplication structure into a nonlinear regime. Instead of a clean amplitude modulation, the slower wave now also induces phase reversals in the faster wave.

 
% A key virtue of keeping the modulation depth $m$ below 1 is that it preserves an orderly spectral structure. \textbf{Moderate modulation ($m < 1$)} results in a \textbf{spectrum with a clear power-law or hierarchical distribution} of energy: the majority of signal power remains concentrated at the fundamental frequencies of each oscillator (and their primary sidebands), and progressively less power appears at higher-order combination frequencies. Intuitively, each successive modulation layer contributes smaller amplitude fluctuations (scaled by powers of $m$), so the spectral contributions diminish rapidly for higher-order terms. This often manifests as a \textbf{1/f-like decay} in power: lower-frequency modulators (being first-order terms) have the strongest amplitudes, and higher-frequency modulations or sidebands (higher-order products) have much lower power.

% When \textbf{$m$ is too high (approaching or exceeding 1)}, the spectral purity is compromised. \textbf{Excessive modulation depth ($m \geq 1$)} drives strong nonlinearities that \textbf{generate a plethora of harmonics and intermodulation frequencies}. Instead of a neat pair of sidebands or a few small extra peaks around each fundamental frequency, the signal develops a wide range of frequency components. The end result is \textbf{spectral clutter}: many frequencies carry significant power. This disrupts any power-law decay; higher-order terms no longer fade out but instead can carry comparable energy.
 
Natural systems that exhibit hierarchical oscillations appear to enforce the $m < 1$ principle to maintain functional integrity. The brain produces many oscillatory nested rhythms (delta, theta, alpha, beta, gamma, etc.).   Maintaining $m < 1$ in these neural modulations preserves \textbf{well-separated oscillatory bands}. Each brainwave band retains its characteristic frequency range and identity, which is important if different bands are to carry separable information streams.

In summary, \textit{overmodulation is avoided in both engineered HAM systems and natural oscillatory hierarchies} because it leads to distortion and loss of information. The condition $m < 1$ keeps each modulation layer linear and the overall signal decomposable into distinct frequency components.

\subsection{Power scaling} \label{app:powerlaw}

\paragraph{Sideband spectra.} We first examine sideband spectra. With equal depths \(m_i\equiv m\in(0,1)\) and log spacing \(f_{k+1}=f_k/r\) (\(r>1\)),
the amplitude of a component involving \(k\) modulators scales as \((m/2)^k\).
Because \(f_k \propto r^{-k}\), we have \(k \approx \log_r(f_1/f_k)\). Hence
\[
A(f_k) \propto \left(\frac{m}{2}\right)^{\log_r (f_1/f_k)}
= \left(\frac{f_k}{f_1}\right)^{\,\log_r(m/2)} ,
\]
and the power scales as
\[
P(f_k)\propto \left(\frac{f_k}{f_1}\right)^{\,2\log_r(m/2)}
= \frac{1}{f_k^{\,\alpha}},\qquad \text{ with }
 \ \alpha \;=\; \frac{2\ln(2/m)}{\ln r}\ >0\ .
\]
Thus, for fixed \(r\), \(\alpha\) increases as \(m\) decreases (weaker modulation \(\Rightarrow\)
steeper slope), and for fixed \(m\), \(\alpha\) decreases as \(r\) increases (wider spacing
\(\Rightarrow\) shallower slope, see Figure~\ref {fig:power_law}). %This is a multiplicative‑cascade result for HAM under log spacing, not a universal account of aperiodic structure.



\paragraph{Full spectrum.} Next, we model a random ensemble of logarithmic oscillators interacting via modulation. 
Consider an ensemble of narrowband oscillators with center frequencies \(f\in[F_{\min},F_{\max}]\).
Assume (i) \emph{log‑uniform} center density \(\rho(f)=C/f\) (equal expected number per
log‑frequency bin, i.e., equal energy per octave), where \(C=[\ln(F_{\max}/F_{\min})]^{-1}\);
(ii) each oscillator contributes the same unit power, shaped by a unit‑area spectral kernel
\(K_\varepsilon(\cdot)\) that is narrow relative to its center (ideally constant‑\(Q\), i.e. fractional
bandwidth approximately constant across \(f\)).%
Equal energy per octave $\Rightarrow$ pink noise ($1/f$) is a classical result in audio/time–frequency
analysis; constant‑$Q$ filterbanks formalize this \emph{logarithmic} tiling.


\medskip
\noindent\textbf{Lemma 1 (Log‑spaced ensembles yield a $1/f$ backbone).}
Let \(S_0\) denote the ensemble power spectral density (PSD) obtained by summing all oscillators:
\[
S_0(f)=\int_{F_{\min}}^{F_{\max}}\rho(u)\,K_\varepsilon(f-u)\,du.
\]
If \(K_\varepsilon\) is narrow (or constant‑\(Q\)), then, for \(f\) away from band edges,
\[
S_0(f)\;\approx\;\rho(f)\underbrace{\int K_\varepsilon}_{=1}\;=\;\frac{C}{f}\,.
\]
\emph{Intuition.} Convolving a slowly varying density \(\rho\) with a narrow kernel evaluates
\(\rho\) locally. With \(\rho(f)\propto 1/f\), equal oscillator mass per octave gives a pink (\(1/f\))
baseline.

We now assume that the oscillators are coupled randomly as carrier and modulator (second-order pairing).  
We draw unordered pairs \((f_c,f_m)\) i.i.d.\ from \(\rho\) with the \emph{AM constraint} \(f_m<f_c\).
First‑order AM generates sidebands at \(f_c\pm f_m\) with weights \(w_\pm\) (for small modulation
depth, \(w_\pm\propto m/2\)).%

% \footnote{Intermodulation/AM sidebands at $f_c\pm f_m$ (and higher‑order sums) are standard in
% nonlinear/SSVEP analyses 
% \cite{norciaSteadystateVisualEvoked2015, gordonIntermodulationComponentsVisual2019,chenIntermodulationFrequencyComponents2024}}

\paragraph{Lemma 2 (Sidebands from a log-uniform population).}
Let $f_c,f_m$ be i.i.d.\ on $[F_{\min},F_{\max}]$ with density $\rho(f)=C/f$, conditioned on $f_c>f_m$. Define lower/upper sideband frequencies $f_L=f_c-f_m$ and $f_U=f_c+f_m$. Then
\[
D(f_L)=\int_{F_{\min}+f_L}^{F_{\max}} \frac{C^2}{a(a-f_L)}\,da
\ =\ \frac{C^2}{f_L}\,\ln\!\left(\frac{(F_{\max}-f_L)(F_{\min}+f_L)}{F_{\max}F_{\min}}\right),
\]
\[
U(f_U)=\!\!\int_{\max(F_{\min},\,f_U-F_{\max})}^{\min(F_{\max},\,f_U-F_{\min})}
\frac{C^2}{a(f_U-a)}\,da
\ =\ \frac{C^2}{f_U}\,\ln\!\Bigg(\frac{a_1[\,f_U-a_0\,]}{a_0[\,f_U-a_1\,]}\Bigg),
\]
where $a_0,a_1$ are the integration limits above.
For $f$ in the interior of $[F_{\min},F_{\max}]$ both marginals satisfy
\[
D(f)=\frac{\kappa_-(f)}{f},\qquad U(f)=\frac{\kappa_+(f)}{f},
\]
with $\kappa_\pm(f)$ bounded and slowly varying (logarithmic) functions of $f$.
As $f\to 0$, $D(f)\to C^2(\frac{1}{F_{\min}}-\frac{1}{F_{\max}})$ (a constant pedestal),
while $U(f)$ exhibits a high-frequency roll-off near $F_{\max}$ due to finite support.
Thus the \emph{expected} sideband contribution preserves the $1/f$ backbone in the interior,
with deviations driven by a low-$f$ pedestal (flattening fitted slopes) and a high-$f$ truncation
(steepening locally).%
%\footnote{The same conclusions follow by changing variables to ratio $r=f_c/f_m$:
%$f_L=(r-1)f_m$, $f_U=(r+1)f_m$, giving $D(f)\propto(1/f)\int (dr/r)$ and
%$U(f)\propto(1/f)\int (dr/r)$ with edge-dependent bounds.}




\medskip
\noindent\textbf{Corollary 1 (Total PSD).}
With carrier lines \(S_0(f)\propto 1/f\) and sidebands \(S_{\pm}(f)\) as above, the expected total PSD is
\[
S_{\mathrm{tot}}(f)
\;=\; S_0(f) + w_-\,D(f) + w_+\,U(f)
\;=\; \frac{\kappa_1}{f}\ +\ \kappa_0\ +\ \text{edge/log corrections}.
\]
Hence the ensemble remains approximately \(1/f\) with an \emph{additive constant} \(\kappa_0\)
determined by small‑difference sidebands and finite‑band edges. Larger \(\kappa_0\) flattens the
apparent slope \(\alpha\) when fitting over restricted ranges.

\medskip
\noindent\textbf{Finite‑range and bandwidth effects.}
(i) \emph{Finite support} \([F_{\min},F_{\max}]\) produces curvature near edges:
high‑frequency sum‑sidebands are truncated at \(F_{\max}\), and very small differences accumulate
near \(F_{\min}\), both tending to \emph{flatten} slopes. (ii) Using a \emph{constant absolute}
kernel width (e.g., Gaussian \(\sigma=\) 0.5\,Hz) is not constant‑\(Q\) and biases the high‑frequency
region. Adopting \(\sigma=\eta f\) (fractional width) restores scale invariance and improves linearity
of log–log fits. These two points explain why, in our simulation, log‑spaced centers produce a
clear \(1/f\) (\(\alpha\approx 1.0\)) without sidebands, while adding sidebands slightly reduces the
fitted \(\alpha\) (constant pedestal added),  as observed in Fig.~\ref{fig:logspectra}.%
%\footnote{See your Fig.~6 for slopes with/without sidebands; constant‑$Q$ improves linearity and stabilizes $\alpha\approx 1$ over decades.}
% (Refer to figure cross‑reference or page number as available in the manuscript.)


\paragraph{Does geometric spacing matter for the $1/f$ law?}
The derivation of $S(f)\!\sim\!1/f$ relies only on the \emph{density} of oscillator
centers $\rho(f)$, not on exact geometric ratios.  Geometric spacing
($f_{k+1}=f_k/r$) yields $\rho(f)\propto 1/f$ exactly, but any monotone sequence
that satisfies the  \emph{super-increasing} condition
\[
f_k>2 \sum_{j>k}f_j
\]
implies that the ratios \(r_k=f_k/f_{k+1}\) are all \(>3\), though not necessarily
constant. This is because
$$
r_k=\frac{f_k}{f_{k+1}}> \frac{2 \sum_{j>k}f_j}{f_{k+1}}= 1 + \frac{2 \sum_{j>k+1}f_j}{f_{k+1}} >3
$$
Writing $\log f_{k+1}=\log f_k-\log r_k$, the sequence of log-frequencies
performs a random walk with mean step $-\langle \log r\rangle$.  By the law of
large numbers,
\[
\log f_k \approx \log f_0 - k\,\langle \log r\rangle,
\qquad
k \approx \frac{\log(f_0/f_k)}{\langle \log r\rangle}.
\]
The cumulative count of oscillators above frequency $f$ is therefore
\[
N(f)=\#\{k: f_k>f\}\approx
\frac{\log(F_{\max}/f)}{\langle \log r\rangle},
\]
so the differential density is
\[
\rho(f)=-\frac{dN}{df}=\frac{1}{\langle \log r\rangle}\frac{1}{f}.
\]
Hence any super-increasing sequence with bounded ratios
and finite $\langle \log r\rangle$ yields an approximately log-uniform
distribution of oscillator centers.  The strict geometric case
($r_k\!\equiv\!r$) is a special instance where $\rho(f)=C/f$ exactly.
Thus, the weaker non-overlap condition suffices—geometric spacing merely
provides a clean analytical form.
Thus, geometric spacing is a \emph{sufficient} but not
\emph{necessary} condition; the weaker non-overlap constraint already implies an
asymptotically log-uniform distribution and therefore the same $1/f$ scaling in
expectation --- geometric spacing merely
provides a clean analytical form.

  
As we saw, for a \emph{single multiplicative cascade} with geometric spacing \(r>1\) and modulation depth \(m\),
the toy analysis gives \(P(f)\sim f^{-\alpha}\) with
\(
\alpha={2\ln(2/m)}/{\ln r}
\)
(derived earlier). That mechanism ties \(\alpha\) to \((r,m)\) within one chain. By contrast, the
\emph{population mixture mechanism} above yields \(\alpha\approx 1\) from the log‑spaced
\emph{distribution of centers} alone, even without sidebands. Both mechanisms can coexist in
neural data; careful parameterization is needed to separate aperiodic from oscillatory peaks.%
On disentangling aperiodic and periodic structure, see Donoghue et al. (2020) \cite{donoghueEvaluatingComparingMeasures2024}.  
Other accounts of $1/f^\alpha$ (mixtures of timescales, synaptic filtering, E/I balance, etc.) are
reviewed in  Bédard \& Destexhe (2009)  and Milotti (2002) \cite{milotti1NoisePedagogical2002}.
 

In summary,
(1) A bank of log‑spaced oscillators with equal power per band produces \(S(f)\approx 1/f\).
(2) Random AM pairings add a constant pedestal; stronger modulation (\(m\)) or more pairs
increase \(\kappa_0\) and flatten the fitted \(\alpha\) over finite windows. (3) Using constant‑\(Q\)
kernels (or filters) should push the fitted slope in Fig.~\ref{fig:logarithmic_hierarchy} toward \(\alpha\approx 1\) and
increase the fitting range.
The assumption \(\rho(f)\propto 1/f\) parallels empirical \emph{logarithmic spacing} of neural bands
(delta\(\to\)gamma) \cite{penttonenNaturalLogarithmicRelationship2003} and is mathematically equivalent to a constant‑\(Q\)
tiling (equal spacing on a log‑frequency axis) \cite{brownCalculationConstantSpectral1991}. 




\paragraph{Logspacing simulation.} As an example of how logspacing leads to power laws, we first simulated 5,000 uncoupled harmonic oscillators sampled either uniformly or logarithmically between 0.5–200 Hz. Sampled oscillator pairs (a carrier and a modulator) were constrained such that the modulator frequency \( f_m \) was less than the carrier frequency \( f_c \). For each valid pair, we added power at \( f_m \) and \( f_c \) using a Gaussian kernel ($\sigma$ = 0.5 Hz). In a second condition, we also added power at the AM-generated sidebands \( f_c \pm f_m \). The resulting aggregate spectrum was normalized, and its slope \( \alpha \) estimated via linear regression in log-log space for \( f > 5 \) Hz. Figure \ref{fig:logspectra} provides an overview of the findings:  
For a \textit{uniform frequency distribution,} there is no power-law scaling (\( \alpha \approx 0.08 \), flat spectrum). For 
\textit{log-spaced frequencies without sidebands}, a clear power-law (\( \alpha \approx 0.84 \)) demonstrates that log-distributed source density and hierarchical pairing are sufficient. Finally, for 
  \textit{log-spaced frequencies, with sidebands,} a shifted scaling is found (\( \alpha \approx 0.95 \)).

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/Power-Law Fits: Effect of Sidebands on Spectral Slope.png}
    \caption{ Power spectra and fitted power-law slopes for hierarchical amplitude modulation (AM) simulations with $f_m < f_c$. 
The \textit{uniform} oscillator distribution shows no meaningful scaling ($\alpha \approx 0.08$). 
The \textit{log-spaced} distribution yields a clear $1/f^\alpha$ structure. 
Including AM sidebands ($f_c \pm f_m$) results in a slightly steeper slope ($\alpha \approx 0.95$) than when only the modulator and carrier frequencies are included ($\alpha \approx 0.84$), reflecting enhanced low-frequency accumulation. 
Dashed lines indicate linear fits in log-log space over $f > 5$ Hz.}
    \label{fig:logspectra}
\end{figure}
%https://colab.research.google.com/drive/1Pi7JAZTRPXa7-UuQ-hx75XIW4hYB8_qr




 We further tested how the geometric spacing ratio between oscillator frequencies affects the emergence of the power law. Using between 5 and 100 logarithmically spaced bins to populate the frequency range (0.5–200 Hz), we found that the slope $\alpha$ increases rapidly between 5 and 20 bins (corresponding to spacing ratios $r \approx 3.31$ to $1.35$), but plateaus beyond 30 bins ($r \approx 1.22$). Increasing the number of bins beyond 30 had no meaningful impact on the spectral slope, which stabilized near $\alpha \approx 0.96$. This indicates that while logarithmic spacing is essential, only moderate resolution is needed to fully express the scaling. Finer band divisions contribute little additional structure once the modulation space is densely sampled.
 
%------------------------------------------------------------
% TEXT POLICY: Guard-bands, spacing, and non-overlap (paste once)
% Requires: amsmath, amssymb, xcolor, tcolorbox (already in your preamble)
%------------------------------------------------------------

% --- Short, reusable macros (use these in the text to stay consistent) ---
\newcommand{\geomspace}{\emph{geometric spacing}}
\newcommand{\constQ}{constant-$Q$}
\newcommand{\ratio}{\ensuremath{r}}            % spacing ratio
\newcommand{\theoryr}{\ensuremath{r>3}}        % theory threshold for disjoint clusters
\newcommand{\weakr}{\ensuremath{r>2}}          % weaker super-increasing threshold
\newcommand{\empiricalr}{\ensuremath{r\approx 2\text{--}3}} % empirical observation
\newcommand{\gbnoc}{\textsc{GBNO}}             % Guard–Band Non–Overlap (short name)
\newcommand{\si}{\textsc{SI}}                  % Super–Increasing (short name)

% Inequalities as named snippets (for equations or inline)
\newcommand{\GBNOineq}{\ensuremath{f_k \;>\; 2\sum_{j>k} f_j \quad \forall k}}
\newcommand{\SIineq}{\ensuremath{f_k \;>\; \sum_{j>k} f_j \quad \forall k}}
\newcommand{\GeomDef}{\ensuremath{f_{k+1} = f_k/\ratio}}
\newcommand{\GeomGBNO}{\ensuremath{\GeomDef\ \Rightarrow\ \ratio>3}}
\newcommand{\GeomSI}{\ensuremath{\GeomDef\ \Rightarrow\ \ratio>2}}

% --- The policy box shown once in the manuscript ---
\begin{tcolorbox}[title={\textbf{Terminology: guard-bands, spacing, and non-overlap}},
                  colback=gray!2, colframe=gray!40, left=1mm, right=1mm, top=1mm, bottom=1mm]

\textbf{Terminology.}
\begin{itemize}
  \item \textbf{Non-overlap (default meaning).} Throughout the paper, \emph{“non-overlap”} means
        \textbf{Guard–Band Non–Overlap} (\gbnoc): 
        \[
          \GBNOineq.
        \]
        Under \geomspace, i.e., \( \GeomDef \), this is equivalent to \( \theoryr \).
  \item \textbf{Weaker condition.} The \textbf{Super–Increasing} (\si) hierarchy
        \[
          \SIineq
        \]
        (geometric case \( \GeomSI \)) prevents slower combinations from \emph{reaching} \(f_k\) but
        \emph{does not} guarantee that the downward spread of the \(k\)th sideband cluster cannot
        overlap the upward spread of the \((k{+}1)\)st. Hence \si\ is \emph{insufficient}
        for strict cluster separation and uniqueness of signed combinations.
  \item \textbf{Geometric/log spacing and \constQ.} We use \geomspace\ (\(f_{k+1}=f_k/\ratio\)) to describe
        log‑uniform centers and a \constQ\ tiling. When we discuss theoretical guarantees
        (non‑overlap, uniqueness, staged demodulation), we state \(\theoryr\). When we describe
        empirical bands, we state \( \empiricalr \) and note that modest overlap can appear in
        practice for finite depth and small modulation.
\end{itemize}

\textbf{Demodulation requirement.}
\begin{itemize}
  \item \emph{Sequential demodulation without interference} assumes \gbnoc\ (\(\theoryr\) under geometry),
        so that each carrier’s sideband cluster is disjoint from adjacent clusters.
  \item When referring to “adequate separation” in simulations or data where depth is finite
        and modulation is moderate (\(m<1\)), we may note that \si\ (\(\weakr\)) can suffice
        operationally, but we reserve the term \emph{“non‑overlap”} \emph{strictly} for \gbnoc.
\end{itemize}

\textbf{Summary of statements.}
\begin{itemize}
  \item \emph{Theory:} Cluster non‑overlap (GBNO) requires \( \GBNOineq \), i.e., \( \GeomGBNO \).
  \item \emph{Weaker statement:} The super‑increasing hierarchy \( \SIineq \) (geometric: \( \GeomSI \))
        prevents slower combinations from reaching \(f_k\) but does not preclude adjacent cluster overlap.
  \item \emph{Empirical:} Canonical bands are roughly log‑spaced with \( \empiricalr \), close to but sometimes
        below the theoretical \( \theoryr \), which explains limited yet tolerable overlap in practice.
\end{itemize}

\end{tcolorbox}

% \paragraph{Simulation.} As an example, we simulated 5,000 uncoupled harmonic oscillators sampled either uniformly or logarithmically between 0.5–200 Hz. Sampled oscillator pairs (a carrier and a modulator) were constrained such that the modulator frequency \( f_m \) was less than the carrier frequency \( f_c \). For each valid pair, we added power at \( f_m \) and \( f_c \) using a Gaussian kernel ($\sigma$ = 0.5 Hz). In a second condition, we also added power at the AM-generated sidebands \( f_c \pm f_m \). The resulting aggregate spectrum was normalized, and its slope \( \alpha \) estimated via linear regression in log-log space for \( f > 5 \) Hz. Figure \ref{fig:logspectra} provides an overview of the findings:  
% For a \textit{uniform frequency distribution,} there is no power-law scaling (\( \alpha \approx 0.08 \), flat spectrum). For 
% \textit{log-spaced frequencies without sidebands}, a clear power-law (\( \alpha \approx 0.84 \)) demonstrates that log-distributed source density and hierarchical pairing are sufficient. Finally, for 
%   \textit{log-spaced frequencies, with sidebands,} a shifted scaling is found (\( \alpha \approx 0.95 \)).

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/Power-Law Fits: Effect of Sidebands on Spectral Slope.png}
%     \caption{ Power spectra and fitted power-law slopes for hierarchical amplitude modulation (AM) simulations with $f_m < f_c$. 
% The \textbf{uniform} oscillator distribution shows no meaningful scaling ($\alpha \approx 0.08$). 
% The \textbf{log-spaced} distribution yields a clear $1/f^\alpha$ structure. 
% Including AM sidebands ($f_c \pm f_m$) results in a slightly steeper slope ($\alpha \approx 0.95$) than when only the modulator and carrier frequencies are included ($\alpha \approx 0.84$), reflecting enhanced low-frequency accumulation. 
% Dashed lines indicate linear fits in log-log space over $f > 5$ Hz.}
%     \label{fig:logspectra}
% \end{figure}
% %https://colab.research.google.com/drive/1Pi7JAZTRPXa7-UuQ-hx75XIW4hYB8_qr


% The emergence of $1/f^\alpha$ spectral structure in hierarchical AM systems is fully explained by two minimal structural constraints: (1) a log-spaced distribution of oscillators and (2) a modulation rule enforcing \( f_m < f_c \). Sidebands are not necessary for scale-free behavior but do enhance low-frequency accumulation and steepen the slope.

% We further tested how the geometric spacing ratio between oscillator frequencies affects the emergence of the power law. Using between 5 and 100 logarithmically spaced bins to populate the frequency range (0.5–200 Hz), we found that the slope $\alpha$ increases rapidly between 5 and 20 bins (corresponding to spacing ratios $r \approx 3.31$ to $1.35$), but plateaus beyond 30 bins ($r \approx 1.22$). Increasing the number of bins beyond 30 had no meaningful impact on the spectral slope, which stabilized near $\alpha \approx 0.96$. This indicates that while logarithmic spacing is essential, only moderate resolution is needed to fully express the scaling. Finer band divisions contribute little additional structure once the modulation space is densely sampled.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


 
 
 
 
%  \section{Hierarchical Neural Encoding in the Brain}
% \label{sec:hierachical}



% %  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{The role of fast and slow oscillations}
% Cross-frequency phase-amplitude coupling between gamma and other rhythms within the same and different brain regions has been well documented, including modulation by theta, alpha, spindle, slow, and ultraslow oscillations \cite{buzsakiMechanismsGammaOscillations2012}. This modulation often occurs via phase-amplitude coupling, where the phase of a slow oscillation influences the amplitude of a faster oscillations \cite{buzsakiRhythmsBrain2006}.
% Gamma oscillations, occurring at frequencies between 30--100 Hz, play crucial roles in various brain functions. They are essential for integrating information within neural circuits, supporting complex processes such as perception, cognition, and memory \cite{buzsakiMechanismsGammaOscillations2012, jensenHumanGammafrequencyOscillations2007, friesNeuronalGammabandSynchronization2009}. In the motor cortex, gamma oscillations are linked with movement and learning, where they enhance motor control and skill acquisition \cite{nowakMotorCorticalGamma2018}. 
% Gamma oscillations also facilitate the binding of perceptual features in the cortex, as well as the integration of diverse information in the hippocampus, contributing to episodic memory formation and retrieval \cite{nyhusFunctionalRoleGamma2010}. Disruptions in gamma rhythms have been associated with neurological and psychiatric disorders, including schizophrenia, Alzheimer's disease, and Parkinson's disease \cite{shinGammaOscillationSchizophrenia2011, guanRoleGammaOscillations2022}. The network of inhibitory interneurons is critical for generating gamma oscillations, and disturbances within this network can lead to pathological conditions \cite{guanRoleGammaOscillations2022}.
% Recent studies have explored gamma entrainment through sensory stimulation as a potential therapeutic approach for neuropsychiatric diseases, showing promising avenues for treatment \cite{manippaUpdateUseGamma2022,blackTherapeuticPotentialGamma2024}. As these findings suggest, gamma oscillations are not only fundamental to normal brain function but also present important clinical implications, highlighting the need for further research into gamma-based therapeutic strategies.

% Gamma oscillations, particularly in the 30-100 Hz range, are widely proposed as mechanisms for information encoding in the brain. Fries (2009) \cite{friesNeuronalGammabandSynchronization2009} suggests that gamma-band synchronization allows for \textit{communication through coherence}, aligning excitability phases across neural populations to enhance selective information routing and integration between cortical areas. This alignment is thought to provide a temporal framework within which neural populations can encode and transfer information efficiently. Singer and Gray (1995) \cite{singerVisualFeatureIntegration1995} proposed that gamma oscillations play a crucial role in \textit{feature binding}, encoding sensory information by synchronizing distributed neural assemblies, effectively linking spatially separated representations into coherent perceptual constructs. Jensen et al. (2007) \cite{jensenHumanGammafrequencyOscillations2007} further emphasized gamma oscillations as \textit{carriers of task-relevant information} across neural networks, particularly in contexts involving attention and working memory. By selectively amplifying specific representations, gamma rhythms enhance signal-to-noise ratios, facilitating the encoding and prioritization of important information. These studies collectively underscore gamma oscillations as essential temporal structures for dynamic encoding, enabling effective communication and information processing across brain networks.  

% Results in recent animal studies suggest that unpredicted stimuli—i.e. “deviants” in an oddball paradigm—evoked robust responses in supragranular layer 2/3, arising after 100-ms post-stimulus, with processing of contextually deviant stimuli in the oddball paradigm involving increases in theta- and gamma-band oscillations in layer 2/3 \cite{gallimoreSpatiotemporalDynamicsVisual2023}. These frequency bands and this layer of cortex are believed to carryout feed-forward processing, which is consistent with deviance detection reflecting a “prediction error” that is fed-forward in cortical circuits. 
% In summary, gamma rhythms are fundamental carriers of information in the brain with a central multifunctional role in neural systems for perception, selective attention, memory, motivation, and behavioral control \cite{bosmanFunctionsGammabandSynchronization2014}.

% %\subsection{Alpha and theta}
% Alpha (8–12 Hz) and theta (4–8 Hz) oscillations have been associated with distinct but complementary roles in neural processing. Alpha rhythms are primarily linked to inhibition and attentional gating, where increased alpha power helps to filter out irrelevant sensory information by suppressing activity in areas not directly involved in a task, thus aiding selective attention \cite{klimeschEEGAlphaOscillations2007,jensenShapingFunctionalArchitecture2010}. This inhibitory function allows alpha to act as a ``gate" in sensory processing areas, modulating cortical activity and stabilizing cognitive processing \cite{hanslmayrRoleOscillationsTemporal2011}. In contrast, theta oscillations are closely tied to cognitive control, working memory, and inter-regional synchronization, particularly within frontal-midline regions during tasks requiring sustained attention and decision-making \cite{klimeschEEGAlphaTheta1999,sederbergThetaGammaOscillations2003,voytekShiftsGammaPhase2010}.
% Furthermore, gamma-oscillations propagate in the feedforward direction, whereas alpha-oscillations propagate in the feedback direction \cite{kerkoerleAlphaGammaOscillations2014}.
% Recent studies have explored how predictions are encoded in neural oscillations, particularly in the alpha and beta frequency bands. Alpha oscillations have been found to carry stimulus-specific visual predictions before stimulus onset, influencing subsequent perceptual performance  \cite{hetenyiPrestimulusAlphaOscillations2024a}. Similarly, alpha/low-beta oscillations in the occipital cortex have been shown to predictively encode the position of moving stimuli, supporting the view of these rhythms as a spectral ``fingerprint" of hierarchical predictive processing \cite{turnerVisualInformationPredictively2023}. Prestimulus alpha oscillations in multisensory networks representing grapheme/phoneme associations increase with predictions and correlate with early sensory component amplitudes, suggesting a role in selective amplification of predicted information \cite{mayerExpectingSeeLetter2016}.

% Similarly,  theta’s role in cross-frequency coupling, especially phase-amplitude coupling (PAC) with gamma oscillations, enables it to organize and integrate information across neural networks \cite{lismanThetaGammaNeuralCode2013}.

% Theta and gamma oscillations PAC, where the phase of the slower theta oscillation (4–8 Hz) modulates the amplitude—specifically, the envelope—of the faster gamma oscillation (30–80 Hz). In PAC, the gamma envelope, representing the overall magnitude or intensity of the gamma signal, waxes and wanes in sync with the theta phase, creating a coupling that helps encode and structure complex information, such as the rhythmic and phonemic components of speech \cite{scheffer-teixeiraCrossfrequencyPhasephaseCoupling2016}. This coupling enables theta oscillations to segment speech into syllabic units, while the gamma envelope encodes finer details within each segment, supporting hierarchical language processing \cite{giraudCorticalOscillationsSpeech2012}.

% EEG and MEG studies show that during selective listening, low-frequency cortical responses in the delta (1–3 Hz) and theta (3–7 Hz) bands preferentially track the temporal envelope of attended speech, allowing the brain to selectively tune into relevant auditory information \cite{dingNeuralCodingContinuous2012,osullivanAttentionalSelectionCocktail2015}. ECoG studies further reveal that the slowly varying envelopes of high-frequency responses in the high-gamma range ($>$70 Hz) also track attended speech, highlighting the role of high-gamma power in auditory attention \cite{mesgaraniSelectiveCorticalRepresentation2012,golumbicMechanismsUnderlyingSelective2013}. In particular, in \cite{golumbicMechanismsUnderlyingSelective2013}, brain activity is seen to dynamically track speech streams using both low-frequency phase and high-frequency amplitude fluctuations


% In Viswanathan (2019) \cite{viswanathanElectroencephalographicSignaturesNeural2019}, a key focus was on feature selection in neural signal processing, specifically in the context of selective attention to auditory stimuli. They reported that for low-frequency bands (delta, theta, alpha, and beta), the filtered EEG signal itself was treated as a feature. However, for higher-frequency gamma bands, they used only the amplitude envelopes as features and discarded phase information, a decision inspired by findings from electrocorticography (ECoG) studies indicating that high-gamma amplitude envelopes track attended speech selectively over ignored speech  \cite{mesgaraniSelectiveCorticalRepresentation2012,golumbicMechanismsUnderlyingSelective2013}.

% Furthermore, for the alpha (8–12 Hz) and beta (12–30 Hz) bands, Viswanathan included the amplitude envelopes as additional features separate from the filtered EEG signal. This choice was motivated by evidence that alpha power fluctuates coherently with attended stimuli (Wöstmann et al., 2016) and that beta power varies systematically with task demands across a range of cognitive and motor tasks (Engel and Fries, 2010). By incorporating both filtered signals and amplitude envelopes in these bands, Viswanathan aimed to capture both the steady-state and fluctuating attentional effects within the neural data, supporting a richer, more nuanced understanding of the neural correlates of selective auditory attention.

% This feature selection approach, outlined by Viswanathan (2019), includes treating the filtered EEG signal in delta, theta, alpha, and beta bands as a feature while using only the amplitude envelope for higher-frequency gamma bands, motivated by findings from ECoG studies. In the alpha (8–12 Hz) and beta (12–30 Hz) bands, both the filtered signal and the amplitude envelope are considered, as alpha power coheres with attentional focus (Wöstmann et al., 2016), and beta-band power varies across cognitive and motor tasks (Engel and Fries, 2010). Wöstmann et al. (2021) further show that alpha oscillations serve a dual role as a spatio-temporal attentional filter: alpha power lateralizes with spatial attention, increasing in the hemisphere ipsilateral to the attended side and decreasing in the contralateral hemisphere. This lateralization can be temporally modulated, as demonstrated by MEG recordings where alpha power lateralization was stronger for spatially attended cues when participants had foreknowledge of a target’s timing. The lateralization precisely at temporally cued onsets of attended numbers indicates alpha’s role in dynamically filtering spatial and temporal attention to optimize perceptual performance.


% Together, alpha and theta rhythms create a dynamic balance between local inhibition and long-range coordination, supporting efficient neural processing and flexible cognitive functioning across various demands \cite{kaplanMedialPrefrontalTheta2014, friesRhythmsCognitionCommunication2015}.


% We previously introduced a laminar neural mass model (LaNMM) designed to capture both superficial-layer fast and deeper-layer slow oscillations along with their interactions, such as phase-amplitude coupling (PAC) and amplitude-amplitude anticorrelation (AAC)\cite{sanchez-todoPhysicalNeuralMass2023}. The spectrolaminar motif is ubiquitous across the primate cortex \cite{mendoza-hallidayUbiquitousSpectrolaminarMotif2024}.  This framework allows for a dynamic instantiation of the Comparator, facilitating the analysis of how prediction errors and top-down predictions may be performed across different cortical depths. LaNMM combines conduction physics with neural mass models (NMMs) to simulate depth-resolved electrophysiology. Cortical function emerges from multi-scale networks, and NMMs provide a high-level representation of the mean activity of large neuronal populations. LaNMM builds on this by incorporating realistic conduction properties, enabling a detailed simulation of how slow and fast oscillatory sources interact across layers.
% By applying LaNMM to laminar recordings from the macaque prefrontal cortex, we generated a minimal model capable of reproducing coupled oscillations across layers. This allowed us to optimize LaNMM parameters to match functional connectivity (FC) patterns derived from empirical data. Here, FC was defined by the covariance between bipolar voltage measurements across cortical depths, with optimal configurations reproducing observed FC patterns, specifically generating fast activity in superficial layers and slow oscillations across deeper layers.
% This framework not only aligns with recent findings in cortical oscillatory dynamics but also offers a practical platform for investigating the role of the Comparator. By analyzing the PAC and AAC patterns in LaNMM simulations, we gain insight into how fast and slow oscillatory interactions may underpin prediction error generation, routing, and inhibition in hierarchical cortical processing.

% LaNMM fits comfortably in the context of hierarchical processing and predictive coding. There exist clear asymmetries in hierarchical cortical connectivity, particularly in the distinct roles of feedforward (FF) and feedback (FB) pathways in sensory processing  \cite{bastosDCMStudySpectral2015}.    Anatomical studies established that FF pathways typically originate from superficial layers and project to granular layers, while FB pathways arise from deeper layers, connecting to non-granular areas, supporting a hierarchical processing model. This anatomical arrangement is reflected physiologically: FF pathways exhibit strong excitatory signals, using ionotropic receptors, while FB pathways are more modulatory, involving both ionotropic and metabotropic receptors and acting at dendritic arbors.  Functionally, FF and FB connections also differ in frequency domains. FF signals often use gamma-band frequencies and are associated with prediction error transmission, while FB signals utilize lower alpha and beta frequencies, hypothesized to convey predictions and aligning with predictive coding models. In this framework, FF connections deliver prediction errors to higher-order areas, while FB signals transmit top-down predictions to explain sensory input. Experimental studies support this model, showing that gamma coherence is stronger in FF pathways, while alpha/beta coherence is more prominent in    FB pathways, suggesting a mechanism where faster frequencies convey discrepancies and slower frequencies signal model-driven predictions \cite{bastosDCMStudySpectral2015}. Finally, in hierarchical processing, the selection of ascending information by adjusting the ‘volume’ or gain of prediction errors that compete for influence over higher levels of processing \cite{fristonLFPOscillationsWhat2015}. 


% \subsection{General model for information  encoding and processing}
% In digital communication systems, information is usually represented as a sequence of binary digits (0’s and 1’s). The physical encoding, transportation, and processing of information in computers are achieved through the manipulation of electrical currents within transistors. These components leverage semiconductor properties to control current flow, thereby enabling the execution of complex computational tasks. However, in analog systems and biological networks, information is conveyed through continuous signals. Amplitude modulation (AM) radio is a classic example, where information is encoded in the amplitude envelope of a high-frequency carrier wave. Similarly, in neural systems, the amplitude envelope of oscillatory activity can carry meaningful information.  In the framework of the brain as an oscillatory computational system, information may be encoded in different ways, for example, in the power, envelope, or phase \cite{buzsakiNeuronalOscillationsCortical2004} of signals. More generally, we may think of information as encoded by the signal themselves (not some features of it), just as audio waves encode information. There is evidence that slow-wave signals encode information in this manner, with high gamma-band activities exhibiting amplitude-modulation at the same rate as the stimulus envelope of speech  \cite{tamuraCorticalRepresentationSpeech2023}.  The seminal study by Pasley et al. (2012) \cite{pasleyReconstructingSpeechHuman2012} successfully reconstructed speech by decoding the high gamma power envelope from ECoG recordings and mapping it to the spectrogram of the speech signal. The high gamma envelope was crucial because it tracked the amplitude and rhythm of the speech signal closely, allowing the researchers to recreate a spectrogram that could then be transformed back into an audible approximation of the original speech.
% This  demonstrated that gamma-band activity in the auditory cortex contains rich, temporally precise information about the auditory signal, which can be leveraged for reconstructing intelligible sounds.  


 

% Crucially, we will consider the idea of coarse-grained multiscale information encoding. %Figure~\ref{fig:multiscale} provides an example of a multi-scale nested signal generated in Python, spanning 6 seconds. The fastest oscillation is a 60 Hz “gamma” carrier, whose amplitude is modulated by a 4 Hz “theta” wave. In turn, the theta wave’s amplitude is further modulated by a 1 Hz low-frequency envelope. This hierarchy illustrates how slower signals can shape the amplitude fluctuations of faster rhythms, reflecting a nested, multiscale structure.




% Thus, in our framework, information is not only carried by the raw signal (for instance, an alpha band oscillation) but also by its successively extracted envelopes across multiple time scales. Building on the notion of hierarchical temporal organization in brain signals \cite{penttonenNaturalLogarithmicRelationship2003,buzsakiScalingBrainSize2013} (Figure~\ref{fig:logarithmic_hierarchy}), we propose that meaningful structure can be revealed when we move from the primary oscillation to its envelope, and then to the envelope of that envelope, and so on. Each “layer” of the envelope captures fluctuations at progressively slower time scales, enabling us to disentangle the nested rhythmic features that are otherwise hidden in the raw signal. In this framework, the laminar model acts as a machine that systematically extracts these envelopes—layer by layer—and compares them, thus exposing the rich, multi-level temporal structure of neural activity. This framework is further discussed below.

 









 

 

% \subsection{Neural Coding Schemes: Rate, Temporal, Phase, and Burst Codes}

% Researchers have long debated how neurons encode information, contrasting \textbf{rate coding} with various \textbf{temporal coding} strategies. In rate coding, information is carried by a neuron’s average firing rate (spikes per second) over some time window. In contrast, temporal codes rely on the precise timing or patterns of spikes. This includes \textit{spike timing codes} (where the exact millisecond timing or the order of spikes matters) and \textit{synchrony codes} (where coordinated firing across neurons can signal relationships or “binding” of stimulus features). Indeed, an \textit{ongoing debate} asks how much information is conveyed by precise spike timing versus mean firing rates \cite{zeldenrustNeuralCodingBurstsCurrent2018}. Temporal codes can enable extremely rapid information readout -- for example, firing-order or first-spike codes can support fast sensory discriminations that would be too quick for pure rate codes \cite{carianiTimeEssenceNeural2022}. In such schemes, which neurons fire first (and at what millisecond) carries meaning, allowing decisions on the order of tens of milliseconds (as observed in vision and audition).

% \textbf{Phase coding} is a special case of temporal coding where spikes convey information by their phase with respect to an ongoing neural oscillation. For instance, hippocampal place cells not only increase firing rate at a particular location (rate code for position), but also fire at progressively earlier phases of the local theta rhythm as an animal moves through the place field. This \textit{phase precession} is thought to provide additional information about position or timing that complements the rate code. More generally, spike timing relative to oscillatory phase can multiplex information: neurons can encode one aspect of a stimulus in their firing rate and another in the phase at which they fire \cite{lismanThetaGammaNeuralCode2013}. Evidence in the hippocampus supports the idea that dual oscillations (theta and gamma) form a \textit{theta--gamma code} for multiple items in sequence -- each theta cycle ($\sim$100 ms) is subdivided into gamma subcycles that represent different pieces of information in an ordered manner \cite{lismanThetaGammaNeuralCode2013}. In this way, phase coding by nested rhythms can create an internal sequencing of information (as seen in spatial navigation and possibly in multi-item working memory).

% \textbf{Burst coding} adds another layer to neural encoding. Neurons often fire \textit{bursts} -- brief clusters of spikes -- in addition to isolated spikes. Bursts have been hypothesized to serve distinct functional roles. One view is that bursts simply enhance reliability at synapses: a barrage of spikes in quick succession can overcome synaptic transmission failures. However, bursts might also carry specific information \textit{in their internal structure}. Researchers ask whether the number of spikes in a burst, or the precise inter-spike intervals within a burst, encode unique information beyond a simple “there was a burst” signal \cite{zeldenrustNeuralCodingBurstsCurrent2018}. Recent reviews highlight that bursts \textit{do} convey information and are not only for reliability \cite{zeldenrustNeuralCodingBurstsCurrent2018}. For example, bursts and single spikes may form \textbf{parallel codes} in the same neuron’s output, each relating to different stimulus features \cite{zeldenrustNeuralCodingBurstsCurrent2018}. A burst could represent a slower or more significant event, while single spikes embedded between bursts represent higher-frequency details \cite{zeldenrustNeuralCodingBurstsCurrent2018}. Bursts also have a disproportionate impact on targets -- acting as a “wake-up call” that can reset or enhance responsiveness in downstream neurons \cite{zeldenrustNeuralCodingBurstsCurrent2018}. This has led to the idea that bursts might flag important events (an attentional \textit{“searchlight”} signal \cite{zeldenrustNeuralCodingBurstsCurrent2018}) or indicate the coincidence of multiple inputs (when generated by dendritic spike events \cite{zeldenrustNeuralCodingBurstsCurrent2018}). In summary, contemporary perspectives view neural coding as \textit{multi-faceted}: firing rates, spike timing, phases of oscillations, and bursts all provide complementary channels of information.

% \subsection{Oscillations and Information Processing}

% Brain oscillations (rhythmic fluctuations of neural activity) are now recognized as central to neural information processing. Oscillations provide a temporal structure -- a scaffold of cycles -- that can organize when neurons fire. One consequence is \textbf{phase-dependent excitability}: the probability or impact of a spike can depend on \textit{when} in an oscillatory cycle it occurs \cite{canoltyFunctionalRoleCrossfrequency2010}. For example, if a neuron is most excitable at the trough of a local field potential oscillation, a spike or input arriving at that phase will be processed more effectively than one arriving at the peak \cite{canoltyFunctionalRoleCrossfrequency2010}. In this way, oscillations can gate information flow by timing it to phases of high responsiveness. Oscillatory synchrony also enables effective communication across brain regions: when groups of neurons oscillate in phase, their spikes arrive at target areas in a coordinated manner, summing to have a larger impact. This is the basis of the \textbf{communication-through-coherence} hypothesis, which proposes that coherence (phase alignment) between sender and receiver neural groups facilitates selective information transmission \cite{friesRhythmsCognitionCommunication2015}. Notably, different frequency bands may subserve different signaling roles. \textbf{Gamma-band (30--100 Hz)} oscillations often reflect local, fast information processing (e.g., encoding sensory details or content), while slower rhythms like \textbf{alpha/beta (8--20 Hz)} can carry top-down influences (e.g., attentional or predictive signals) that modulate the gamma activity \cite{friesRhythmsCognitionCommunication2015}. Indeed, in one model, feed-forward (bottom-up) signals are carried by high-frequency gamma synchrony, whereas feed-back (top-down) signals are conveyed by slower alpha/beta oscillations that can modulate the gamma timing \cite{friesRhythmsCognitionCommunication2015}. Even the theta band ($\sim$4--8 Hz) has been implicated in sampling or sequencing information -- for instance, attention might effectively “sample” inputs at a theta rhythm \cite{friesRhythmsCognitionCommunication2015}. Thus, multiple oscillations interact to make neural communication \textit{effective, precise, and selective} \cite{friesRhythmsCognitionCommunication2015}.

% Beyond their role in coordinating timing, oscillations also allow the brain to \textbf{multitask or multiplex} information streams. Because oscillations occur at various frequencies, neural signals can be segregated in the frequency domain. A striking observation is that brain activity organizes into a \textbf{hierarchy of oscillatory bands} -- from slow ($<$1 Hz, delta) up through theta, alpha, beta, gamma, and even faster “ripple” oscillations -- and this hierarchy is preserved across brain regions and species \cite{penttonenNaturalLogarithmicRelationship2003, buzsakiBrainRhythmsNeural2012}. These rhythms are not independent; they \textbf{interact across frequencies}, supporting cross-scale communication. In other words, the brain appears to use a layered timing structure, where slow fluctuations can align or modulate faster ones. This is evident in measures of \textbf{oscillatory amplitude modulation}: for example, the amplitude (power) of a high-frequency gamma oscillation often waxes and wanes in step with the phase of a slower theta wave. Such \textit{phase-amplitude coupling} suggests that slower oscillations can control the gain of faster oscillatory activity. Because slow oscillations tend to involve larger, more distributed networks (operating on behavioral or cognitive timescales), and fast oscillations involve local circuits (processing fine details), their coupling provides a way to bridge \textbf{multiple spatial and temporal scales of processing} \cite{canoltyFunctionalRoleCrossfrequency2010}. In fact, accumulating evidence indicates that information processing in the brain is intrinsically \textbf{multi-scale}, and a \textit{hierarchy of coupled oscillations} is well-suited to regulate this integration across scales \cite{canoltyFunctionalRoleCrossfrequency2010}. Through oscillations, the brain can synchronize distant regions, segment continuous streams into discrete “frames,” and dynamically route signals -- all essential for complex cognitive functions such as attention, memory, and perception [\cite{carianiTimeEssenceNeural2022}.

% \subsection{Cross-Frequency Coupling and Nested Hierarchies}

% \textbf{Cross-frequency coupling (CFC)} refers to interactions between oscillations at different frequencies, and it is a key mechanism for hierarchical encoding in neural circuits. In nested \textbf{oscillatory hierarchies}, a slower rhythm can organize the occurrence or amplitude of faster rhythms [\cite{buzsakiBrainRhythmsNeural2012}. One well-studied example is theta--gamma coupling: the phase of a 5 Hz theta oscillation modulates the amplitude or timing of $\sim$50 Hz gamma bursts. This nesting effectively creates “packets” of high-frequency activity within each cycle of the slow wave, which can be viewed as a higher-order coding unit. Lisman and colleagues have argued that theta--gamma coupling constitutes a neural code capable of ordering multiple items (as mentioned above for hippocampal sequences) \cite{lismanThetaGammaNeuralCode2013}. More generally, CFC has been observed across many brain areas and frequency combinations, suggesting a \textbf{general hierarchical organization} of brain rhythms \cite{buzsakiBrainRhythmsNeural2012}. The \textbf{strength} of phase-amplitude coupling often reflects behavioral or cognitive states: it varies with task demands and learning, and higher coupling can correlate with better performance \cite{canoltyFunctionalRoleCrossfrequency2010}. For example, during learning tasks, the degree of theta--gamma coupling in relevant circuits changes rapidly with new information and can predict memory retention or decision success \cite{canoltyFunctionalRoleCrossfrequency2010}. In working memory paradigms, stronger coupling between prefrontal low-frequency rhythms and high-frequency activity has been linked to greater memory capacity or focus, implying that coupling helps coordinate the maintenance of multiple items in memory \cite{buzsakiBrainRhythmsNeural2012}. Thus, CFC is not just an epiphenomenon -- it appears to \textbf{serve functional roles} in computation, communication, and plasticity \cite{canoltyFunctionalRoleCrossfrequency2010}.

% One way to understand nested oscillations is as a \textbf{layered coding scheme}, akin to a compositional syntax. Just as letters form words and words form sentences, fast oscillatory cycles can be grouped by slower cycles into larger assemblies. Buzs\'aki and colleagues describe this as a potential \textbf{“neural syntax”}, where cross-frequency coupling imposes syntactic rules on neural firing -- i.e., which spike patterns (or gamma bursts) go together in time \cite{buzsakiBrainRhythmsNeural2012}. The slower rhythm provides a reference frame that segments and orders the faster activity, making it easier for downstream “readers” (neuronal targets) to interpret the spikes. In practical terms, if individual spikes or high-frequency bursts are like letters conveying fine details, a slower oscillation can concatenate these into intelligible “words” or “phrases” that carry a compound meaning \cite{buzsakiBrainRhythmsNeural2012}. This hierarchical \textbf{packaging of information} could underlie complex pattern generation and parsing in the brain. Notably, cross-frequency coupling comes in different forms -- not only phase--amplitude (slow phase modulates fast amplitude) but also phase--phase locking (integer frequency relationships) and amplitude--amplitude correlations between bands \cite{buzsakiBrainRhythmsNeural2012}. Each type of coupling might support different computations or interactions. Phase--phase locking (often an $n : m$ frequency relationship) can temporally align neural events across scales (e.g., one gamma burst every cycle of a beta rhythm, etc.), whereas phase--amplitude coupling effectively uses the slow wave as a carrier signal that gates fast content. Across the cortex and hippocampus, virtually all co-occurring rhythms can exhibit CFC \cite{buzsakiBrainRhythmsNeural2012}, indicating \textbf{ubiquitous nesting} from very slow ($<$0.1 Hz) oscillations up to fast ripples ($\sim$150--200 Hz). This nested organization means the brain operates with a deep \textbf{temporal hierarchy}, where higher-order cortical areas or network states (often oscillating slower) can modulate and orchestrate the fine-scale activity of local circuits (oscillating faster). Such dynamics have been hypothesized to support cognitive functions that require integration of information over multiple timescales -- for instance, parsing speech (syllables within words within phrases) or forming episodic memories (encoding sequences of events in order).

% \subsection{Integration of Coding Schemes in Hierarchical Layers}

% Rather than using any single coding scheme in isolation, neural systems likely \textbf{integrate multiple codes} that interact across layers of processing. An emerging view is that the brain multiplexes information much like a communication system, using different “channels” (whether defined by neuron populations, time windows, or frequency bands) to carry different aspects of information simultaneously [\cite{carianiTimeEssenceNeural2022} L159-L167]. The architecture of the brain -- with its layered cortex and recurrent loops -- supports such parallel and hierarchical coding. For example, a single neuron could use a combination of rate and temporal codes: it might increase firing rate to signal the intensity of a stimulus, while the precise spike timing relative to a network oscillation conveys qualitative information about that stimulus. Similarly, as mentioned, \textbf{single spikes vs. bursts} from the same cell may form a dual code \cite{zeldenrustNeuralCodingBurstsCurrent2018}. Experimental evidence in sensory systems shows that bursts often code for \textit{slow} or context features, whereas isolated spikes code for \textit{rapid detail} features, effectively splitting the information stream by timescale \cite{zeldenrustNeuralCodingBurstsCurrent2018}. This can be seen as a form of \textbf{time-division multiplexing} -- using the timing pattern of spikes (clustered vs.\ singular) to send two messages on one output line.

% At the network level, oscillations of different frequencies provide distinct \textbf{communication channels} that can operate concurrently. This resembles \textbf{frequency-division multiplexing} in engineered systems: distinct frequency bands can carry separate streams without interference \cite{carianiTimeEssenceNeural2022}. Brain rhythms might dynamically assign certain computations to gamma band activity and others to beta or theta bands, all happening in parallel. Critically, these channels are not independent; they \textbf{interact} to ensure the streams are recombined appropriately for unified perception and behavior. The interplay of gamma, beta, alpha, theta, etc., can thus implement a layered processing hierarchy. For instance, in attention and perception, \textbf{fast gamma oscillations} in visual cortex can encode the features of a currently attended object, while \textbf{alpha oscillations} regulate the timing of these gamma bursts and segregate other objects in the scene by suppressing or desynchronizing their gamma activity. A recent framework for visual attention proposes exactly this kind of hierarchical multiplexing: different elements of an object are processed in separate high-frequency bursts (gamma) \textit{within} a single cycle of an alpha rhythm, ensuring they are bound together as one object, whereas different objects are processed in successive alpha cycles (thereby kept separate in time) \cite{bonnefondVisualProcessingHierarchical2024}. Furthermore, even slower oscillations like theta (perhaps tied to eye movements or attentional shifts) could align these alpha cycles when scanning a scene \cite{bonnefondVisualProcessingHierarchical2024}. This \textbf{nested multiplexing} idea illustrates how multiple encoding schemes (phase coding, bursting, oscillatory gating) can dovetail: a slow rhythm sets up a temporal frame for each object (like a “time slot”), within which rapid bursts encode the object’s features by phase coding along the alpha cycle \cite{bonnefondVisualProcessingHierarchical2024}. Such a system is flexible and hierarchical -- akin to a multi-layer computer architecture where signals are modulated and demodulated across different layers of a network.

% From a theoretical standpoint, researchers have drawn analogies between neural coding and the layered protocols of communication systems. Cariani and others suggest that the brain might implement forms of \textbf{time-division, frequency-division, and code-division multiplexing} to handle the immense complexity of information it processes \cite{carianiTimeEssenceNeural2022}. In this view, neural oscillations and precisely timed spike patterns act like carriers and modulators, allowing neurons to \textbf{broadcast multiplexed, temporally-patterned signals} that can be selectively read by receivers tuned to the appropriate timing or frequency \cite{carianiTimeEssenceNeural2022}. Just as radio transmitters broadcast at different frequencies to avoid mutual interference, different neural assemblies might oscillate at distinct frequencies to avoid cross-talk and then use moments of synchrony (phase alignment) to communicate when needed. The concept of \textbf{polychronous ensembles} is another example of a layered temporal code: Izhikevich and colleagues proposed that neurons with specific axonal conduction delays can fire in reproducible spatiotemporal patterns (with precise delays between spikes across neurons), creating a high-dimensional coding space for patterns (a form of \textit{time-offset code}). These ensembles can be active in overlapping ways as long as their spike timing patterns differ, analogous to code-division multiplexing. Such polychronous firing patterns essentially embed a hidden layer of temporal code that only a circuit with matching delay properties could interpret -- hinting at a kind of combinatorial coding that goes beyond simple rate or synchrony. While experimental evidence for polychronization is still emerging, it aligns with the broader idea that \textbf{neuronal networks exploit temporal diversity for coding capacity}.

% In summary, modern perspectives conceive of neural encoding as inherently \textbf{hierarchical and multi-layered}. Low-level neural events (spikes) are organized in time by intermediate patterns (bursts, synchrony, phase-locking), which are further structured by emergent global rhythms (oscillations, couplings). This layered structure allows the nervous system to encode \textbf{multiple features and contexts simultaneously} and to flexibly route information as task demands change. The interplay of different codes -- rate, temporal, phase, and bursts -- in a hierarchical manner enables something analogous to a \textbf{neural information architecture}. It ensures that local details and global context are integrated, much like how, in a computer, bits are organized into bytes, packets, and higher-level data structures. Crucially, a growing body of theoretical models and empirical data supports this view. We see evidence from hippocampal theta--gamma coding of sequences \cite{lismanThetaGammaNeuralCode2013}, to cross-frequency coupling correlating with learning and memory performance \cite{canoltyFunctionalRoleCrossfrequency2010,buzsakiBrainRhythmsNeural2012}, to human neuroimaging studies showing nested oscillations tracking multi-level linguistic structures during speech comprehension \textsuperscript{\dag}. All of these findings converge on the idea that \textbf{neuronal information is encoded across multiple temporal scales} in a coordinated fashion. This hierarchical encoding may be fundamental to how the brain achieves efficient, robust, and context-sensitive computation. 

\end{document}




% \subsection{PEIX: A Normalized Slope Measure of Excitation–Inhibition Balance}
% \label{sec:PEIX}
% \newcommand{\peix}{\mathrel{\raisebox{-0.1ex}{\scalebox{1.5}{\reflectbox{$\propto$}}}}}

% Neural populations are often modeled by a sigmoid function,
% \[
% S(V) = \frac{2e_0}{1+\exp\big(r(V_0-V)\big)},
% \]
% where \(V_0\) is the half-maximum potential (the linear operating point), \(e_0\) is the maximum firing rate, and \(r\) sets the slope. The derivative,
% \[
% S'(V) = \frac{2e_0\, r\,\exp\big(r(V_0-V)\big)}{\Bigl(1+\exp\big(r(V_0-V)\big)\Bigr)^2},
% \]
% peaks at \(V=V_0\) with \(S'(V_0)=\frac{e_0r}{2}\) and decreases as the system enters its excitatory (\(V \gg V_0\)) or inhibitory (\(V \ll V_0\)) saturation regimes.

% To quantify the deviation from the ideal linear regime, we define the dimensionless variable
% \[
% x = r\big(\langle V\rangle - V_0\big),
% \]
% where \(\langle V\rangle\) is the average membrane potential. By normalizing the local slope \(S'(V)\) with its maximum value at \(V_0\), we obtain a measure of nonlinearity that distinguishes between excitation and inhibition. The \textbf{Population Excitation-Inhibition Index (PEIX)} is defined as
% \[
% \peix(x) = -\operatorname{sign}(x) \left(\frac{4\,\exp(-x)}{\Bigl(1+\exp(-x)\Bigr)^2} - 1\right).
% \]
% This formulation ensures that:
% \begin{itemize}
%     \item \(\peix(x) \approx 0\) when \(\langle V\rangle \approx V_0\) (linear regime),
%     \item \(\peix(x) > 0\) when \(\langle V\rangle > V_0\) (excitatory state),
%     \item \(\peix(x) < 0\) when \(\langle V\rangle < V_0\) (inhibitory state),
%     \item \(|\peix(x)| \approx 1\) in the saturated regimes.
% \end{itemize}
% Thus, by focusing on the normalized slope of the sigmoid, PEIX provides a compact, dimensionless index of both the deviation from the optimal operating point and the underlying excitation–inhibition balance.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 


% \section{Comparator (Innovation) Role in a Hierarchical Kalman Filter}

% A standard Kalman filter is a sequential algorithm for estimating a hidden state 
% $\mathbf{x}_k$ of a system at each discrete time step $k$. 
% It operates in two main steps:

% \begin{enumerate}
%     \item \textbf{Prediction:} Use the previous state estimate and the system model to 
%     predict the next state.
%     \item \textbf{Update:} Incorporate new measurements to correct the predicted state, 
%     guided by the \emph{innovation} (or \emph{comparator output}), which is the discrepancy 
%     between predicted and observed measurements.
% \end{enumerate}

% \paragraph{Standard Formulation.}
% We typically assume a linear state-space model:
% \[
% \begin{cases}
% \mathbf{x}_k \;=\; \mathbf{A}_k\,\mathbf{x}_{k-1} + \mathbf{B}_k\,\mathbf{u}_{k-1} + \mathbf{w}_{k-1},\\
% \mathbf{z}_k \;=\; \mathbf{H}_k\,\mathbf{x}_k + \mathbf{v}_k,
% \end{cases}
% \]
% where $\mathbf{x}_k$ is the state at time $k$, $\mathbf{u}_k$ is a known control input, 
% and $\mathbf{z}_k$ is the measurement at time $k$. The matrices 
% $\mathbf{A}_k$, $\mathbf{B}_k$, and $\mathbf{H}_k$ define how the system evolves and 
% how measurements map from the state. The noise terms $\mathbf{w}_{k-1}$ and $\mathbf{v}_k$ 
% represent process and measurement noise, respectively, with covariances 
% $\mathbf{Q}_{k-1}$ and $\mathbf{R}_k$.

% At each step $k$, the filter does:

% \[
% \textbf{(1) Prediction Step:}
% \quad
% \begin{cases}
% \mathbf{x}_{k}^{\mathrm{pred}} \;=\; \mathbf{A}_k\,\mathbf{x}_{k-1}^{\mathrm{upd}}
% \;+\; \mathbf{B}_k\,\mathbf{u}_{k-1},\\
% \mathbf{P}_{k}^{\mathrm{pred}} \;=\; \mathbf{A}_k\,\mathbf{P}_{k-1}^{\mathrm{upd}}\,\mathbf{A}_k^{T}
% \;+\; \mathbf{Q}_{k-1},
% \end{cases}
% \]
% where $\mathbf{x}_{k}^{\mathrm{pred}}$ and $\mathbf{P}_{k}^{\mathrm{pred}}$ are the 
% \emph{predicted} state and its error covariance, respectively. They depend only on 
% information up to time $k-1$.

% \[
% \textbf{(2) Innovation (Comparator) Output:}
% \quad
% \boldsymbol{\nu}_k \;=\; \mathbf{z}_k \;-\; \mathbf{H}_k\,\mathbf{x}_{k}^{\mathrm{pred}}.
% \]
% This quantity $\boldsymbol{\nu}_k$ is the \emph{difference} between the 
% \emph{predicted measurement} $\mathbf{H}_k\,\mathbf{x}_{k}^{\mathrm{pred}}$ 
% and the \emph{actual} measurement $\mathbf{z}_k$; a large discrepancy indicates 
% a large prediction error.

% \[
% \textbf{(3) Kalman Gain:}
% \quad
% \mathbf{K}_k \;=\; \mathbf{P}_{k}^{\mathrm{pred}}\,\mathbf{H}_k^{T}\,
% \Bigl(\mathbf{H}_k\,\mathbf{P}_{k}^{\mathrm{pred}}\,\mathbf{H}_k^{T} + \mathbf{R}_k\Bigr)^{-1}.
% \]
% \[
% \textbf{(4) Update Step:}
% \quad
% \begin{cases}
% \mathbf{x}_{k}^{\mathrm{upd}} \;=\; \mathbf{x}_{k}^{\mathrm{pred}} + \mathbf{K}_k\,\boldsymbol{\nu}_k,\\[4pt]
% \mathbf{P}_{k}^{\mathrm{upd}} \;=\; \bigl(\mathbf{I} - \mathbf{K}_k\,\mathbf{H}_k\bigr)\,\mathbf{P}_{k}^{\mathrm{pred}}.
% \end{cases}
% \]
% Hence, $\mathbf{x}_{k}^{\mathrm{upd}}$ and $\mathbf{P}_{k}^{\mathrm{upd}}$ become the \emph{posterior} 
% (or “corrected”) estimates at time $k$, which are then used to \emph{predict} time $k+1$ in the same fashion.

% \medskip

% \paragraph{Hierarchical Extension.}
% In this work, we adopt a \emph{hierarchical} perspective akin to multi-scale or 
% multi-layer processing: rather than applying the Kalman filter to a single timescale, 
% we apply a version at \emph{each} level in a cortical hierarchy (or each frequency 
% band/envelope). In particular:

% \begin{figure} [t]
%     \centering
%     \includegraphics[width=0.99\linewidth]{figures/collisions.png}
%     \caption{Sequential filtering.}
%     \label{fig:collisions}
% \end{figure}


% \begin{itemize}
% \item \textbf{Coarse-Grained Predictions.} 
%   At each layer, the state $\mathbf{x}_{k}^{(\ell)}$ represents coarser features of the 
%   signal (e.g., slower dynamics or envelope information). The update aims to cancel out 
%   prediction error \emph{at that layer’s timescale}.
% \item \textbf{Comparator Mechanism.} 
%   Each layer receives an “incoming observation” $\mathbf{z}_{k}^{(\ell)}$ (potentially 
%   derived from the layer below) and compares it to the \emph{layer’s own} predicted 
%   measurement $\mathbf{H}^{(\ell)}\mathbf{x}_{k}^{(\ell), \mathrm{pred}}$. This 
%   innovation drives the layer-specific correction.
% \item \textbf{Up/Down Interactions.} 
%   Higher layers process information at lower temporal frequencies (or coarser resolution), 
%   while lower layers operate at finer scales. The mismatch (innovation) at each level 
%   feeds back to refine predictions at that level and potentially provides feedback to 
%   adjacent layers.
% \end{itemize}

% Thus, although the \emph{mathematical} form of each layer’s filter update appears similar 
% to a standard Kalman filter, the \textbf{intent} and \textbf{information flow} differ: 
% \emph{each layer’s comparator “kills the error” at its own spatiotemporal granularity.} 
% In neural terms, we could interpret this as a hierarchy of cortical areas, each predicting 
% the activity (or envelopes) in the layer below, with the local “innovation” indicating the 
% prediction error that drives learning or adjustment across layers. 
% This multi-level scheme helps the system track and predict signals with \emph{layer-specific} 
% dynamics and noise characteristics, rather than a single Kalman filter struggling with 
% all temporal scales at once.


% \textbf{Kalman Filtering and Active Inference: A Least-Squares Perspective}

% \textbf{1. Kalman Filter in Least-Squares Form}

% Assumptions: We consider a linear state-space model with Gaussian noise. Let $x_k$ be the hidden state at time $k$, and $y_k$ the observation. The dynamics are $x_k = A,x_{k-1} + w_k$ and the observation model $y_k = H,x_k + v_k$, where $w_k \sim \mathcal{N}(0, Q)$ and $v_k \sim \mathcal{N}(0, R)$ are process and measurement noise (zero-mean Gaussian) with covariances $Q$ and $R$. The goal is to estimate $x_k$ given observations up to time $k$.

% Least-squares cost function: Under these linear Gaussian assumptions, the maximum a posteriori (MAP) estimate of $x_k$ (which coincides with the minimum mean-square error estimate for Gaussian noise) can be obtained by minimizing a weighted least-squares cost.  Intuitively, we want $x_k$ to be close to the prior prediction $x_{k|k-1}$ (the predicted state from time $k-1$) and also close to the value that explains the new observation $y_k$. This leads to the cost function:

% $$
% J(x_k) = \underbrace{(x_k - \hat{x}{k|k-1})^T P{k|k-1}^{-1}(x_k - \hat{x}{k|k-1})}{\text{distance from prior (prediction error)}} ;+;
% $$
% $$ \underbrace{(y_k - H,x_k)^T R^{-1}(y_k - H,x_k)}_{\text{distance from measurement (observation error)}},
% $$

% where $\hat{x}{k|k-1}$ is the prior state estimate (prediction) and $P{k|k-1}$ its covariance. The first term is the squared prediction error (the difference between $x_k$ and the predicted state) weighted by the inverse of the prior covariance (the precision of the prediction). The second term is the squared observation prediction error (innovation) weighted by the measurement precision $R^{-1}$. This cost function is essentially the negative log-posterior (up to an additive constant), so minimizing it is equivalent to Bayesian MAP estimation.

% Update via prediction error minimization: To find the optimal $x_k$, we set $\nabla_{x_k} J = 0$. Differentiating and setting to zero gives the normal equations for the minimizer:

% $$
% P_{k|k-1}^{-1}(x_k - \hat{x}_{k|k-1}) - H^T R^{-1}(y_k - H,x_k) = 0.
% $$

% Re-arranging terms, we obtain:

% $$(P_{k|k-1}^{-1} + H^T R^{-1} H),x_k = P_{k|k-1}^{-1}\hat{x}_{k|k-1} + H^T R^{-1}y_k.$$

% Solving for $x_k$ yields the a posteriori estimate $\hat{x}_{k|k}$, which can be written in the familiar Kalman update form:

% $$
% \hat{x}{k|k} ;=; \hat{x}{k|k-1} + K_k,\big(y_k - H,\hat{x}_{k|k-1}\big),
% $$

% where

% $$
% K_k = P_{k|k-1} H^T, (H P_{k|k-1} H^T + R)^{-1}
% $$

% is the Kalman gain.  The term $(y_k - H,\hat{x}_{k|k-1})$ is the prediction error or innovation, and $K_k$ specifies the optimal weight given to this error. Notably, this gain $K_k$ is chosen precisely to minimize the posterior error covariance (i.e. it minimizes the expected mean-squared estimation error). In other words, the Kalman filter update finds the state estimate that minimizes the least-squares cost, balancing the prediction and observation errors according to their uncertainties. This is an explicit realization of prediction error minimization: the new estimate is the old prediction plus a correction term proportional to the prediction error.

% 2. Active Inference in Least-Squares Form

% Variational free energy (VFE) functional: Active inference casts state estimation as a variational inference problem. One defines a variational free energy $F$ which the agent (or estimator) minimizes instead of directly computing the posterior. In filtering, a common choice is to use a Gaussian variational density (a Laplace approximation), so that the free energy essentially becomes (up to constant terms) the negative log joint probability of the hidden state and observations, evaluated at the current approximate estimate. For the linear Gaussian case (same $A, H, Q, R$ as above), the VFE can be written as a sum of squared prediction errors, just like the Kalman least-squares cost. In fact, under a variational Gaussian (Laplace) assumption, one finds:

% $$
% F(x_k) ;=; \frac{1}{2}(y_k - H,x_k)^T R^{-1}(y_k - H,x_k) ;+; \frac{1}{2}(x_k - A,\hat{x}{k-1|k-1})^T Q^{-1}(x_k - A,\hat{x}{k-1|k-1}) ;+; \text{const},
% $$

% where $\hat{x}{k-1|k-1}$ is the previous posterior mean (serving as the prior mean for $x_k$ via the linear dynamics). Here the first term is the sensory prediction error (observation minus expected observation $H x_k$) weighted by its precision $R^{-1}$, and the second term is the state prediction error (difference between $x_k$ and its prediction $A\hat{x}{k-1|k-1}$) weighted by the process precision $Q^{-1}$. This form of $F$ mirrors the Kalman least-squares cost above. Minimizing VFE thus corresponds to explaining observations and dynamics as well as possible under the model, using a quadratic penalty for deviations – exactly akin to a weighted least-squares estimation.

% Free-energy minimization and update rule: To perform inference, active inference minimizes $F$ with respect to the approximate posterior parameters (here, effectively the mean $x_k$ since we assume a Gaussian form). Setting $\nabla_{x_k}F = 0$ leads to the condition:

% $$
% H^T R^{-1}(y_k - H,x_k);+; A^T Q^{-1}(\hat{x}_{k-1|k-1} - A^{-1}x_k);=;0,
% $$

% which simplifies to the same normal equation obtained for the Kalman filter solution. In other words, the stationary point of $F$ yields the exact same update for $x_k$ as the Kalman filter posterior mean. We can also see this by taking partial derivatives of $F$ as in a gradient descent scheme: the VFE gradient includes terms proportional to $H^T R^{-1}(y_k - H,x_k)$ (the observation prediction error weighted by precision) and $Q^{-1}(x_k - A,\hat{x}_{k-1|k-1})$ (the state prediction error weighted by precision) ￼. Driving these gradients to zero (or performing gradient descent until convergence) gives an equilibrium point satisfying

% $$x_k = \hat{x}{k-1|k-1} + P{k|k-1} H^T (H P_{k|k-1} H^T + R)^{-1}(y_k - H,x_k),$$

% which is exactly the Kalman update formula. In fact, it has been noted that in the linear Gaussian case, minimizing variational free energy recovers the same update equations as the Kalman filter ￼. Active inference thus reproduces the Bayesian filtering solution via a least-squares (free-energy) minimization principle. Importantly, what the Kalman filter achieves in closed-form at each time step can also be achieved by an active inference agent through gradient-based minimization of VFE – the end result (optimal state estimate) is the same in this linear case.

% \textbf{3. Linking the Updates: Kalman Gain vs. Precision Weighting}

% The Kalman filter update and the active inference update can now be directly compared. The Kalman gain $K_k = P_{k|k-1}H^T (H P_{k|k-1} H^T + R)^{-1}$ determines how much the prediction is adjusted in light of new evidence. This expression can be understood in terms of precisions (inverse variances). The term $(H P_{k|k-1} H^T + R)$ in the denominator is the innovation covariance (the uncertainty in the prediction error); its inverse is the precision of the innovation. Thus $K_k$ essentially equals $P_{k|k-1}H^T$ times the precision of the prediction error. If the observation is very reliable (small $R$, high precision), $K_k$ will be larger, giving the observation prediction error more weight. If the prior estimate is very uncertain (large $P_{k|k-1}$), $K_k$ will also be larger, meaning the system trusts the new data more. This is exactly the logic of precision-weighted prediction errors: the update $\hat{x}{k|k} = \hat{x}{k|k-1} + K_k (y_k - H\hat{x}_{k|k-1})$ says that the change in the estimate is proportional to the prediction error, scaled by a factor that reflects the relative confidence in that error.

% In active inference, the principle of precision-weighted prediction errors plays a central role in the update rules. In our VFE expression above, each prediction error term is multiplied by the inverse covariance (precision) of that term. Consequently, when we take the gradient or update equations, the magnitude of each correction is weighted by precision in the same way as the Kalman gain does. For example, in the simple case of estimating a static variable with a Gaussian prior and likelihood, the posterior mean is the prior mean plus a fraction $\frac{\text{likelihood precision}}{\text{prior precision} + \text{likelihood precision}}$ of the prediction error – i.e. the mean is updated by a precision-weighted error. This is precisely the Kalman filter logic: the posterior estimate is a weighted combination of prior and new information, with weights proportional to their precisions (equivalently, inversely proportional to their variances).

% In summary, the Kalman filter’s gain $K_k$ embeds the precision weighting: it tells us how to trade off the prior estimate versus the new sensory evidence. Active inference makes this weighting explicit by formulating an objective (VFE) where precisions appear as coefficients. Minimizing that objective naturally leads to an update of the state estimate by precision-weighted prediction errors, which is mathematically equivalent to the Kalman update in the linear Gaussian case. Thus, the Kalman filter can be seen as implementing a Bayesian prediction error correction, and active inference captures the same mechanism under the umbrella of variational (free-energy) minimization. Both frameworks agree that the optimal update increment is proportional to the prediction error, weighted by the certainty (precision) of that error ￼.

% 4. Linearization and Nonlinear Extensions

% So far we assumed linear dynamics and observations. How do Kalman filtering and active inference handle nonlinear problems? In the Kalman filter world, one common approach is the Extended Kalman Filter (EKF), which linearizes the nonlinear system about the current estimate. Essentially, the EKF uses a first-order Taylor expansion of the dynamics $f(x)$ and observation function $g(x)$ to compute local approximations $A \approx \partial f/\partial x$ and $H \approx \partial g/\partial x$. It then performs a Kalman update with these local Jacobians. This can be viewed as a Gauss-Newton step or local least-squares linearization at each time step. The result is a Gaussian approximate posterior for $x_k$ (mean and covariance) updated iteratively, which is approximately optimal for weakly nonlinear systems.

% Active inference tackles nonlinearity through its variational inference foundation. Rather than explicitly linearizing the system equations, active inference uses an approximate posterior that can be updated by gradient descent on the VFE. A common choice is to assume a Gaussian form for the posterior (the Laplace approximation), which leads to update equations very similar to an EKF. In fact, under the Laplace assumption, the variational free energy minimization entails expanding the (log) posterior to second order around its peak (the mode), which is mathematically equivalent to forming a locally linear Gaussian approximation of the model ￼. The resulting filter (sometimes called a generalized filtering or dynamic expectation-maximization scheme) yields recognition dynamics that closely match extended Kalman filtering updates ￼ ￼. In other words, active inference with a Gaussian approximate density will internally perform a linearization (through the curvature of the log-likelihood and log-prior) much like the EKF does externally by Jacobians. Indeed, it has been noted that under certain conditions, the updates from active inference are equivalent to an (extended) Kalman filter for nonlinear systems ￼ ￼.

% The advantage of the active inference approach is that it provides a principled Bayesian framework for extension to more complex scenarios. Nonlinear and even non-Gaussian models can be handled by changing the variational approximation or using more sophisticated inference (e.g. one could use mixture densities, particles, or iterative message passing to better approximate the posterior). The Kalman filter, in contrast, must be specifically adapted (EKF, unscented KF, particle filter, etc.) for non-linear/non-Gaussian cases. Active inference naturally generalizes because it rests on minimizing VFE: for any given generative model $p(y, x)$, one can write down $F$ and attempt to minimize it. If one uses a Gaussian ansatz, this becomes analogous to an EKF (a Laplace method); if one uses particles, it approaches a particle filter, and so on. Thus, active inference extends Kalman filtering to nonlinear regimes by casting filtering as general variational Bayesian inference. The linear Gaussian case is a useful special case where everything is analytically tractable (and the Kalman filter emerges exactly), while in truly nonlinear cases active inference will resort to iterative refinements (gradients, fixed-point iterations) to find a solution, much like one would linearize at each step in an EKF.

% In summary, both Kalman filtering and active inference share the same core principle of Bayesian least squares estimation. Kalman filtering provides closed-form update equations under linear Gaussian assumptions, choosing gains that minimize squared-error cost ￼. Active inference formulates the same problem via a variational free-energy objective that, in the linear case, leads to identical update rules ￼. In nonlinear scenarios, Kalman filters require explicit linearization (extended or unscented transforms), whereas active inference handles them through variational approximations (e.g. Laplace) as part of the inference process. This means that active inference inherently performs prediction error minimization with precision weighting, just as the Kalman filter does, while also providing a route to systematically handle complexity beyond the reach of basic linear Kalman filtering. The connection between the two is that Kalman filtering is essentially a specific case of active inference (or Bayesian filtering) under Gaussian assumptions – making the link between prediction-error minimization in control theory and variational free-energy minimization in cognitive neuroscience explicit and mathematically concrete

% References: In the above, we used standard results from Bayesian estimation theory and linear filtering. The Kalman gain is derived by minimizing the estimation error variance ￼. The active inference update in the linear-Gaussian case yields the same form, as shown by deriving the VFE and its gradient ￼ ￼ ￼. The notion of precision-weighted prediction errors appears in both frameworks ￼. For further reading, see e.g. Friston et al. on generalized filtering and predictive coding, which discuss how under Laplace assumptions active inference reduces to an equivalent of the extended Kalman filter ￼ ￼. This demonstrates the deep connection between Kalman filtering and active inference as approaches to optimal state estimation.
% %%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


